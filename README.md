 ## MLX8-W2: Two Tower Document Search
 https://hackmd.io/@-OWUHe7_QquNsx0fCrE8Qg/rJSMl2W4le

### ðŸ“ Perceptron Party

> #### AJ  
> #### Uncle Ben
> #### Charles
> #### Maria 

---

## Model Structure

```
two_tower_best_vivid-sweep-1_20250620_062217.pt
â”œâ”€â”€ ðŸ§  model_state_dict                    # PyTorch model weights & biases
â”‚   â”œâ”€â”€ query_tower.layers.0.weight       # Query encoder layer 1 weights
â”‚   â”œâ”€â”€ query_tower.layers.0.bias         # Query encoder layer 1 biases  
â”‚   â”œâ”€â”€ query_tower.layers.1.weight       # Query encoder layer 2 weights
â”‚   â”œâ”€â”€ query_tower.layers.1.bias         # Query encoder layer 2 biases
â”‚   â”œâ”€â”€ document_tower.layers.0.weight    # Document encoder layer 1 weights
â”‚   â”œâ”€â”€ document_tower.layers.0.bias      # Document encoder layer 1 biases
â”‚   â”œâ”€â”€ document_tower.layers.1.weight    # Document encoder layer 2 weights
â”‚   â”œâ”€â”€ document_tower.layers.1.bias      # Document encoder layer 2 biases
â”‚   â””â”€â”€ embedding.weight                  # Word embeddings (vocab_size Ã— embed_dim)
â”‚
â”œâ”€â”€ ðŸ“š embedding_matrix                    # Pre-trained embeddings tensor [vocab_size, embed_dim]
â”œâ”€â”€ ðŸ”¤ word_to_index                      # Dict: {"word": index, ...}
â”œâ”€â”€ ðŸ”¢ index_to_word                      # Dict: {index: "word", ...}
â”‚
â”œâ”€â”€ âš™ï¸  model_config                       # Model architecture parameters
â”‚   â”œâ”€â”€ hidden_dim: 256                   # Hidden layer dimension
â”‚   â”œâ”€â”€ num_layers: 2                     # Number of encoder layers
â”‚   â””â”€â”€ dropout: 0.1                     # Dropout rate
â”‚
â”œâ”€â”€ ðŸ“Š training_stats                     # Complete training history
â”‚   â”œâ”€â”€ ðŸŽ›ï¸  config                        # FULL SWEEP CONFIGURATION
â”‚   â”‚   â”œâ”€â”€ batch_size: 32               # Training batch size
â”‚   â”‚   â”œâ”€â”€ learning_rate: 0.0001        # Learning rate
â”‚   â”‚   â”œâ”€â”€ optimizer: "adamw"           # Optimizer type
â”‚   â”‚   â”œâ”€â”€ weight_decay: 1e-05          # L2 regularization
â”‚   â”‚   â”œâ”€â”€ beta1: 0.9                   # Adam beta1
â”‚   â”‚   â”œâ”€â”€ beta2: 0.999                 # Adam beta2
â”‚   â”‚   â”œâ”€â”€ loss_type: "contrastive"     # Loss function type
â”‚   â”‚   â”œâ”€â”€ temperature: 0.1             # Contrastive loss temperature
â”‚   â”‚   â”œâ”€â”€ margin: 0.2                  # Margin for triplet/margin losses
â”‚   â”‚   â”œâ”€â”€ scheduler: "cosine"          # Learning rate scheduler
â”‚   â”‚   â”œâ”€â”€ epochs: 5                    # Total training epochs
â”‚   â”‚   â”œâ”€â”€ hidden_dim: 256              # Architecture: hidden dimension
â”‚   â”‚   â”œâ”€â”€ num_layers: 2                # Architecture: number of layers
â”‚   â”‚   â”œâ”€â”€ dropout: 0.1                 # Architecture: dropout rate
â”‚   â”‚   â”œâ”€â”€ patience: 3                  # Early stopping patience
â”‚   â”‚   â”œâ”€â”€ grad_clip: 1.0               # Gradient clipping threshold
â”‚   â”‚   â”œâ”€â”€ eval_every_epoch: true       # Evaluation frequency
â”‚   â”‚   â”œâ”€â”€ eval_max_queries: 1000       # Max queries for evaluation
â”‚   â”‚   â”œâ”€â”€ triplets_file: "path/file"   # Training data path
â”‚   â”‚   â”œâ”€â”€ checkpoint_dir: "./checkpoints" # Model save directory
â”‚   â”‚   â”œâ”€â”€ evaluation_path: "./evaluation"  # Evaluation module path
â”‚   â”‚   â””â”€â”€ num_workers: 0               # Data loading workers
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“ˆ training_stats[]              # Per-epoch training metrics
â”‚   â”‚   â”œâ”€â”€ [epoch 1]
â”‚   â”‚   â”‚   â”œâ”€â”€ epoch: 1
â”‚   â”‚   â”‚   â”œâ”€â”€ train_loss: 0.245
â”‚   â”‚   â”‚   â”œâ”€â”€ train_accuracy: 0.673
â”‚   â”‚   â”‚   â”œâ”€â”€ learning_rate: 0.0001
â”‚   â”‚   â”‚   â”œâ”€â”€ eval_mrr: 0.234
â”‚   â”‚   â”‚   â”œâ”€â”€ eval_mrr@10: 0.234
â”‚   â”‚   â”‚   â”œâ”€â”€ eval_recall@10: 0.456
â”‚   â”‚   â”‚   â”œâ”€â”€ eval_ndcg@10: 0.312
â”‚   â”‚   â”‚   â””â”€â”€ eval_map: 0.198
â”‚   â”‚   â”œâ”€â”€ [epoch 2] {...}
â”‚   â”‚   â””â”€â”€ [epoch N] {...}
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“Š evaluation_stats[]            # Per-epoch detailed evaluation
â”‚   â”‚   â”œâ”€â”€ [epoch 1]
â”‚   â”‚   â”‚   â”œâ”€â”€ MRR: 0.234
â”‚   â”‚   â”‚   â”œâ”€â”€ MRR@10: 0.234
â”‚   â”‚   â”‚   â”œâ”€â”€ Recall@10: 0.456
â”‚   â”‚   â”‚   â”œâ”€â”€ NDCG@10: 0.312
â”‚   â”‚   â”‚   â”œâ”€â”€ MAP: 0.198
â”‚   â”‚   â”‚   â””â”€â”€ evaluation_type: "full_eval"
â”‚   â”‚   â””â”€â”€ [epoch N] {...}
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ† best_metrics                   # Best evaluation scores
â”‚   â”‚   â”œâ”€â”€ MRR: 0.3834                  # Best Mean Reciprocal Rank
â”‚   â”‚   â”œâ”€â”€ MRR@10: 0.3834               # Best MRR@10
â”‚   â”‚   â”œâ”€â”€ Recall@10: 0.6789            # Best Recall@10
â”‚   â”‚   â”œâ”€â”€ NDCG@10: 0.4567              # Best NDCG@10
â”‚   â”‚   â””â”€â”€ MAP: 0.2891                  # Best Mean Average Precision
â”‚   â”‚
â”‚   â””â”€â”€ ðŸ¥‡ best_epoch: 3                 # Epoch with best performance
â”‚
â”œâ”€â”€ ðŸ“… epoch: 3                          # Final epoch number
â”œâ”€â”€ ðŸ“Š eval_results                      # Final evaluation results (same as best_metrics)
â””â”€â”€ ðŸ·ï¸  wandb_run_id: "vivid-sweep-1"    # Weights & Biases run identifier
```


---

## Caching Documents into Redis Vector DB

```
Sample document IDs from SQLite:
  19699_0: Since 2007, the RBA's outstanding reputation has been affect...
  19699_1: The Reserve Bank of Australia (RBA) came into being on 14 Ja...
  19699_2: RBA Recognized with the 2014 Microsoft US Regional Partner o...
  19699_3: The inner workings of a rebuildable atomizer are surprisingl...
  19699_4: Results-Based AccountabilityÂ® (also known as RBA) is a disci...








```


---

## Evaluation

```
"evaluation_stats": [
    {
      "num_queries_evaluated": 484,
      "num_queries_with_relevance": 484,
      "MRR": 0.34510937295028205,
      "MRR@10": 0.34510937295028205,
      "MAP": 0.34510937295028205,
      "Recall@1": 0.12603305785123967,
      "NDCG@1": 0.12603305785123967,
      "Recall@5": 0.6714876033057852,
      "NDCG@5": 0.38945517026146864,
      "Recall@10": 1.0,
      "NDCG@10": 0.4986663919672747,
      "Recall@20": 1.0,
      "NDCG@20": 0.4986663919672747,
      "epoch": 1
    },
```


---

## Evaluation II

+ Alt model using CBOW embeds from Week 1

+ Two Tower (GRU) NN

```
"evaluation_stats": [
    {
      "Recall@10": 0.2241, (solid according to ChatGPT ðŸ˜Š)
      "Recall@5": 0.0131,
      "Recall@3": 0.0020,
    },
```

---

# ðŸ¤”
![Screenshot 2025-06-20 at 15.38.48](https://hackmd.io/_uploads/rk4KMgmNgx.png)

---

## Evaluation III

```
================================================================================
ðŸ† CHECKPOINT ANALYSIS RESULTS
================================================================================
ðŸ“Š Metric used: MRR
ðŸ“ Total checkpoints: 5
ðŸ“ˆ Checkpoints with metrics: 5

ðŸ¥‡ TOP 5 CHECKPOINTS:
------------------------------------------------------------------------------------------------------------------------
#   Checkpoint                               MRR      Recall@1 Recall@5 Recall@10  Size(MB)  Modified
------------------------------------------------------------------------------------------------------------------------
1   two_tower_best_true-sweep-1_20250620_084640.pt   0.4118   0.1667   0.7292   1.0000     185.2     2025-06-20 08:46
    ðŸ“‹ Training history: training_history_two_tower_best_true-sweep-1_20250620_084640.json
    ðŸ”„ Sweep: true-sweep-1_20250620

2   two_tower_model_20250619_085100.pt         0.3755   0.1250   0.7292   1.0000     190.7     2025-06-19 08:51
    ðŸ“‹ Training history: training_history_two_tower_model_20250619_085100.json

3   two_tower_best_true-sweep-1_20250620_065748.pt   0.3630   0.1458   0.6458   1.0000     185.2     2025-06-20 06:57
    ðŸ“‹ Training history: training_history_two_tower_best_true-sweep-1_20250620_065748.json
    ðŸ”„ Sweep: true-sweep-1_20250620

4   two_tower_best_epoch1_20250619_101644.pt   0.3451   0.1042   0.7083   1.0000     190.7     2025-06-19 10:16
    ðŸ“‹ Training history: training_history_two_tower_best_epoch1_20250619_101644.json
    ðŸ”„ Sweep: epoch1_20250619

5   two_tower_model_20250619_080117.pt         0.3237   0.1042   0.6458   1.0000     190.7     2025-06-19 08:01
    ðŸ“‹ Training history: training_history_two_tower_model_20250619_080117.json
```

---

## Model Permance (Charles)

Recall@50: 0.74
Recall@10: 0.23 ~ 0.45 
(lost track of hyper parameters, strangely MLP >> RNN)

![image](https://hackmd.io/_uploads/BJmcW-XEle.png)

---

### Inference (Charles)

Query: "cooking egg"
> 2025-06-20 15:26:40 --INFO-- Faiss search took 0.70 ms on GPU

- 1 [ **0.9026**, #12186]: 1 Remove the egg: ...

- 2 [**0.9021**, #11718]: 2 eggs. Pick and ...

- 3 [**0.8992**, #12104]: Rinse the pork, ...
- 4 [**0.8991**, #12066]: That's right, ... kitchen
- 5 [**0.8984**, #12186]: It ... 10 minutes ... an egg,
- ...

---

A colorful / colorcoded Terminal UI :100:
![image](https://hackmd.io/_uploads/Skxa1bWmNlx.png)

---


A colorful / colorcoded Terminal UI :100: 

![image](https://hackmd.io/_uploads/rJDteZQVxl.png)

---

## ðŸ” Inference: Query 1 â€“ _"How to train your model"_

**Top Results:**
- ðŸ“„ Use of interferon-gamma release assays (0.936)  
- ðŸ“„ Blood test metrics like PT and INR (0.935)  
- ðŸ“„ Facebook password reset guide (0.935)  

**âš ï¸ Observation:**  
Results are **off-topic** â€” medical and tech support content unrelated to model training.

---

## ðŸ” Inference: Query 2 â€“ _"Cooking an egg ðŸ³"_

**Top Results:**
- âœ… Cooking with olive oil and flour (0.933)  
- âœ… Soup recipe with ham and bay leaves (0.931)  
- âŒ Watermelon calorie info (0.931)  
- âŒ Wendyâ€™s sandwich nutrition (0.930)

**âœ… Observation:**  
Top results are **mostly relevant**, showing better alignment for everyday queries.

---

## ðŸ“Š What Did We Learn?

- âŒ Struggles with technical queries.
- âœ… Performs better on practical, simple tasks.
- ðŸ” Retrieval quality is **heavily data-dependent**.

---


Live Demos are idiotic
*(but we do it anyways)*

> uv run vector_db/unified_search.py --checkpoint $BEST_CHECKPOINT --config vector_db/redis_config.json --db data/documents.db --query "Cooking an egg"

