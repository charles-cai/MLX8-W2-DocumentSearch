{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c24b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c9376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Load MS MARCO V1.1 training dataset\n",
    "# -----------------------------\n",
    "\n",
    "# This will stream the data, you don't have to download the full file\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"train\")  # or \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ffdf890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building passage pool...: 100%|██████████| 82326/82326 [00:21<00:00, 3773.26it/s]\n",
      "Creating triples...:   1%|          | 999/82326 [00:20<27:26, 49.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example query: what is rba\n",
      "Relevant docs: [\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", 'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ', 'The inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is made of some type of resistance wire, normally Kanthal or nichrome. When a current is applied to the coil (resistance wire), it heats up and the heated coil then vaporizes the eliquid. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.', 'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.', 'RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The “end” or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the user is.', 'A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.', 'Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. ']\n",
      "Irrelevant docs: [\"If the ribs are raw, about 30 minutes turning every 5-6 minutes on a med-hot grill.   If you want good ribs y … ou will cook them low and slow with indirect heat of about 250-300 for about 3-5 hours. Once they start to pull back from the edge of the bone and can be pulled apart they are done. Take the ribs out of the fridge about 30 minutes before you plan to cook them, so they'll reach room temperature. About an hour before you're ready to cook, soak a couple wood chunks in water. Heat your charcoal in a chimney starter, then arrange your grill so that all the coals are one side.\", 'Look it up on the Australian Department of Immigration website. There is a long list of preferred immigrants, mainly tradespeople and some professionals. Unemployment is an issue here at the moment, so expect not to have a job for the first 6 months. Also in Sydney anyway, housing is expensive.', \"One huge disadvantage of SpeedTrader is that you need to make at least 15 trades per quarter or otherwise you'll be paying $120 ($195 for IRA) a year to maintain account at the firm. If you are not an active trader, you'll be better off with one of the companies in our Top Brokerage Firms list.\", 'Reservoir of infection: Any person, animal, plant, soil or substance in which an infectious agent normally lives and multiplies. The reservoir typically harbors the infectious agent without injury to itself and serves as a source from which other individuals can be infected.', 'Hemorrhagic Shock. Severe bleeding or loss of body fluid from trauma, burns, surgery, or dehydration from severe nausea and vomiting. Blood pressure decreases, thus blood flow is reduced to cells, tissue, and organs. \\ue041 \\ue042 \\ue012.', \"The new zombie drama nabs Season 2 pickup. The official Z Nation Twitter account has announced a second season renewal for the freshman series. Syfy's new post-zombie apocalypse drama, currently at the halfway point of the season, has gotten off to a solid start in the ratings since its debut last month. \", \"Black Sea Bass and Striped Bass are true Bass. White Sea Bass is actually a type of fish called a Drum, which is not really a Bass. The nearly extinct, but very popular Chilean Sea Bass is actually a Patagonian Toothfish; not something you'd order off the menu. Giant Sea Bass are Groupers. Anyway, you get the idea\", 'Food supplements also play an important role in gaining weight by adding lean muscle mass. However, food supplements alone cannot cause weight gain. In order to get maximum results, these supplements should be used along with other food items. Given below are the best supplements that should be consumed for gaining weight: 1. Protein Powder: Proteins are the building blocks of muscle tissue. Eating greater amounts of food and increasing your protein and calorie intake helps in building muscle mass and thus increasing your weight. It is not advisable to gorge on unhealthy trans-fats like fried chips, cookies and processed foods as these will cause obesity rather than building healthy', \"What You Pawn I Will Redeem. Edit 0 5 …. Summary: Sherman Alexie's short story What You Pawn I Will Redeem details a homeless, alcoholic Native American, Jackson Jackson, who finds his grandmother's powwow dance regalia at a pawn shop and goes on a quest to buy it back. The narrator of What You Pawn I Will Redeem discloses both his race and his homeless status in the first paragraph, saying I'm not going to tell you my particular reasons for being homeless, because it's my secret story, and Indians have to work hard to keep secrets from hungry white folks (Alexie 438).\", 'Between 1851 and 1871, the Australian population trebled as thousands of migrants – from Britain, China, America, France, Italy, Germany and Poland – arrived in search of gold. The first discoveries of payable gold were at Ophir in New South Wales and then at Ballarat and Bendigo Creek in Victoria. 1851: Gold rushes in New South Wales and Victoria begin. In 1851, gold-seekers from around the world began pouring into the colonies, changing the course of Australian history. The gold rushes greatly expanded Australia’s population, boosted its economy, and led to the emergence of a new national identity.']\n",
      "Selected gold doc: Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 2. Create triples of (query, relevant_docs, irrelevant_docs)\n",
    "# -----------------------------\n",
    "\n",
    "# 1. Build passage pool\n",
    "passage_to_idx = {}\n",
    "idx_to_passage = []\n",
    "\n",
    "for row in tqdm(dataset, desc=\"Building passage pool...\"):\n",
    "    for p in row['passages']['passage_text']:\n",
    "        if p not in passage_to_idx:\n",
    "            passage_to_idx[p] = len(idx_to_passage)\n",
    "            idx_to_passage.append(p)\n",
    "num_passages = len(idx_to_passage)\n",
    "\n",
    "# 2. Create triples and selected passages\n",
    "triples = []\n",
    "selected_passages = []\n",
    "\n",
    "for row in tqdm(dataset, desc=\"Creating triples...\"):\n",
    "    query = row['query']\n",
    "    passage_texts = row['passages']['passage_text']\n",
    "    is_selected = row['passages']['is_selected']\n",
    "\n",
    "    num_rels = len(passage_texts)\n",
    "    if num_rels == 0:\n",
    "        continue  # No passages for this query\n",
    "\n",
    "    # Gold passage: any with is_selected==1\n",
    "    gold_idxs = [i for i, flag in enumerate(is_selected) if flag == 1]\n",
    "    if gold_idxs:\n",
    "        selected_passages.append(passage_texts[gold_idxs[0]])\n",
    "    else:\n",
    "        selected_passages.append(None)\n",
    "\n",
    "    # Relevant docs: all passages for this query (usually 1-10, can be 0, already filtered)\n",
    "    relevant_passages = passage_texts\n",
    "\n",
    "    # Build mask to exclude relevant passages\n",
    "    rel_indices = [passage_to_idx[p] for p in passage_texts]\n",
    "    mask = np.ones(num_passages, dtype=bool)\n",
    "    mask[rel_indices] = False\n",
    "\n",
    "    # Sample matching number of irrels\n",
    "    if mask.sum() >= num_rels:\n",
    "        irrel_indices = np.random.choice(np.where(mask)[0], num_rels, replace=False)\n",
    "    else:\n",
    "        irrel_indices = np.random.choice(np.where(mask)[0], num_rels, replace=True)\n",
    "    irrelevant_passages = [idx_to_passage[i] for i in irrel_indices]\n",
    "\n",
    "    triples.append((query, relevant_passages, irrelevant_passages))\n",
    "    if len(triples) % 1000 == 0:\n",
    "        break\n",
    "\n",
    "# Save\n",
    "with open(\"triples_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(triples, f)\n",
    "with open(\"selected_passages.pkl\", \"wb\") as f:\n",
    "    pickle.dump(selected_passages, f)\n",
    "\n",
    "# Sanity check\n",
    "print(\"Example query:\", triples[0][0])\n",
    "print(\"Relevant docs:\", triples[0][1])      # List of N passages (N varies)\n",
    "print(\"Irrelevant docs:\", triples[0][2])    # List of N passages (N = len(rels))\n",
    "print(\"Selected gold doc:\", selected_passages[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1b. Load triples & selected docs from file\n",
    "# -------------------------------\n",
    "\n",
    "# Load triples\n",
    "with open(\"triples_full.pkl\", \"rb\") as f:\n",
    "    triples = pickle.load(f)\n",
    "\n",
    "with open(\"selected_passages.pkl\", \"rb\") as g:\n",
    "    selected_passages= pickle.load(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1c. Tokenize triples\n",
    "# -------------------------------\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9 ]+', '', text)\n",
    "    return text.split()\n",
    "\n",
    "tokenized_triples = []\n",
    "for query, rel_docs, irrel_docs in tqdm(triples, desc=\"Tokenizing triples\"):\n",
    "    tokenized_query = preprocess(query)\n",
    "    tokenized_rels = [preprocess(doc) for doc in rel_docs]\n",
    "    tokenized_irrels = [preprocess(doc) for doc in irrel_docs]\n",
    "    tokenized_triples.append((tokenized_query, tokenized_rels, tokenized_irrels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96b5f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Load Vocabulary & Pre-trained Embeddings\n",
    "# -----------------------------\n",
    "with open(\"vocab_new.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_to_ix = json.load(f)\n",
    "\n",
    "ix_to_word = {int(i): w for w, i in word_to_ix.items()}\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "embed_dim = 200  \n",
    "state = torch.load(\"text8_cbow_embeddings.pth\", map_location='cpu')  # Shape: [vocab_size, embed_dim]\n",
    "embeddings = state[\"embeddings.weight\"] \n",
    "\n",
    "assert embeddings.shape[0] == vocab_size, \"Vocab size mismatch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25c2e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. CBOW Model\n",
    "# -----------------------------\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).mean(dim=1)\n",
    "        return self.linear(embeds)\n",
    "\n",
    "cbow_model = CBOW(vocab_size, embed_dim)\n",
    "cbow_model.embeddings.weight.data.copy_(embeddings)\n",
    "cbow_model.embeddings.weight.requires_grad = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920cd3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CBOW embedding + streaming: 100%|██████████| 5120/5120 [00:36<00:00, 141.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 4. Embed queries, rel and irrel documents using pre-trained CBOW model\n",
    "# -------------------------------\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "#torch.set_num_threads(12)\n",
    "\n",
    "batch_size = 64  # adjust to taste; big enough to speed up, small enough to never threaten RAM\n",
    "\n",
    "def process_and_save_embeddings(\n",
    "    tokenized_triples, word_to_ix, cbow_model, \n",
    "    query_embeds_path, rel_doc_embeds_path, irrel_doc_embeds_path,\n",
    "    batch_size=64\n",
    "):\n",
    "    query_embeds_batch = []\n",
    "    rel_doc_embeds_batch = []\n",
    "    irrel_doc_embeds_batch = []\n",
    "    \n",
    "    for i, (tokenized_query, tokenized_rels, tokenized_irrels) in enumerate(\n",
    "        tqdm(tokenized_triples, desc=\"CBOW embedding + streaming\", total=len(tokenized_triples))\n",
    "    ):\n",
    "        # Query: embeddings per token\n",
    "        q_ids = [word_to_ix[t] for t in tokenized_query if t in word_to_ix]\n",
    "        if q_ids:\n",
    "            with torch.no_grad():\n",
    "                q_vecs = cbow_model.embeddings(torch.tensor(q_ids))\n",
    "            query_embeds_batch.append(q_vecs)  # [q_len, embed_dim]\n",
    "        else:\n",
    "            query_embeds_batch.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "        \n",
    "        # Relevant docs: list of (doc_len, embed_dim)\n",
    "        rel_embs = []\n",
    "        for doc_tokens in tokenized_rels:\n",
    "            doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "            if doc_ids:\n",
    "                with torch.no_grad():\n",
    "                    doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "                rel_embs.append(doc_vecs)\n",
    "            else:\n",
    "                rel_embs.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "        rel_doc_embeds_batch.append(rel_embs)\n",
    "\n",
    "        # Irrelevant docs: list of (doc_len, embed_dim)\n",
    "        irrel_embs = []\n",
    "        for doc_tokens in tokenized_irrels:\n",
    "            doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "            if doc_ids:\n",
    "                with torch.no_grad():\n",
    "                    doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "                irrel_embs.append(doc_vecs)\n",
    "            else:\n",
    "                irrel_embs.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "        irrel_doc_embeds_batch.append(irrel_embs)\n",
    "\n",
    "        # Save every batch_size triples\n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == len(tokenized_triples):\n",
    "            with open(query_embeds_path, 'ab') as fq:\n",
    "                pickle.dump(query_embeds_batch, fq)\n",
    "            with open(rel_doc_embeds_path, 'ab') as fr:\n",
    "                pickle.dump(rel_doc_embeds_batch, fr)\n",
    "            with open(irrel_doc_embeds_path, 'ab') as fi:\n",
    "                pickle.dump(irrel_doc_embeds_batch, fi)\n",
    "\n",
    "            # Free RAM\n",
    "            query_embeds_batch.clear()\n",
    "            rel_doc_embeds_batch.clear()\n",
    "            irrel_doc_embeds_batch.clear()\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "process_and_save_embeddings(\n",
    "    tokenized_triples[:5120], word_to_ix, cbow_model,\n",
    "    \"query_embeds.pkl\", \"rel_doc_embeds.pkl\", \"irrel_doc_embeds.pkl\",\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# # Usage:\n",
    "# process_and_save_embeddings(\n",
    "#     tokenized_triples, word_to_ix, cbow_model,\n",
    "#     \"query_embeds.pkl\", \"rel_doc_embeds.pkl\", \"irrel_doc_embeds.pkl\",\n",
    "#     batch_size=2048\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Load Embeddings\n",
    "# -----------------------------\n",
    "\n",
    "def load_all_batches(path):\n",
    "    all_data = []\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                batch = pickle.load(f)\n",
    "                all_data.extend(batch)  # for lists of tensors, this flattens batches\n",
    "            except EOFError:\n",
    "                break\n",
    "    return all_data\n",
    "\n",
    "# Load them all\n",
    "query_embeds = load_all_batches(\"query_embeds.pkl\")           # list of [query_len, embed_dim] tensors\n",
    "rel_doc_embeds = load_all_batches(\"rel_doc_embeds.pkl\")       # list of lists: each is [num_docs] of [doc_len, embed_dim] tensors\n",
    "irrel_doc_embeds = load_all_batches(\"irrel_doc_embeds.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2822065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Define distance function & Triplet loss function\n",
    "# -----------------------------\n",
    "\n",
    "# Cosine similarity for calculation of cosine distance\n",
    "def cosine_similarity(x, y):\n",
    "    return F.cosine_similarity(x, y, dim=1)\n",
    "\n",
    "# Triplet loss function - will compute the loss for a batch of triplets\n",
    "def triplet_loss_function(query, relevant_doc, irrelevant_doc, distance_function, margin):\n",
    "    rel_dist = distance_function(query, relevant_doc)         # (batch,)\n",
    "    irrel_dist = distance_function(query, irrelevant_doc)     # (batch,)\n",
    "    triplet_loss = torch.relu(rel_dist - irrel_dist + margin)\n",
    "    return triplet_loss.mean()                                # Average over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b1c3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Define Two Tower Model (QueryTower and DocTower)\n",
    "# -----------------------------\n",
    "\n",
    "class QueryTower(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers=1, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown rnn_type: choose 'gru' or 'lstm'\")\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (batch, seq_len, embed_dim)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, h = self.rnn(packed)\n",
    "        if isinstance(h, tuple):  # LSTM\n",
    "            h = h[0]\n",
    "        return h[-1]  # (batch, hidden_dim)\n",
    "\n",
    "class DocTower(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers=1, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown rnn_type: choose 'gru' or 'lstm'\")\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, h = self.rnn(packed)\n",
    "        if isinstance(h, tuple):\n",
    "            h = h[0]\n",
    "        return h[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbab9d09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_embeds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Create the dataset and dataloader for batching\u001b[39;00m\n\u001b[32m     38\u001b[39m batch_size = \u001b[32m64\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m dataset = TripleDataset(\u001b[43mquery_embeds\u001b[49m, rel_doc_embeds, irrel_doc_embeds)\n\u001b[32m     40\u001b[39m dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn=collate_fn)\n",
      "\u001b[31mNameError\u001b[39m: name 'query_embeds' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6. Prepare dataset for DataLoader\n",
    "# -----------------------------\n",
    "\n",
    "def collate_fn(batch):\n",
    "    q_seqs, r_seqs, i_seqs = zip(*batch)\n",
    "    q_lens = [x.shape[0] for x in q_seqs]\n",
    "    r_lens = [x.shape[0] for x in r_seqs]\n",
    "    i_lens = [x.shape[0] for x in i_seqs]\n",
    "    q_padded = pad_sequence(q_seqs, batch_first=True)\n",
    "    r_padded = pad_sequence(r_seqs, batch_first=True)\n",
    "    i_padded = pad_sequence(i_seqs, batch_first=True)\n",
    "    return q_padded, r_padded, i_padded, q_lens, r_lens, i_lens\n",
    "\n",
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, X_queries, X_rels, X_irrels):\n",
    "        self.X_queries = X_queries    # list of N_query tensors\n",
    "        self.X_rels = X_rels          # list of lists (variable-length)\n",
    "        self.X_irrels = X_irrels      # list of lists (same variable-length)\n",
    "        self.pairs = []\n",
    "        for i in range(len(self.X_queries)):\n",
    "            n = len(self.X_rels[i])\n",
    "            for j in range(n):\n",
    "                self.pairs.append((i, j))  # (query idx, doc pair idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_idx, d_idx = self.pairs[idx]\n",
    "        qry = self.X_queries[q_idx]\n",
    "        rel = self.X_rels[q_idx][d_idx]\n",
    "        irrel = self.X_irrels[q_idx][d_idx]\n",
    "        return qry, rel, irrel\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader for batching\n",
    "batch_size = 64\n",
    "dataset = TripleDataset(query_embeds, rel_doc_embeds, irrel_doc_embeds)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44335531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 1600/1600 [10:28<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Loss: 0.0916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 1600/1600 [08:48<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Loss: 0.0322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 1600/1600 [09:45<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Loss: 0.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 1600/1600 [10:03<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Loss: 0.0088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 1600/1600 [11:44<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Loss: 0.0055\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 7. Train the model\n",
    "# -----------------------------\n",
    "\n",
    "# embed_dim defined in section 3.\n",
    "\n",
    "# Define hyperparameters\n",
    "hidden_dim = 128  # Dimension of the hidden state in RNNs (GRU/LSTM - can be adjusted)\n",
    "margin = 0.2  # Margin for triplet loss (can be adjusted)\n",
    "distance_function = cosine_similarity)\n",
    "\n",
    "qry_tower = QueryTower(embed_dim, hidden_dim, rnn_type='gru')  # or 'lstm'\n",
    "doc_tower = DocTower(embed_dim, hidden_dim, rnn_type='gru')  # or 'lstm'\n",
    "\n",
    "optimizer = torch.optim.Adam(list(qry_tower.parameters()) + list(doc_tower.parameters()), lr=1e-3)\n",
    "num_epochs = 5  # Set as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        qry_embeds, rel_embeds, irrel_embeds, q_lens, r_lens, i_lens = batch\n",
    "\n",
    "        # For each sample in the batch, check if ANY of qry, rel, irrel is all zeros\n",
    "        qry_zero_mask = torch.all(qry_embeds == 0, dim=(1,2))\n",
    "        rel_zero_mask = torch.all(rel_embeds == 0, dim=(1,2))\n",
    "        irrel_zero_mask = torch.all(irrel_embeds == 0, dim=(1,2))\n",
    "\n",
    "        mask = ~(qry_zero_mask | rel_zero_mask | irrel_zero_mask)\n",
    "\n",
    "        if mask.sum() == 0:\n",
    "            continue  # All are bad samples\n",
    "\n",
    "        qry_embeds   = qry_embeds[mask]\n",
    "        rel_embeds   = rel_embeds[mask]\n",
    "        irrel_embeds = irrel_embeds[mask]\n",
    "        q_lens = [q_lens[i] for i in range(len(mask)) if mask[i]]\n",
    "        r_lens = [r_lens[i] for i in range(len(mask)) if mask[i]]\n",
    "        i_lens = [i_lens[i] for i in range(len(mask)) if mask[i]]\n",
    "\n",
    "        qry_vecs    = qry_tower(qry_embeds, q_lens)\n",
    "        rel_vecs    = doc_tower(rel_embeds, r_lens)\n",
    "        irrel_vecs  = doc_tower(irrel_embeds, i_lens)\n",
    "\n",
    "        loss = triplet_loss_function(\n",
    "            qry_vecs, rel_vecs, irrel_vecs,\n",
    "            distance_function=distance_function,\n",
    "            margin=margin\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "# Save final model after all epochs are done\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'qry_tower_state_dict': qry_tower.state_dict(),\n",
    "    'doc_tower_state_dict': doc_tower.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': epoch_loss,  # from last epoch\n",
    "}, \"two_tower_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9129194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8. Load the trained model\n",
    "# -----------------------------\n",
    "hidden_dim = 128\n",
    "embed_dim = 200\n",
    "# 1. Define your model classes (as in your code)\n",
    "qry_tower = QueryTower(embed_dim, hidden_dim, num_layers=1, rnn_type='gru')\n",
    "doc_tower = DocTower(embed_dim, hidden_dim, num_layers=1, rnn_type='gru')\n",
    "\n",
    "# 2. Create optimizer with the same params as training\n",
    "optimizer = torch.optim.Adam(list(qry_tower.parameters()) + list(doc_tower.parameters()), lr=1e-3)\n",
    "\n",
    "# 3. Load checkpoint\n",
    "checkpoint = torch.load(\"two_tower_final.pt\", map_location=\"cpu\")  # or map_location=\"cuda\" if using GPU\n",
    "\n",
    "# 4. Restore state dicts\n",
    "qry_tower.load_state_dict(checkpoint['qry_tower_state_dict'])\n",
    "doc_tower.load_state_dict(checkpoint['doc_tower_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# 5. (Optional) get other info\n",
    "epoch = checkpoint['epoch']\n",
    "last_loss = checkpoint['loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac1598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building passage pool...: 100%|██████████| 10047/10047 [00:01<00:00, 5255.59it/s]\n",
      "Creating validation triples...: 100%|██████████| 10047/10047 [00:18<00:00, 532.19it/s]\n",
      "Tokenizing triples: 100%|██████████| 1000/1000 [00:01<00:00, 958.37it/s]\n",
      "CBOW embedding + streaming: 100%|██████████| 1000/1000 [00:06<00:00, 148.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example validation query: walgreens store sales average\n",
      "Relevant docs: ['The average Walgreens salary ranges from approximately $15,000 per year for Customer Service Associate / Cashier to $179,900 per year for District Manager. Average Walgreens hourly pay ranges from approximately $7.35 per hour for Laboratory Technician to $68.90 per hour for Pharmacy Manager. Salary information comes from 7,810 data points collected directly from employees, users, and jobs on Indeed.', 'The average revenue in 2011 of a Starbuck Store was $1,078,000, up  from $1,011,000 in 2010.    The average ticket (total purchase) at domestic Starbuck stores in  No … vember 2007 was reported at $6.36.    In 2008, the average ticket was flat (0.0% change).', 'In fiscal 2014, Walgreens opened a total of 184 new locations and acquired 84 locations, for a net decrease of 273 after relocations and closings. How big are your stores? The average size for a typical Walgreens is about 14,500 square feet and the sales floor averages about 11,000 square feet. How do we select locations for new stores? There are several factors that Walgreens takes into account, such as major intersections, traffic patterns, demographics and locations near hospitals.', 'th store in 1984, reaching $4 billion in sales in 1987, and $5 billion two years later. Walgreens ended the 1980s with 1,484 stores, $5.3 billion in revenues and $154 million in profits. However, profit margins remained just below 3 percent of sales, and returns on assets of less than 10 percent.', 'The number of Walgreen stores has risen from 5,000 in 2005 to more than 8,000 at present. The average square footage per store stood at approximately 10,200 and we forecast the figure to remain constant over our review period. Walgreen earned $303 as average front-end revenue per store square foot in 2012.', 'Your Walgreens Store. Select a store from the search results to make it Your Walgreens Store and save time getting what you need. Your Walgreens Store will be the default location for picking up prescriptions, photos, in store orders and finding deals in the Weekly Ad.']\n",
      "Irrelevant docs: [\"Filial piety is a very important part of Chinese culture and it is deeply ingrained in Chinese people's minds. In many ways this dedication to the well-being of one's parents is admirable and certainly provides stability and harmony in families as well as in society as a whole. The concept of filial piety is closely related to other Confucian concepts, including ren and yi. For instance, if a child is lacking in ren, then his elders have a responsibility in terms of xiao to help him develop it. These concepts can even trump other obligations, in some cases.\", 'Definition. Staph infections are caused by staphylococcus bacteria, types of germs commonly found on the skin or in the nose of even healthy individuals. Most of the time, these bacteria cause no problems or result in relatively minor skin infections. ', \"At the heart of DNA evidence is the biological molecule itself, which serves as an instruction manual and blueprint for everything in your body (see How Cells Work for details). A DNA molecule is a long, twisting chain known as a double helix. Since then, DNA evidence has played a bigger and bigger role in many nations' criminal justice systems. It has been used to prove that suspects were involved in crimes and to free people who were wrongly convicted.\", \"A. Lupus (systemic lupus erythematosus or SLE) is a condition of chronic inflammation caused by an autoimmune disease. Autoimmune diseases are illnesses that occur when the body's tissues are attacked by its own immune system. The precise reason for the abnormal autoimmunity that causes lupus is not known. Inherited genes, viruses, ultraviolet light, and drugs may all play some role.\", 'I usually cook porch chops in the oven at 350 degrees for 30 minutes to one hour. This depends upon the thickness of the pork chop. Also, always thaw the porck chops before baking them.', 'Maldon (Population 1,500) is located 135km northwest of Melbourne in the heart of the Victorian Goldfields. The town is nestled on the slopes of Mt. Tarrangower in an agricultural, pastoral and mining district. It has retained an authentic pioneer-like appearance with history of the gold-mining era being preserved.']\n",
      "Selected gold doc: The average Walgreens salary ranges from approximately $15,000 per year for Customer Service Associate / Cashier to $179,900 per year for District Manager. Average Walgreens hourly pay ranges from approximately $7.35 per hour for Laboratory Technician to $68.90 per hour for Pharmacy Manager. Salary information comes from 7,810 data points collected directly from employees, users, and jobs on Indeed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9. Load MS MARCO V1.1 validation dataset, process into triples and tokenize & embed\n",
    "# -----------------------------\n",
    "\n",
    "# Load MS MARCO validation split\n",
    "val_dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")\n",
    "\n",
    "# 1. Build passage pool (same as training)\n",
    "passage_to_idx = {}\n",
    "idx_to_passage = []\n",
    "for row in tqdm(val_dataset, desc=\"Building passage pool...\"):\n",
    "    for p in row['passages']['passage_text']:\n",
    "        if p not in passage_to_idx:\n",
    "            passage_to_idx[p] = len(idx_to_passage)\n",
    "            idx_to_passage.append(p)\n",
    "num_passages = len(idx_to_passage)\n",
    "\n",
    "# 2. Create triples and selected passages\n",
    "triples_val = []\n",
    "selected_passages_val = []\n",
    "\n",
    "for row in tqdm(val_dataset, desc=\"Creating validation triples...\"):\n",
    "    query = row['query']\n",
    "    passage_texts = row['passages']['passage_text']\n",
    "    is_selected = row['passages']['is_selected']\n",
    "\n",
    "    num_rels = len(passage_texts)\n",
    "    if num_rels == 0:\n",
    "        continue  # No passages for this query\n",
    "\n",
    "    # Gold passage: any with is_selected==1\n",
    "    gold_idxs = [i for i, flag in enumerate(is_selected) if flag == 1]\n",
    "    if gold_idxs:\n",
    "        selected_passages_val.append(passage_texts[gold_idxs[0]])\n",
    "    else:\n",
    "        selected_passages_val.append(None)\n",
    "\n",
    "    # Relevant docs: all passages for this query\n",
    "    relevant_passages = passage_texts\n",
    "\n",
    "    # Build mask to exclude relevant passages\n",
    "    rel_indices = [passage_to_idx[p] for p in passage_texts]\n",
    "    mask = np.ones(num_passages, dtype=bool)\n",
    "    mask[rel_indices] = False\n",
    "\n",
    "    # Sample matching number of irrels\n",
    "    if mask.sum() >= num_rels:\n",
    "        irrel_indices = np.random.choice(np.where(mask)[0], num_rels, replace=False)\n",
    "    else:\n",
    "        irrel_indices = np.random.choice(np.where(mask)[0], num_rels, replace=True)\n",
    "    irrelevant_passages = [idx_to_passage[i] for i in irrel_indices]\n",
    "\n",
    "    triples_val.append((query, relevant_passages, irrelevant_passages))\n",
    "\n",
    "# Save\n",
    "with open(\"triples_val.pkl\", \"wb\") as f:\n",
    "    pickle.dump(triples_val, f)\n",
    "with open(\"selected_passages_val.pkl\", \"wb\") as f:\n",
    "    pickle.dump(selected_passages_val, f)\n",
    "\n",
    "\n",
    "# Load triples & selected docs\n",
    "with open(\"triples_val.pkl\", \"rb\") as f:\n",
    "    triples_val = pickle.load(f)\n",
    "\n",
    "with open(\"selected_passages_val.pkl\", \"rb\") as g:\n",
    "    selected_passages_val = pickle.load(g)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9 ]+', '', text)\n",
    "    return text.split()\n",
    "\n",
    "tokenized_triples_val = []\n",
    "for query, rel_docs, irrel_docs in tqdm(triples, desc=\"Tokenizing triples\"):\n",
    "    tokenized_query = preprocess(query)\n",
    "    tokenized_rels = [preprocess(doc) for doc in rel_docs]\n",
    "    tokenized_irrels = [preprocess(doc) for doc in irrel_docs]\n",
    "    tokenized_triples_val.append((tokenized_query, tokenized_rels, tokenized_irrels))\n",
    "\n",
    "\n",
    "batch_size = 64  # adjust to taste; big enough to speed up, small enough to never threaten RAM\n",
    "\n",
    "\n",
    "def process_and_save_embeddings(\n",
    "    tokenized_triples_val, word_to_ix, cbow_model, \n",
    "    query_embeds_path, rel_doc_embeds_path, irrel_doc_embeds_path,\n",
    "    batch_size=64\n",
    "):\n",
    "    query_embeds_batch = []\n",
    "    rel_doc_embeds_batch = []\n",
    "    irrel_doc_embeds_batch = []\n",
    "    \n",
    "    for i, (tokenized_query, tokenized_rels, tokenized_irrels) in enumerate(\n",
    "        tqdm(tokenized_triples_val, desc=\"CBOW embedding + streaming\", total=len(tokenized_triples_val))\n",
    "    ):\n",
    "        # Query: embeddings per token\n",
    "        q_ids = [word_to_ix[t] for t in tokenized_query if t in word_to_ix]\n",
    "        if q_ids:\n",
    "            with torch.no_grad():\n",
    "                q_vecs = cbow_model.embeddings(torch.tensor(q_ids))\n",
    "            query_embeds_batch.append(q_vecs)  # shape: [query_len, embed_dim]\n",
    "        else:\n",
    "            query_embeds_batch.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "        \n",
    "        # Relevant docs: list of (doc_len, embed_dim)\n",
    "        rel_embs = []\n",
    "        for doc_tokens in tokenized_rels:\n",
    "            doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "            if doc_ids:\n",
    "                with torch.no_grad():\n",
    "                    doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "                rel_embs.append(doc_vecs)\n",
    "            else:\n",
    "                rel_embs.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "        rel_doc_embeds_batch.append(rel_embs)\n",
    "\n",
    "        # Irrelevant docs: list of (doc_len, embed_dim)\n",
    "        irrel_embs = []\n",
    "        for doc_tokens in tokenized_irrels:\n",
    "            doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "            if doc_ids:\n",
    "                with torch.no_grad():\n",
    "                    doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "                irrel_embs.append(doc_vecs)\n",
    "            else:\n",
    "                irrel_embs.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "        irrel_doc_embeds_batch.append(irrel_embs)\n",
    "\n",
    "        # Save every batch_size triples\n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == len(tokenized_triples):\n",
    "            with open(query_embeds_path, 'ab') as fq:\n",
    "                pickle.dump(query_embeds_batch, fq)\n",
    "            with open(rel_doc_embeds_path, 'ab') as fr:\n",
    "                pickle.dump(rel_doc_embeds_batch, fr)\n",
    "            with open(irrel_doc_embeds_path, 'ab') as fi:\n",
    "                pickle.dump(irrel_doc_embeds_batch, fi)\n",
    "\n",
    "            query_embeds_batch.clear()\n",
    "            rel_doc_embeds_batch.clear()\n",
    "            irrel_doc_embeds_batch.clear()\n",
    "\n",
    "# Usage example:\n",
    "process_and_save_embeddings(\n",
    "    tokenized_triples_val, word_to_ix, cbow_model,\n",
    "    \"query_embeds_val.pkl\", \"rel_doc_embeds_val.pkl\", \"irrel_doc_embeds_val.pkl\",\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0e9610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 9b. Load validation data embeddings\n",
    "# -----------------------------\n",
    "\n",
    "# Load them all\n",
    "query_embeds_val = load_all_batches(\"query_embeds_val.pkl\")           # list of [query_len, embed_dim] tensors\n",
    "rel_doc_embeds_val = load_all_batches(\"rel_doc_embeds_val.pkl\")       # list of lists: each is [num_docs] of [doc_len, embed_dim] tensors\n",
    "irrel_doc_embeds_val = load_all_batches(\"irrel_doc_embeds_val.pkl\")\n",
    "\n",
    "val_data = []\n",
    "for i in range(len(query_embeds_val)):\n",
    "    q_embed = query_embeds_val[i]\n",
    "    rel_embeds = rel_doc_embeds_val[i]         # List of (rel_len, embed_dim)\n",
    "    irrel_embeds = irrel_doc_embeds_val[i]     # List of (irrel_len, embed_dim)\n",
    "    val_data.append((q_embed, rel_embeds, irrel_embeds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c8edefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8. Evaluate the model using Recall@K\n",
    "# -----------------------------\n",
    "\n",
    "def evaluate_model(\n",
    "    qry_tower, doc_tower, val_data, selected_passages_val, rel_doc_texts_val, irrel_doc_texts_val,\n",
    "    distance_fn, K=1, device=\"cpu\"\n",
    "):\n",
    "    qry_tower.eval()\n",
    "    doc_tower.eval()\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (q_embed, rel_embeds, irrel_embeds) in enumerate(val_data):\n",
    "            # Skip queries with no gold passage\n",
    "            gold_text = selected_passages_val[i]\n",
    "            if gold_text is None:\n",
    "                continue\n",
    "\n",
    "            # Build all candidate docs and their texts\n",
    "            candidate_embeds = rel_embeds + irrel_embeds\n",
    "            candidate_texts = rel_doc_texts_val[i] + irrel_doc_texts_val[i]  # rels + irrels\n",
    "\n",
    "            # Find gold doc index among candidates\n",
    "            try:\n",
    "                gold_idx = candidate_texts.index(gold_text)\n",
    "            except ValueError:\n",
    "                # Gold doc not among candidates\n",
    "                continue\n",
    "\n",
    "            all_doc_tensors = [doc.to(device) for doc in candidate_embeds]\n",
    "            doc_lens = [doc.shape[0] for doc in all_doc_tensors]\n",
    "            padded_docs = torch.nn.utils.rnn.pad_sequence(all_doc_tensors, batch_first=True)\n",
    "\n",
    "            # Encode query\n",
    "            q_input = q_embed.unsqueeze(0).to(device)\n",
    "            q_len = [q_embed.shape[0]]\n",
    "            q_vec = qry_tower(q_input, q_len)\n",
    "\n",
    "            # Encode all docs in batch\n",
    "            d_vecs = doc_tower(padded_docs, doc_lens)\n",
    "\n",
    "            # Compute distances (query vs. all docs)\n",
    "            sims = distance_fn(q_vec.repeat(len(doc_lens), 1), d_vecs)  # (num_candidates,)\n",
    "            sorted_indices = torch.argsort(sims, descending=True)  # Smallest = most similar\n",
    "\n",
    "            # Recall@K: Is gold doc in top K?\n",
    "            if gold_idx in sorted_indices[:K]:\n",
    "                num_correct += 1\n",
    "            total += 1\n",
    "\n",
    "    recall_at_k = num_correct / total if total > 0 else 0.0\n",
    "    return recall_at_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_doc_texts_val = [rels for _, rels, _ in triples_val]\n",
    "irrel_doc_texts_val = [irrels for _, _, irrels in triples_val]\n",
    "\n",
    "\n",
    "recall = evaluate_model(\n",
    "    qry_tower, doc_tower, val_data, selected_passages_val, rel_doc_texts_val, irrel_doc_texts_val,\n",
    "    distance_fn=cosine_similarity, K=1, device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f\"Recall@1: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05505892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 11. Inference: Encode documents and queries, find top-k relevant docs\n",
    "# -----------------------------\n",
    "\n",
    "def encode_documents(doc_tower, all_doc_embeds, all_doc_lens, device='cpu', batch_size=128):\n",
    "    doc_tower.eval()\n",
    "    all_vecs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(all_doc_embeds), batch_size):\n",
    "            batch_embeds = all_doc_embeds[i:i+batch_size].to(device)\n",
    "            batch_lens = all_doc_lens[i:i+batch_size]\n",
    "            vecs = doc_tower(batch_embeds, batch_lens)  # Shape: (batch, hidden_dim)\n",
    "            all_vecs.append(vecs.cpu())\n",
    "    return torch.cat(all_vecs, dim=0)  # Shape: (num_docs, hidden_dim)\n",
    "\n",
    "def encode_query(qry_tower, query_embed, query_len, device='cpu'):\n",
    "    qry_tower.eval()\n",
    "    with torch.no_grad():\n",
    "        query_vec = qry_tower(query_embed.to(device), query_len)\n",
    "    return query_vec.cpu()  # Shape: (1, hidden_dim)\n",
    "\n",
    "def find_top_k(query_vec, doc_vecs, k=5, distance_fn=None, similarity=True):\n",
    "    # query_vec: (1, hidden_dim), doc_vecs: (num_docs, hidden_dim)\n",
    "    if distance_fn is None:\n",
    "        # Default: cosine similarity\n",
    "        sims = F.cosine_similarity(query_vec, doc_vecs)\n",
    "        topk = torch.topk(sims, k)  # highest similarity\n",
    "        indices = topk.indices.cpu().numpy()\n",
    "        scores = topk.values.cpu().numpy()\n",
    "        return indices, scores\n",
    "    else:\n",
    "        vals = distance_fn(query_vec.expand_as(doc_vecs), doc_vecs)\n",
    "        if similarity:\n",
    "            # Higher value = more similar (like cosine similarity)\n",
    "            topk = torch.topk(vals, k)  # highest similarity\n",
    "            indices = topk.indices.cpu().numpy()\n",
    "            scores = topk.values.cpu().numpy()\n",
    "            return indices, scores\n",
    "        else:\n",
    "            # Lower value = more similar (like distances)\n",
    "            topk = torch.topk(-vals, k)  # lowest distance\n",
    "            indices = topk.indices.cpu().numpy()\n",
    "            scores = -topk.values.cpu().numpy()\n",
    "            return indices, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a33a854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_doc_embeds shape: torch.Size([51200, 175, 200])\n",
      "all_doc_lens shape: torch.Size([51200])\n",
      "all_doc_texts length: 823260\n",
      "all_query_texts length: 82326\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 12. Calculate all document embeddings and lengths\n",
    "# -----------------------------\n",
    "\n",
    "num_docs = 10  # Number of relevant docs per query\n",
    "\n",
    "all_doc_texts = []  # To hold all relevant doc texts for padding\n",
    "all_query_texts = []\n",
    "\n",
    "for query, rels, irrels in triples:\n",
    "    all_query_texts.append(query)\n",
    "    # rels is the list of relevant doc texts for this query\n",
    "    # We slice to num_docs to match the embedding logic, then pad if needed\n",
    "    for doc in rels[:num_docs]:\n",
    "        all_doc_texts.append(doc)\n",
    "    # Pad if fewer than num_docs\n",
    "    while len(rels) < num_docs:\n",
    "        all_doc_texts.append(\"\")\n",
    "        rels.append(\"\")  # So embedding and text padding always match\n",
    "\n",
    "# Step 1: Gather your document embedding sequences and their lengths\n",
    "all_doc_embeds_list = []\n",
    "all_doc_lens_list = []\n",
    "\n",
    "for batch in dataloader:\n",
    "    rel_embeds, r_lens = batch[1], batch[4]  # rel_embeds: (batch, seq_len, embed_dim)\n",
    "    for i in range(rel_embeds.shape[0]):\n",
    "        all_doc_embeds_list.append(rel_embeds[i])  # (seq_len, embed_dim)\n",
    "        all_doc_lens_list.append(r_lens[i])\n",
    "\n",
    "# Step 2: Pad all embeddings to max seq_len\n",
    "# pad_sequence wants a list of (seq_len, embed_dim), returns (max_seq_len, num_docs, embed_dim)\n",
    "padded = pad_sequence(all_doc_embeds_list, batch_first=True)  # (num_docs, max_seq_len, embed_dim)\n",
    "\n",
    "all_doc_embeds = padded  # (num_docs, max_seq_len, embed_dim)\n",
    "all_doc_lens = torch.tensor(all_doc_lens_list)\n",
    "\n",
    "print(\"all_doc_embeds shape:\", all_doc_embeds.shape)\n",
    "print(\"all_doc_lens shape:\", all_doc_lens.shape)\n",
    "print(\"all_doc_texts length:\", len(all_doc_texts))\n",
    "print(\"all_query_texts length:\", len(all_query_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c68b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  in animals somatic cells are produced by and gametic cells are produced by\n",
      "Top document matches:\n",
      "1: To get your learner's permit, you'll need to visit your local Alaska DMV office and: 1  Be at least 14 years old. 2  Have your parent/legal guardian complete a Parent/Guardian Consent for a Minor (Form 433). 3  Complete an Application for Alaska Driver License, Permit or Identification Card (Form 478). Unrestricted Alaska Driver's License. You are eligible for an unrestricted driver's license after you hold your provisional license for at least 6 months OR until you are 18 years old. If you decide to upgrade your driver's license before you turn 18 years old, you'll need to:\n",
      "   (score: 0.7120)\n",
      "2: Agar plates may also be indicator plates, in which the organisms are not selected on the basis of growth, but are instead distinguished by a color change in some colonies, typically caused by the action of an enzyme on some compound added to the medium. Some commonly used agar plate types are: Thus, the plate can be used either to estimate the concentration of organisms in a liquid culture or a suitable dilution of that culture using a colony counter, or to generate genetically pure cultures from a mixed culture of genetically different organisms, using a technique known as  streaking .\n",
      "   (score: 0.6844)\n",
      "3: \n",
      "   (score: 0.6721)\n",
      "4: They earn a similar salary to registered nurses ($68,910), but take home more than radiologic technologists ($56,760), physical therapist assistants ($53,320) and medical equipment repairers ($47,120). Diagnostic medical sonographers earn less than nurse practitioners ($95,070) and physical therapists ($82,180). Due to the technical nature of the job, diagnostic medical sonographers earn more than many other health care jobs, taking in an average salary of $67,170 in 2013.\n",
      "   (score: 0.6668)\n",
      "5: Wood Retaining Wall Prices. Wooden retaining walls start at around $20 per square foot including materials and installation if using cost-effective pressure treated lumber. Hardwoods such as redwood, Cyprus or mahogany will raise the price over $30 per surface square foot but add an element of beauty to your wall.\n",
      "   (score: 0.6665)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 13. Encode documents and queries, find top-k relevant docs\n",
    "# -----------------------------\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 1. Encode all documents\n",
    "doc_vecs = encode_documents(doc_tower, all_doc_embeds, all_doc_lens, device=device)\n",
    "#print(\"Encoded document vectors shape:\", doc_vecs.shape)\n",
    "\n",
    "# 2. Encode a sample query (here, just using first doc for demo)\n",
    "query_embed = all_doc_embeds[0].unsqueeze(0)\n",
    "query_len = all_doc_lens[0].unsqueeze(0)\n",
    "query_vec = encode_query(qry_tower, query_embed, query_len, device=device)\n",
    "#print(\"Encoded query vector shape:\", query_vec.shape)\n",
    "\n",
    "# 3. Retrieve top k relevant documents\n",
    "k = 5\n",
    "indices, scores = find_top_k(query_vec, doc_vecs, k=k)\n",
    "\n",
    "print(\"Query: \", all_query_texts[0])\n",
    "print(\"Top document matches:\")\n",
    "for rank, i in enumerate(indices):\n",
    "    print(f\"{rank+1}: {all_doc_texts[i]}\")\n",
    "    print(f\"   (score: {scores[rank]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a3d56ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Query: what is rba\n",
      "Top document matches:\n",
      "1: Although sintering does occur in loose powders, it is greatly enhanced by compacting the powder, and most commercial sintering is done on compacts. Compacting is generally done at room temperature, and the resulting compact is subsequently sintered at elevated temperature without application of pressure. Making ceramic, plastic and metal objects by heating a powder. Sintering by laser is one of the additive fabrication methods used in rapid prototyping and manufacturing. See laser sintering and 3D printing.\n",
      "   (score: 0.6989)\n",
      "2: 1 Such a building would run at an average of $8.56 million to complete. 2  This does not include acquisition of the land or any demolition costs, however. 3  The above figures place this construction at a $59 per square foot cost, though a national average of stands between $50 to $70 for most projects.\n",
      "   (score: 0.6966)\n",
      "3: The kilometre (International spelling as used by the International Bureau of Weights and Measures; SI symbol: km; /ˈkɪləmiːtə/ or /kɪˈlɒmɪtə/) or kilometer (American spelling) is a unit of length in the metric system, equal to one thousand metres (kilo-being the SI prefix for 7003100000000000000♠1000). \n",
      "   (score: 0.6803)\n",
      "4: Ample perch space, again, guarantees each parakeet its own space. If the plan is for one large community cage, a couple of smaller cages are necessary for emergencies and to house a troublemaking budgie or two. Your parakeet needs a cage that is at least 24 inches by 24 inches. Measure the area where your parakeet will reside to get an idea of the size of cage the area will accommodate. This will keep you bringing home a bird cage only to find that it is too large for the space — or that you could have purchased a larger cage. A minimum cage size is 24 by 24 inches with 1/4-inch bar spacing so the budgie cannot fit its head through the bars. Birds spend just about every minute of every day on their feet. Be sure to give your parakeet at least three perches, with at least one made of natural wood.\n",
      "   (score: 0.6661)\n",
      "5: To have a whole house staged, the cost can be anywhere from $2,500-$5,000. Staging costs start for 30 days, and then there's a daily cost involved for every day staging is needed outside the 30 days, typically. A very rough estimate would be $2,500. The cost varies with everything: staging company, real estate agent, number of rooms to stage, how busy the staging companies are, how long until the home is sold, what furniture is needed to create a great impression ....\n",
      "   (score: 0.6650)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 14. Custom Query Inference\n",
    "# -----------------------------\n",
    "\n",
    "# Hardcode your query here\n",
    "custom_query = \"what is rba\"\n",
    "\n",
    "# Tokenize and embed the custom query\n",
    "tokenizer = get_tokenizer(\"basic_english\")  # or any other tokenizer you prefer\n",
    "tokenized_query = tokenizer(custom_query)  # returns List[int] or torch.Tensor\n",
    "\n",
    "# Map tokens to ids, filtering out tokens not in vocab\n",
    "q_ids = [word_to_ix[t] for t in tokenized_query if t in word_to_ix]\n",
    "\n",
    "if q_ids:\n",
    "    with torch.no_grad():\n",
    "        q_vecs = cbow_model.embeddings(torch.tensor(q_ids))  # (seq_len, embed_dim)\n",
    "else:\n",
    "    q_vecs = torch.zeros(1, embed_dim)  # Fallback for empty/unknown queries)\n",
    "\n",
    "# Pad to (1, seq_len, embed_dim) for model\n",
    "query_embed = q_vecs.unsqueeze(0)  # (1, seq_len, embed_dim)\n",
    "query_len = torch.tensor([q_vecs.shape[0]])\n",
    "\n",
    "# Move to device if needed\n",
    "query_embed = query_embed.to(device)\n",
    "query_len = query_len.to(device)\n",
    "\n",
    "# Encode the hardcoded query\n",
    "query_vec = encode_query(qry_tower, query_embed, query_len, device=device)\n",
    "\n",
    "# Retrieve top k relevant documents\n",
    "k = 5\n",
    "indices, scores = find_top_k(query_vec, doc_vecs, k=k)\n",
    "\n",
    "print(\"Custom Query:\", custom_query)\n",
    "print(\"Top document matches:\")\n",
    "for rank, i in enumerate(indices):\n",
    "    print(f\"{rank+1}: {all_doc_texts[i]}\")\n",
    "    print(f\"   (score: {scores[rank]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26dddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 15. Redis test\n",
    "# -----------------------------\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "\n",
    "r.set('foo', 'bar')\n",
    "# True\n",
    "r.get('foo')\n",
    "# bar\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
