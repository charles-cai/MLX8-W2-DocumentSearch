{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c24b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c9376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # 1. Load MS MARCO V1.1 training dataset\n",
    "# # -----------------------------\n",
    "\n",
    "# # This will stream the data, you don't have to download the full file\n",
    "# dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"train\")  # or \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdf890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building passage pool...: 100%|██████████| 82326/82326 [00:23<00:00, 3508.72it/s]\n",
      "Creating triples...: 100%|██████████| 82326/82326 [25:29<00:00, 53.82it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is rba\n",
      "[\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", 'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ', 'The inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is made of some type of resistance wire, normally Kanthal or nichrome. When a current is applied to the coil (resistance wire), it heats up and the heated coil then vaporizes the eliquid. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.', 'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.', 'RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The “end” or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the user is.', 'A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.', 'Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. ']\n",
      "['DOCUMENTATION………………………………………………….…………………...29 1. TRANSFER PRICING GUIDELINES 1. INTRODUCTION Transfer pricing generally relates to the system of pricing the cross-border transfer of goods, services and intangibles between entities in a group of Multinational Enterprise (MNE). Transfer pricing also applies if such transactions were to take place between associated companies within the country. For transfer pricing purposes, adherence to the following documentation and record keeping requirements will be advantageous to the taxpayer as it reduces the risk of a tax audit and subsequent adjustments under section 140, which will be made according to what the DG thinks are reasonable transfer prices.', \"Because the Earth is spinning around its axis. It's the same reason the Sun moves across the sky. As the Earth rotates from west to east around its axis once every 23 hours 56 minutes, the celestial sphere and all objects on it appear to rotate from east to west. Therefore stars will rise in the east, culminate on the north-south line (meridian) and set in the west.\", 'The Lay Counseling Ministry offers up to ten weeks of counseling to the All Saints community, at no cost, with counselors who are trained and supervised by licensed professionals. For more information or to schedule an appointment, call 626.583.2706.', \"Helicobacter Pylori (H. pylori) Overview. Helicobacter pylori (H. pylori) is a type of bacteria responsible for widespread infection with more than 50% of the world's population infected, even though most of those infected have no symptoms.\", 'Hemorrhoids are clumps of blood vessels of the rectum. The hemorrhoidal veins are located in the lowest area of the rectum just above the anus. Sometimes they swell when the veins enlarge and their walls become stretched, thin, and irritated by passing bowel movements. Symptoms of Prolapsed Internal Hemorrhoids. Prolapse of an internal hemorrhoid occurs when the internal hemorrhoids swell and extend from their location in the rectum through the anus. In the anal canal, the hemorrhoid is exposed to the trauma of passing stool, particularly hard stools associated with constipation.', 'Differences. Generally, the Sole Proprietorship & Partnership business entity is similar to each other in many ways. Some of the differences include: 1  Own partnership agreements are to be made – Or set to default, governed by Malaysia’s Partnership Act 1961.', 'A sedimentation rate is common blood test that is used to detect and monitor inflammation in the body. The sedimentation rate is also called the erythrocyte sedimentation rate because it is a measure of the speed that the red blood cells (erythrocytes) in a tube of blood fall to the bottom of the tube, or sediment. Sedimentation rate is often abbreviated as sed rate or ESR. Doctors use the sedimentation rate to help to determine if inflammation is present in the patient. Additionally, the sedimentation rate can be a convenient method of monitoring the progress of treatment of diseases that are characterized by inflammation. Examples of diseases that are commonly monitored with the sedimentation rate test include: 1  rheumatoid arthritis, 2  systemic lupus erythematosus, 3  abscesses, 4  psoriatic arthritis, 5  septic arthritis, 6  vasculitis,', 'ASL emerged as a language in the American School for the Deaf (ASD), founded in 1817. This school brought together Old French Sign Language (OFSL), various village sign languages, and home sign systems; ASL was created in this situation of language contact. Besides North America, dialects of ASL and ASL-based creoles are used in many countries around the world, including much of West Africa and parts of Southeast Asia. ASL is also widely learned as a second language, serving as a lingua franca. ASL is most closely related to French Sign Language (FSL).', 'nutrisystem plans range between $ 259 99 and $ 548 02 per month if you want the tastier frozen meals 1 per day in addtion to the shelf stable options you must opt for the pricier nutrisystem uniquely yours plans', 'But this comes at a pretty steep price. Most architects will work for either an hourly rate or for a fixed fee based on a percentage of the construction costs. Typical fees range from 5% to 15% of construction costs for new construction, and from 15% to 20% for remodeling. PROS & CONS OF USING AN ARCHITECT. Hiring an architect for their full suite of services is expensive and may be justified on a large complex project. On a simpler project, it is more cost-effective to hire an architect for just the service you need.']\n"
     ]
    }
   ],
   "source": [
    "# # -----------------------------\n",
    "# # 2. Create triples of (query, relevant_docs, irrelevant_docs)\n",
    "# # -----------------------------\n",
    "\n",
    "# # 1. Build passage pool and index mapping for fast sampling and seed for reproducibility\n",
    "# passage_to_idx = dict()\n",
    "# idx_to_passage = []\n",
    "# for row in tqdm(dataset, desc=\"Building passage pool...\"):\n",
    "#     for p in row['passages']['passage_text']:\n",
    "#         if p not in passage_to_idx:\n",
    "#             passage_to_idx[p] = len(idx_to_passage)\n",
    "#             idx_to_passage.append(p)\n",
    "# num_passages = len(idx_to_passage)\n",
    "\n",
    "# # 2. For each query, map relevant passage indices\n",
    "# triples = []\n",
    "# for row in tqdm(dataset, desc=\"Creating triples...\"):\n",
    "#     query = row['query']\n",
    "#     relevant_passages = row['passages']['passage_text'][:10]\n",
    "#     relevant_indices = [passage_to_idx[p] for p in relevant_passages]\n",
    "    \n",
    "#     # For fast sampling: mask out relevant indices\n",
    "#     mask = np.ones(num_passages, dtype=bool)\n",
    "#     mask[relevant_indices] = False\n",
    "#     irrelevant_indices = np.random.choice(np.where(mask)[0], 10, replace=False)\n",
    "#     irrelevant_passages = [idx_to_passage[i] for i in irrelevant_indices]\n",
    "\n",
    "#     triples.append((query, relevant_passages, irrelevant_passages))\n",
    "\n",
    "# with open(\"triples_full.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(triples, f)\n",
    "\n",
    "# print(triples[0][0])  # query\n",
    "# print(triples[0][1])  # 10 relevant docs\n",
    "# print(triples[0][2])  # 10 irrelevant docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02fd8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2b. Load triples from file\n",
    "# -------------------------------\n",
    "\n",
    "# Load .pkl\n",
    "with open(\"triples_full.pkl\", \"rb\") as f:\n",
    "    triples = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920cd3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   0%|          | 26/10566 [01:03<7:09:15,  2.44s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m irrel_doc_texts_flat = [doc \u001b[38;5;28;01mfor\u001b[39;00m docs \u001b[38;5;129;01min\u001b[39;00m irrel_doc_texts \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m#query_embeds, query_lens         = tokenize_and_embed(query_texts, batch_size=64)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m rel_doc_embeds, rel_doc_lens     = \u001b[43mtokenize_and_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_doc_texts_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m irrel_doc_embeds, irrel_doc_lens = tokenize_and_embed(irrel_doc_texts_flat, batch_size=\u001b[32m64\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtokenize_and_embed\u001b[39m\u001b[34m(texts, batch_size, show_progress)\u001b[39m\n\u001b[32m     21\u001b[39m enc = tokenizer(batch_texts, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43menc\u001b[49m\u001b[43m)\u001b[49m.last_hidden_state  \u001b[38;5;66;03m# (batch, seq_len, embed_dim)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Move outputs to CPU only once\u001b[39;00m\n\u001b[32m     25\u001b[39m token_embeddings.extend(output.cpu().split(\u001b[32m1\u001b[39m, dim=\u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1016\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1014\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1028\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1029\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:662\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    651\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    652\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    653\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    659\u001b[39m         output_attentions,\n\u001b[32m    660\u001b[39m     )\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    541\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    542\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    549\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    550\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    551\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    561\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:482\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    473\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    474\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    480\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    481\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    492\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:407\u001b[39m, in \u001b[36mBertSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[32m    403\u001b[39m is_causal = (\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    405\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    417\u001b[39m attn_output = attn_output.reshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m.all_head_size)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 3. Embed queries and documents using a pre-trained SentenceTransformer model\n",
    "# -------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_docs = 10  # Number of relevant/irrelevant documents to consider\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/msmarco-MiniLM-L6-v3')\n",
    "model     = AutoModel.from_pretrained('sentence-transformers/msmarco-MiniLM-L6-v3').to(device)\n",
    "embed_dim = model.config.hidden_size  # 384 for MiniLM\n",
    "\n",
    "def tokenize_and_embed(texts, batch_size=64, show_progress=True):\n",
    "    token_embeddings = []\n",
    "    lengths = []\n",
    "    total = len(texts)\n",
    "    it = range(0, total, batch_size)\n",
    "    if show_progress:\n",
    "        it = tqdm(it, desc=\"Embedding\", total=(total+batch_size-1)//batch_size)\n",
    "    for i in it:\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**enc).last_hidden_state  # (batch, seq_len, embed_dim)\n",
    "        # Move outputs to CPU only once\n",
    "        token_embeddings.extend(output.cpu().split(1, dim=0))\n",
    "        lengths.extend(enc['attention_mask'].sum(dim=1).cpu().tolist())\n",
    "    # Remove the batch dimension from each embedding\n",
    "    token_embeddings = [emb.squeeze(0) for emb in token_embeddings]\n",
    "    return token_embeddings, lengths\n",
    "\n",
    "query_texts   = [t[0] for t in triples]\n",
    "rel_doc_texts = [t[1] for t in triples]\n",
    "irrel_doc_texts = [t[2] for t in triples]\n",
    "\n",
    "# Flatten the lists of lists to a single list of strings\n",
    "rel_doc_texts_flat = [doc for docs in rel_doc_texts for doc in docs]\n",
    "irrel_doc_texts_flat = [doc for docs in irrel_doc_texts for doc in docs]\n",
    "\n",
    "query_embeds, query_lens         = tokenize_and_embed(query_texts, batch_size=64)\n",
    "rel_doc_embeds, rel_doc_lens     = tokenize_and_embed(rel_doc_texts_flat, batch_size=64)\n",
    "irrel_doc_embeds, irrel_doc_lens = tokenize_and_embed(irrel_doc_texts_flat, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding all triples:  62%|██████▏   | 51289/82326 [10:41<03:10, 162.51it/s]  "
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 7. Embed Queries, Relevant Documents and Irrelevant Documents (randomly sampled)\n",
    "# -----------------------------\n",
    "\n",
    "query_embeddings = []\n",
    "relevant_doc_embeddings = []\n",
    "irrelevant_doc_embeddings = []\n",
    "\n",
    "num_docs = 10  # Number of relevant/irrelevant documents to consider\n",
    "\n",
    "for i, (tokenized_query, tokenized_rels, tokenized_irrels) in enumerate(\n",
    "    tqdm(tokenized_triples, desc=\"Embedding all triples\")\n",
    "):\n",
    "    try:\n",
    "        # --- Query embedding\n",
    "        q_ids = [word_to_ix[t] for t in tokenized_query if t in word_to_ix]\n",
    "        if q_ids:\n",
    "            with torch.no_grad():\n",
    "                q_vecs = cbow_model.embeddings(torch.tensor(q_ids))\n",
    "            query_embeddings.append(q_vecs)\n",
    "        else:\n",
    "            query_embeddings.append(torch.zeros(1, embed_dim))\n",
    "\n",
    "        # --- Relevant docs\n",
    "        rel_embs = []\n",
    "        for doc_tokens in tokenized_rels[:num_docs]:\n",
    "            doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "            if doc_ids:\n",
    "                with torch.no_grad():\n",
    "                    doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "                rel_embs.append(doc_vecs)\n",
    "            else:\n",
    "                rel_embs.append(torch.zeros(1, embed_dim))\n",
    "        while len(rel_embs) < num_docs:\n",
    "            rel_embs.append(torch.zeros(1, embed_dim))\n",
    "        relevant_doc_embeddings.append(rel_embs)\n",
    "\n",
    "        # --- Irrelevant docs\n",
    "        irrel_embs = []\n",
    "        for doc_tokens in tokenized_irrels[:num_docs]:\n",
    "            doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "            if doc_ids:\n",
    "                with torch.no_grad():\n",
    "                    doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "                irrel_embs.append(doc_vecs)\n",
    "            else:\n",
    "                irrel_embs.append(torch.zeros(1, embed_dim))\n",
    "        while len(irrel_embs) < num_docs:\n",
    "            irrel_embs.append(torch.zeros(1, embed_dim))\n",
    "        irrelevant_doc_embeddings.append(irrel_embs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "        break  # or continue, depending on preference\n",
    "\n",
    "\n",
    "print(len(query_embeddings))        # Should be N\n",
    "print(len(relevant_doc_embeddings)) # Should be N\n",
    "print(len(irrelevant_doc_embeddings)) # Should be N\n",
    "print(query_embeddings[0].shape)    # (seq_len, embed_dim)\n",
    "print(relevant_doc_embeddings[0][0].shape) # (doc_seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2822065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Define distance functions & Triplet loss function\n",
    "# -----------------------------\n",
    "\n",
    "# Cosine similarity for calculation of cosine distance\n",
    "def cosine_similarity(x, y):\n",
    "    return F.cosine_similarity(x, y, dim=1)\n",
    "\n",
    "# Cosine - Smallest value means most similar\n",
    "def cosine_distance(x, y):\n",
    "    return 1 - cosine_similarity(x, y)\n",
    "\n",
    "# Euclidean (L2) - Smallest value means most similar\n",
    "def euclidean_distance(x, y):\n",
    "    return torch.norm(x - y, p=2, dim=1)\n",
    "\n",
    "# Squared Euclidean - Smallest value means most similar\n",
    "def squared_euclidean_distance(x, y):\n",
    "    return torch.sum((x - y) ** 2, dim=1)\n",
    "\n",
    "# Manhattan (L1) - Smallest value means most similar\n",
    "def manhattan_distance(x, y):\n",
    "    return torch.norm(x - y, p=1, dim=1)\n",
    "\n",
    "# Chebyshev (L-infinity) - Smallest value means most similar\n",
    "def chebyshev_distance(x, y):\n",
    "    return torch.max(torch.abs(x - y), dim=1).values\n",
    "\n",
    "# Minkowski - Smallest value means most similar\n",
    "def minkowski_distance(x, y, p=3):\n",
    "    return torch.norm(x - y, p=p, dim=1)\n",
    "\n",
    "# Triplet loss function - will compute the loss for a batch of triplets\n",
    "def triplet_loss_function(query, relevant_doc, irrelevant_doc, distance_function, margin):\n",
    "    rel_dist = distance_function(query, relevant_doc)         # (batch,)\n",
    "    irrel_dist = distance_function(query, irrelevant_doc)     # (batch,)\n",
    "    triplet_loss = torch.relu(rel_dist - irrel_dist + margin)\n",
    "    return triplet_loss.mean()                                # Average over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b1c3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Define Two Tower Model (QueryTower and DocTower)\n",
    "# -----------------------------\n",
    "\n",
    "class QueryTower(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers=1, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown rnn_type: choose 'gru' or 'lstm'\")\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (batch, seq_len, embed_dim)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, h = self.rnn(packed)\n",
    "        if isinstance(h, tuple):  # LSTM\n",
    "            h = h[0]\n",
    "        return h[-1]  # (batch, hidden_dim)\n",
    "\n",
    "class DocTower(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers=1, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown rnn_type: choose 'gru' or 'lstm'\")\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, h = self.rnn(packed)\n",
    "        if isinstance(h, tuple):\n",
    "            h = h[0]\n",
    "        return h[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbab9d09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rel_doc_embeds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Create the dataset and dataloader for batching\u001b[39;00m\n\u001b[32m     37\u001b[39m batch_size = \u001b[32m32\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m dataset = TripleDataset(query_embeds, \u001b[43mrel_doc_embeds\u001b[49m, irrel_doc_embeds)\n\u001b[32m     39\u001b[39m dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn=collate_fn)\n",
      "\u001b[31mNameError\u001b[39m: name 'rel_doc_embeds' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6. Prepare dataset for DataLoader\n",
    "# -----------------------------\n",
    "\n",
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, X_queries, X_rels, X_irrels):\n",
    "        self.X_queries = X_queries    # list of N tensors (seq_len, embed_dim)\n",
    "        self.X_rels = X_rels          # list of N lists of 10 tensors (seq_len, embed_dim)\n",
    "        self.X_irrels = X_irrels      # list of N lists of 10 tensors (seq_len, embed_dim)\n",
    "        self.n = len(X_queries)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n * 10\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triple_idx = idx // 10\n",
    "        doc_idx = idx % 10\n",
    "\n",
    "        qry = self.X_queries[triple_idx]              # (q_seq_len, embed_dim)\n",
    "        rel = self.X_rels[triple_idx][doc_idx]        # (rel_seq_len, embed_dim)\n",
    "        irrel = self.X_irrels[triple_idx][doc_idx]    # (irrel_seq_len, embed_dim)\n",
    "\n",
    "        return qry, rel, irrel\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    q_seqs, r_seqs, i_seqs = zip(*batch)\n",
    "    q_lens = [x.shape[0] for x in q_seqs]\n",
    "    r_lens = [x.shape[0] for x in r_seqs]\n",
    "    i_lens = [x.shape[0] for x in i_seqs]\n",
    "    q_padded = pad_sequence(q_seqs, batch_first=True)\n",
    "    r_padded = pad_sequence(r_seqs, batch_first=True)\n",
    "    i_padded = pad_sequence(i_seqs, batch_first=True)\n",
    "    return q_padded, r_padded, i_padded, q_lens, r_lens, i_lens\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader for batching\n",
    "batch_size = 32\n",
    "dataset = TripleDataset(query_embeds, rel_doc_embeds, irrel_doc_embeds)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44335531",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     19\u001b[39m     epoch_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mdataloader\u001b[49m, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     21\u001b[39m         qry_embeds, rel_embeds, irrel_embeds, q_lens, r_lens, i_lens = batch\n\u001b[32m     22\u001b[39m         \u001b[38;5;66;03m# qry_embeds: (batch, q_seq_len, embed_dim)\u001b[39;00m\n\u001b[32m     23\u001b[39m         \u001b[38;5;66;03m# rel_embeds: (batch, r_seq_len, embed_dim)\u001b[39;00m\n\u001b[32m     24\u001b[39m         \u001b[38;5;66;03m# irrel_embeds: (batch, i_seq_len, embed_dim)\u001b[39;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m         \u001b[38;5;66;03m# Mask: keep items where rel doc is *not* all zeros\u001b[39;00m\n\u001b[32m     27\u001b[39m         \u001b[38;5;66;03m# mask shape: (batch,)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 7. Train the model (change hyperparameters as needed)\n",
    "# -----------------------------\n",
    "\n",
    "# embed_dim defined in section 3.\n",
    "\n",
    "# Define hyperparameters\n",
    "hidden_dim = 128  # Dimension of the hidden state in RNNs (GRU/LSTM - can be adjusted)\n",
    "margin = 0.2  # Margin for triplet loss (can be adjusted)\n",
    "distance_function = cosine_distance  # Choose distance function (cosine_distance, euclidean_distance, manhattan_distance, squared_euclidean_distance, chebyshev_distance, minkowski_distance)\n",
    "\n",
    "qry_tower = QueryTower(embed_dim, hidden_dim)\n",
    "doc_tower = DocTower(embed_dim, hidden_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(qry_tower.parameters()) + list(doc_tower.parameters()), lr=1e-3)\n",
    "num_epochs = 5  # Set as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        qry_embeds, rel_embeds, irrel_embeds, q_lens, r_lens, i_lens = batch\n",
    "        # qry_embeds: (batch, q_seq_len, embed_dim)\n",
    "        # rel_embeds: (batch, r_seq_len, embed_dim)\n",
    "        # irrel_embeds: (batch, i_seq_len, embed_dim)\n",
    "\n",
    "        # Mask: keep items where rel doc is *not* all zeros\n",
    "        # mask shape: (batch,)\n",
    "        mask = ~torch.all(rel_embeds == 0, dim=(1,2))\n",
    "\n",
    "        # If mask is all False, skip batch (shouldn't happen)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # Only keep non-padded triples\n",
    "        qry_embeds    = qry_embeds[mask]\n",
    "        rel_embeds    = rel_embeds[mask]\n",
    "        irrel_embeds  = irrel_embeds[mask]\n",
    "        q_lens        = [q_lens[i] for i in range(len(mask)) if mask[i]]\n",
    "        r_lens        = [r_lens[i] for i in range(len(mask)) if mask[i]]\n",
    "        i_lens        = [i_lens[i] for i in range(len(mask)) if mask[i]]\n",
    "\n",
    "        qry_vecs    = qry_tower(qry_embeds, q_lens)\n",
    "        rel_vecs    = doc_tower(rel_embeds, r_lens)\n",
    "        irrel_vecs  = doc_tower(irrel_embeds, i_lens)\n",
    "\n",
    "        loss = triplet_loss_function(\n",
    "            qry_vecs, rel_vecs, irrel_vecs,\n",
    "            distance_function=distance_function,\n",
    "            margin=margin\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "# Save final model after all epochs are done\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'qry_tower_state_dict': qry_tower.state_dict(),\n",
    "    'doc_tower_state_dict': doc_tower.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': epoch_loss,  # from last epoch\n",
    "}, \"twotower_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ec982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 313/313 [01:36<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.1173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 313/313 [01:46<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Loss: 0.0490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 313/313 [01:38<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Loss: 0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 313/313 [02:02<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Loss: 0.0879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 313/313 [02:35<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Loss: 0.1506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 313/313 [02:35<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Loss: 0.1439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 313/313 [02:34<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Loss: 0.1373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 313/313 [01:50<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Loss: 0.1374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 313/313 [02:28<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Loss: 0.1361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 313/313 [02:16<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Loss: 0.1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # -----------------------------\n",
    "# # 7. Train the model (change hyperparameters as needed)\n",
    "# # -----------------------------\n",
    "\n",
    "# # embed_dim defined in section 3.\n",
    "\n",
    "# # Define hyperparameters\n",
    "# hidden_dim = 128  # Dimension of the hidden state in RNNs (GRU/LSTM - can be adjusted)\n",
    "# margin = 0.2  # Margin for triplet loss (can be adjusted)\n",
    "# distance_function = cosine_distance  # Choose distance function (cosine_distance, euclidean_distance, manhattan_distance, squared_euclidean_distance, chebyshev_distance, minkowski_distance)\n",
    "\n",
    "# qry_tower = QueryTower(embed_dim, hidden_dim)\n",
    "# doc_tower = DocTower(embed_dim, hidden_dim)\n",
    "\n",
    "# optimizer = torch.optim.Adam(list(qry_tower.parameters()) + list(doc_tower.parameters()), lr=1e-3)\n",
    "# num_epochs = 10  # Set as needed\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_loss = 0.0\n",
    "#     for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "#         qry_embeds, rel_embeds, irrel_embeds, q_lens, r_lens, i_lens = batch\n",
    "\n",
    "#         qry_vecs    = qry_tower(qry_embeds, q_lens)\n",
    "#         rel_vecs    = doc_tower(rel_embeds, r_lens)\n",
    "#         irrel_vecs  = doc_tower(irrel_embeds, i_lens)\n",
    "\n",
    "#         loss = triplet_loss_function(\n",
    "#             qry_vecs, rel_vecs, irrel_vecs,\n",
    "#             distance_function=distance_function,\n",
    "#             margin=margin\n",
    "#         )\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "# # Save final model after all epochs are done\n",
    "# torch.save({\n",
    "#     'epoch': num_epochs,\n",
    "#     'qry_tower_state_dict': qry_tower.state_dict(),\n",
    "#     'doc_tower_state_dict': doc_tower.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'loss': epoch_loss,  # from last epoch\n",
    "# }, \"twotower_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05505892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 12. Inference: Encode documents and queries, find top-k relevant docs\n",
    "# -----------------------------\n",
    "\n",
    "def encode_documents(doc_tower, all_doc_embeds, all_doc_lens, device='cpu', batch_size=128):\n",
    "    doc_tower.eval()\n",
    "    all_vecs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(all_doc_embeds), batch_size):\n",
    "            batch_embeds = all_doc_embeds[i:i+batch_size].to(device)\n",
    "            batch_lens = all_doc_lens[i:i+batch_size]\n",
    "            vecs = doc_tower(batch_embeds, batch_lens)  # Shape: (batch, hidden_dim)\n",
    "            all_vecs.append(vecs.cpu())\n",
    "    return torch.cat(all_vecs, dim=0)  # Shape: (num_docs, hidden_dim)\n",
    "\n",
    "def encode_query(qry_tower, query_embed, query_len, device='cpu'):\n",
    "    qry_tower.eval()\n",
    "    with torch.no_grad():\n",
    "        query_vec = qry_tower(query_embed.to(device), query_len)\n",
    "    return query_vec.cpu()  # Shape: (1, hidden_dim)\n",
    "\n",
    "def find_top_k(query_vec, doc_vecs, k=5, distance_fn=None):\n",
    "    # query_vec: (1, hidden_dim), doc_vecs: (num_docs, hidden_dim)\n",
    "    if distance_fn is None:\n",
    "        # Default to cosine distance\n",
    "        def distance_fn(q, d):\n",
    "            return 1 - torch.nn.functional.cosine_similarity(q, d)\n",
    "    distances = distance_fn(query_vec, doc_vecs)\n",
    "    # If query_vec is (1,hidden_dim), expand to (num_docs,hidden_dim)\n",
    "    if query_vec.shape[0] == 1:\n",
    "        distances = distance_fn(query_vec.expand_as(doc_vecs), doc_vecs)\n",
    "    # Get top k smallest distances\n",
    "    topk = torch.topk(-distances, k)  # negative because smallest distance = highest relevance\n",
    "    indices = topk.indices.cpu().numpy()\n",
    "    scores = -topk.values.cpu().numpy()\n",
    "    return indices, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a33a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 13. Calculate all document embeddings and lengths\n",
    "# -----------------------------\n",
    "\n",
    "all_doc_texts = []  # To hold all relevant doc texts for padding\n",
    "all_query_texts = []\n",
    "\n",
    "for query, rels, irrels in triples:\n",
    "    all_query_texts.append(query)\n",
    "    # rels is the list of relevant doc texts for this query\n",
    "    # We slice to num_docs to match the embedding logic, then pad if needed\n",
    "    for doc in rels[:num_docs]:\n",
    "        all_doc_texts.append(doc)\n",
    "    # Pad if fewer than num_docs\n",
    "    while len(rels) < num_docs:\n",
    "        all_doc_texts.append(\"\")\n",
    "        rels.append(\"\")  # So embedding and text padding always match\n",
    "\n",
    "# Step 1: Gather your document embedding sequences and their lengths\n",
    "all_doc_embeds_list = []\n",
    "all_doc_lens_list = []\n",
    "\n",
    "for batch in dataloader:\n",
    "    rel_embeds, r_lens = batch[1], batch[4]  # rel_embeds: (batch, seq_len, embed_dim)\n",
    "    for i in range(rel_embeds.shape[0]):\n",
    "        all_doc_embeds_list.append(rel_embeds[i])  # (seq_len, embed_dim)\n",
    "        all_doc_lens_list.append(r_lens[i])\n",
    "\n",
    "# Step 2: Pad all embeddings to max seq_len\n",
    "# pad_sequence wants a list of (seq_len, embed_dim), returns (max_seq_len, num_docs, embed_dim)\n",
    "padded = pad_sequence(all_doc_embeds_list, batch_first=True)  # (num_docs, max_seq_len, embed_dim)\n",
    "\n",
    "all_doc_embeds = padded  # (num_docs, max_seq_len, embed_dim)\n",
    "all_doc_lens = torch.tensor(all_doc_lens_list)\n",
    "\n",
    "print(\"all_doc_embeds shape:\", all_doc_embeds.shape)\n",
    "print(\"all_doc_lens shape:\", all_doc_lens.shape)\n",
    "print(\"all_doc_texts length:\", len(all_doc_texts))\n",
    "print(\"all_query_texts length:\", len(all_query_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 14. Encode documents and queries, find top-k relevant docs\n",
    "# -----------------------------\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 1. Encode all documents\n",
    "doc_vecs = encode_documents(doc_tower, all_doc_embeds, all_doc_lens, device=device)\n",
    "#print(\"Encoded document vectors shape:\", doc_vecs.shape)\n",
    "\n",
    "# 2. Encode a sample query (here, just using first doc for demo)\n",
    "query_embed = all_doc_embeds[0].unsqueeze(0)\n",
    "query_len = all_doc_lens[0].unsqueeze(0)\n",
    "query_vec = encode_query(qry_tower, query_embed, query_len, device=device)\n",
    "#print(\"Encoded query vector shape:\", query_vec.shape)\n",
    "\n",
    "# 3. Retrieve top k relevant documents\n",
    "k = 5\n",
    "indices, scores = find_top_k(query_vec, doc_vecs, k=k)\n",
    "\n",
    "print(\"Query: \", all_query_texts[0])\n",
    "print(\"Top document matches:\")\n",
    "for rank, i in enumerate(indices):\n",
    "    print(f\"{rank+1}: {all_doc_texts[i]}\")\n",
    "    print(f\"   (score: {scores[rank]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d56ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 14b. Custom Query Inference\n",
    "# -----------------------------\n",
    "\n",
    "# Hardcode your query here\n",
    "custom_query = \"what is machine learning?\"\n",
    "\n",
    "# Tokenize and embed the custom query\n",
    "tokenized_query = tokenizer(custom_query)  # returns List[int] or torch.Tensor\n",
    "\n",
    "# Map tokens to ids, filtering out tokens not in vocab\n",
    "q_ids = [word_to_ix[t] for t in tokenized_query if t in word_to_ix]\n",
    "\n",
    "if q_ids:\n",
    "    with torch.no_grad():\n",
    "        q_vecs = cbow_model.embeddings(torch.tensor(q_ids))  # (seq_len, embed_dim)\n",
    "else:\n",
    "    q_vecs = torch.zeros(1, embed_dim)  # Fallback for empty/unknown queries)\n",
    "\n",
    "# Pad to (1, seq_len, embed_dim) for model\n",
    "query_embed = q_vecs.unsqueeze(0)  # (1, seq_len, embed_dim)\n",
    "query_len = torch.tensor([q_vecs.shape[0]])\n",
    "\n",
    "# Move to device if needed\n",
    "query_embed = query_embed.to(device)\n",
    "query_len = query_len.to(device)\n",
    "\n",
    "# Encode the hardcoded query\n",
    "query_vec = encode_query(qry_tower, query_embed, query_len, device=device)\n",
    "\n",
    "# Retrieve top k relevant documents\n",
    "k = 5\n",
    "indices, scores = find_top_k(query_vec, doc_vecs, k=k)\n",
    "\n",
    "print(\"Custom Query:\", custom_query)\n",
    "print(\"Top document matches:\")\n",
    "for rank, i in enumerate(indices):\n",
    "    print(f\"{rank+1}: {all_doc_texts[i]}\")\n",
    "    print(f\"   (score: {scores[rank]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26dddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 15. Redis test\n",
    "# -----------------------------\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "\n",
    "r.set('foo', 'bar')\n",
    "# True\n",
    "r.get('foo')\n",
    "# bar\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
