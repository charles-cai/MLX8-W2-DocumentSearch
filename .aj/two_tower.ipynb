{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c9376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Load MS MARCO V1.1 training dataset\n",
    "# -----------------------------\n",
    "\n",
    "# This will stream the data, you don't have to download the full file\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"train\")  # or \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdf890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building passage pool...:   0%|          | 0/82326 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building passage pool...: 100%|██████████| 82326/82326 [00:24<00:00, 3327.26it/s]\n",
      "Creating triples...:   1%|          | 999/82326 [04:14<5:45:39,  3.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is rba\n",
      "[\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", 'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ', 'The inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is made of some type of resistance wire, normally Kanthal or nichrome. When a current is applied to the coil (resistance wire), it heats up and the heated coil then vaporizes the eliquid. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.', 'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.', 'RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The “end” or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the user is.', 'A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.', 'Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. ']\n",
      "['DNA replication occurs in the cytoplasm of prokaryotes and in the nucleus of eukaryotes. Regardless of where DNA replication occurs, the basic process is the same. The structure of DNA lends itself easily to DNA replication.', 'Confidence votes 4.0K. Parathyroid hormone has effects antagonistic to those of calcitonin. It increases blood calcium levels by stimulating osteoclasts to break down bone and release calcium. It also increases gastrointestinal calcium absorption by activating vitamin D, and promotes calcium uptake by the kidneys. The hormone produced by the para follicular cells of the thyroid gland is calcitonin (CT). CT can decrease the level of calcium in the blood by inhibiting the action of … osteoclasts, the cells that break down bone extracellular matrix.', 'Code of Ethics. A code of ethics is a document, usually issued by a board of directors, that outlines a set of principles that affect decision-making. For example, a code of ethics might stipulate that XYZ Corporation is committed to environmental protection and green initiatives. Codes of ethics and conduct have proliferated in part because of increasing public concern about the way companies do business. Codes of ethics, which govern decision-making, and codes of conduct, which govern actions, represent two of the most common ways that companies self-regulate.', 'For anywhere from six to 12 months after hip replacement surgery, pivoting or twisting on the involved leg should be avoided. You should also not cross the involved leg past the midline of the body nor turn the involved leg inward and you should not bend at the hip past 90 degrees.', \"Two early sources for Bruegel's biography are Lodovico Guicciardini 's account of the Low Countries and Karel van Mander 's 1604 Schilder-boeck. Guicciardini recorded that Bruegel was born in Breda, but according to van Mander, he was born in Breugel near the (now Dutch) town of Eindhoven. Often Bruegel painted a community event, as in The Peasant Wedding and The Fight Between Carnival and Lent. In paintings like The Peasant Wedding, Bruegel painted individual, identifiable people while the people in The Fight Between Carnival and Lent are unidentifiable, muffin-faced allegories of greed or gluttony.\", '(United States). The average pay for a Medical Office Manager in Miami, Florida is $45,000 per year. The highest paying skills associated with this job are Human Resources, Operations Management, and Medical Credentialing. Most people with this job move on to other positions after 20 years in this career. ', 'Spirulina is a type of blue-green algae available as a dietary supplement in powdered, capsule or tablet form. According to MedlinePlus, spirulina is marketed as an alternative treatment for cardiovascular, digestive and immune system problems, but none of these claims are supported by scientific evidence. Like other sea vegetables such as seaweed, spirulina has a high concentration of iodine.', \"With a single shot, the Soviet Union vaulted ahead in the Space Race. The country sent Sputnik, the world's first artificial satellite, into space on Oct. 4, 1957. The small satellite brought the Soviet Union into the technological spotlight and demonstrated that the country was capable of modern feats. \", 'Washington, DC. On behalf of President Obama and the people of the United States, I extend my best wishes to the people of Paraguay as they celebrate 202 years of independence on May 15. ', 'Psychologists and other qualified mental health professionals use psychological tests to measure specific psychological constructs in individuals. This lesson will explore the different types of psychological tests and provide several examples. These are instruments used to measure how much of a specific psychological construct an individual has. Psychological tests are used to assess many areas, including: 1  Traits such as introversion and extroversion. 2  Certain conditions such as depression and anxiety.']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 2. Create triples of (query, relevant_docs, irrelevant_docs)\n",
    "# -----------------------------\n",
    "\n",
    "# Create a list of all passages to pass into loop to help create irrelevant documents\n",
    "all_passages = set()\n",
    "for row in tqdm(dataset, desc=\"Building passage pool...\"):\n",
    "    all_passages.update(row['passages']['passage_text'])\n",
    "all_passages = list(all_passages)\n",
    "\n",
    "# Create triples of (query, relevant_docs, irrelevant_docs)\n",
    "triples = []\n",
    "for row in tqdm(dataset, desc=\"Creating triples...\"):\n",
    "    query = row['query']\n",
    "    relevant = row['passages']['passage_text'][:10]\n",
    "    relevant_set = set(relevant)\n",
    "    irrelevant_pool = list(set(all_passages) - relevant_set)\n",
    "    irrelevant = random.sample(irrelevant_pool, 10)\n",
    "    triples.append((query, relevant, irrelevant))\n",
    "    if len(triples) >= 1000:\n",
    "        break\n",
    "\n",
    "# Save the triples to a file for later use\n",
    "with open(\"triples_1000.pkl\", \"wb\") as f:\n",
    "    pickle.dump(triples, f)\n",
    "\n",
    "with open(\"triples_1000.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(triples, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(triples[0][0])  # query\n",
    "print(triples[0][1])  # 10 relevant docs\n",
    "print(triples[0][2])  # 10 irrelevant docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02fd8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2b. Load triples from file\n",
    "# -------------------------------\n",
    "\n",
    "# Load .pkl\n",
    "with open(\"triples_1000.pkl\", \"rb\") as f:\n",
    "    triples = pickle.load(f)\n",
    "\n",
    "# # Load .json\n",
    "# with open(\"triples_1000.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     triples = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "920cd3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['what', 'is', 'rba'], [['since', '2007', ',', 'the', 'rba', \"'\", 's', 'outstanding', 'reputation', 'has', 'been', 'affected', 'by', 'the', \"'\", 'securency', \"'\", 'or', 'npa', 'scandal', '.', 'these', 'rba', 'subsidiaries', 'were', 'involved', 'in', 'bribing', 'overseas', 'officials', 'so', 'that', 'australia', 'might', 'win', 'lucrative', 'note-printing', 'contracts', '.', 'the', 'assets', 'of', 'the', 'bank', 'include', 'the', 'gold', 'and', 'foreign', 'exchange', 'reserves', 'of', 'australia', ',', 'which', 'is', 'estimated', 'to', 'have', 'a', 'net', 'worth', 'of', 'a$101', 'billion', '.', 'nearly', '94%', 'of', 'the', 'rba', \"'\", 's', 'employees', 'work', 'at', 'its', 'headquarters', 'in', 'sydney', ',', 'new', 'south', 'wales', 'and', 'at', 'the', 'business', 'resumption', 'site', '.'], ['the', 'reserve', 'bank', 'of', 'australia', '(', 'rba', ')', 'came', 'into', 'being', 'on', '14', 'january', '1960', 'as', 'australia', \"'\", 's', 'central', 'bank', 'and', 'banknote', 'issuing', 'authority', ',', 'when', 'the', 'reserve', 'bank', 'act', '1959', 'removed', 'the', 'central', 'banking', 'functions', 'from', 'the', 'commonwealth', 'bank', '.', 'the', 'assets', 'of', 'the', 'bank', 'include', 'the', 'gold', 'and', 'foreign', 'exchange', 'reserves', 'of', 'australia', ',', 'which', 'is', 'estimated', 'to', 'have', 'a', 'net', 'worth', 'of', 'a$101', 'billion', '.', 'nearly', '94%', 'of', 'the', 'rba', \"'\", 's', 'employees', 'work', 'at', 'its', 'headquarters', 'in', 'sydney', ',', 'new', 'south', 'wales', 'and', 'at', 'the', 'business', 'resumption', 'site', '.'], ['rba', 'recognized', 'with', 'the', '2014', 'microsoft', 'us', 'regional', 'partner', 'of', 'the', '.', '.', '.', 'by', 'pr', 'newswire', '.', 'contract', 'awarded', 'for', 'supply', 'and', 'support', 'the', '.', 'securitisations', 'system', 'used', 'for', 'risk', 'management', 'and', 'analysis', '.'], ['the', 'inner', 'workings', 'of', 'a', 'rebuildable', 'atomizer', 'are', 'surprisingly', 'simple', '.', 'the', 'coil', 'inside', 'the', 'rba', 'is', 'made', 'of', 'some', 'type', 'of', 'resistance', 'wire', ',', 'normally', 'kanthal', 'or', 'nichrome', '.', 'when', 'a', 'current', 'is', 'applied', 'to', 'the', 'coil', '(', 'resistance', 'wire', ')', ',', 'it', 'heats', 'up', 'and', 'the', 'heated', 'coil', 'then', 'vaporizes', 'the', 'eliquid', '.', '1', 'the', 'bottom', 'feed', 'rba', 'is', ',', 'perhaps', ',', 'the', 'easiest', 'of', 'all', 'rba', 'types', 'to', 'build', ',', 'maintain', ',', 'and', 'use', '.', '2', 'it', 'is', 'filled', 'from', 'below', ',', 'much', 'like', 'bottom', 'coil', 'clearomizer', '.', '3', 'bottom', 'feed', 'rbas', 'can', 'utilize', 'cotton', 'instead', 'of', 'silica', 'for', 'the', 'wick', '.', '4', 'the', 'genesis', ',', 'or', 'genny', ',', 'is', 'a', 'top', 'feed', 'rba', 'that', 'utilizes', 'a', 'short', 'woven', 'mesh', 'wire', '.'], ['results-based', 'accountability®', '(', 'also', 'known', 'as', 'rba', ')', 'is', 'a', 'disciplined', 'way', 'of', 'thinking', 'and', 'taking', 'action', 'that', 'communities', 'can', 'use', 'to', 'improve', 'the', 'lives', 'of', 'children', ',', 'youth', ',', 'families', ',', 'adults', 'and', 'the', 'community', 'as', 'a', 'whole', '.', 'rba', 'is', 'also', 'used', 'by', 'organizations', 'to', 'improve', 'the', 'performance', 'of', 'their', 'programs', '.', 'rba', 'improves', 'the', 'lives', 'of', 'children', ',', 'families', ',', 'and', 'communities', 'and', 'the', 'performance', 'of', 'programs', 'because', 'rba', '1', 'gets', 'from', 'talk', 'to', 'action', 'quickly', '2', 'is', 'a', 'simple', ',', 'common', 'sense', 'process', 'that', 'everyone', 'can', 'understand', '3', 'helps', 'groups', 'to', 'surface', 'and', 'challenge', 'assumptions', 'that', 'can', 'be', 'barriers', 'to', 'innovation'], ['results-based', 'accountability®', '(', 'also', 'known', 'as', 'rba', ')', 'is', 'a', 'disciplined', 'way', 'of', 'thinking', 'and', 'taking', 'action', 'that', 'communities', 'can', 'use', 'to', 'improve', 'the', 'lives', 'of', 'children', ',', 'youth', ',', 'families', ',', 'adults', 'and', 'the', 'community', 'as', 'a', 'whole', '.', 'rba', 'is', 'also', 'used', 'by', 'organizations', 'to', 'improve', 'the', 'performance', 'of', 'their', 'programs', '.', 'creating', 'community', 'impact', 'with', 'rba', '.', 'community', 'impact', 'focuses', 'on', 'conditions', 'of', 'well-being', 'for', 'children', ',', 'families', 'and', 'the', 'community', 'as', 'a', 'whole', 'that', 'a', 'group', 'of', 'leaders', 'is', 'working', 'collectively', 'to', 'improve', '.', 'for', 'example', '“residents', 'with', 'good', 'jobs', ',', '”', '“children', 'ready', 'for', 'school', ',', '”', 'or', '“a', 'safe', 'and', 'clean', 'neighborhood”', '.'], ['rba', 'uses', 'a', 'data-driven', ',', 'decision-making', 'process', 'to', 'help', 'communities', 'and', 'organizations', 'get', 'beyond', 'talking', 'about', 'problems', 'to', 'taking', 'action', 'to', 'solve', 'problems', '.', 'it', 'is', 'a', 'simple', ',', 'common', 'sense', 'framework', 'that', 'everyone', 'can', 'understand', '.', 'rba', 'starts', 'with', 'ends', 'and', 'works', 'backward', ',', 'towards', 'means', '.', 'the', '“end”', 'or', 'difference', 'you', 'are', 'trying', 'to', 'make', 'looks', 'slightly', 'different', 'if', 'you', 'are', 'working', 'on', 'a', 'broad', 'community', 'level', 'or', 'are', 'focusing', 'on', 'your', 'specific', 'program', 'or', 'organization', '.', 'rba', 'improves', 'the', 'lives', 'of', 'children', ',', 'families', ',', 'and', 'communities', 'and', 'the', 'performance', 'of', 'programs', 'because', 'rba', '1', 'gets', 'from', 'talk', 'to', 'action', 'quickly', '2', 'is', 'a', 'simple', ',', 'common', 'sense', 'process', 'that', 'everyone', 'can', 'understand', '3', 'helps', 'groups', 'to', 'surface', 'and', 'challenge', 'assumptions', 'that', 'can', 'be', 'barriers', 'to', 'innovation'], ['vs', '.', 'netiq', 'identity', 'manager', '.', 'risk-based', 'authentication', '(', 'rba', ')', 'is', 'a', 'method', 'of', 'applying', 'varying', 'levels', 'of', 'stringency', 'to', 'authentication', 'processes', 'based', 'on', 'the', 'likelihood', 'that', 'access', 'to', 'a', 'given', 'system', 'could', 'result', 'in', 'its', 'being', 'compromised', '.', 'risk-based', 'authentication', 'can', 'be', 'categorized', 'as', 'either', 'user-dependent', 'or', 'transaction-dependent', '.', 'user-dependent', 'rba', 'processes', 'employ', 'the', 'same', 'authentication', 'for', 'every', 'session', 'initiated', 'by', 'a', 'given', 'user', 'the', 'exact', 'credentials', 'that', 'the', 'site', 'demands', 'depend', 'on', 'who', 'the', 'user', 'is', '.'], ['a', 'rebuildable', 'atomizer', '(', 'rba', ')', ',', 'often', 'referred', 'to', 'as', 'simply', 'a', '“rebuildable', ',', '”', 'is', 'just', 'a', 'special', 'type', 'of', 'atomizer', 'used', 'in', 'the', 'vape', 'pen', 'and', 'mod', 'industry', 'that', 'connects', 'to', 'a', 'personal', 'vaporizer', '.', '1', 'the', 'bottom', 'feed', 'rba', 'is', ',', 'perhaps', ',', 'the', 'easiest', 'of', 'all', 'rba', 'types', 'to', 'build', ',', 'maintain', ',', 'and', 'use', '.', '2', 'it', 'is', 'filled', 'from', 'below', ',', 'much', 'like', 'bottom', 'coil', 'clearomizer', '.', '3', 'bottom', 'feed', 'rbas', 'can', 'utilize', 'cotton', 'instead', 'of', 'silica', 'for', 'the', 'wick', '.', '4', 'the', 'genesis', ',', 'or', 'genny', ',', 'is', 'a', 'top', 'feed', 'rba', 'that', 'utilizes', 'a', 'short', 'woven', 'mesh', 'wire', '.'], ['get', 'to', 'know', 'us', '.', 'rba', 'is', 'a', 'digital', 'and', 'technology', 'consultancy', 'with', 'roots', 'in', 'strategy', ',', 'design', 'and', 'technology', '.', 'our', 'team', 'of', 'specialists', 'help', 'progressive', 'companies', 'deliver', 'modern', 'digital', 'experiences', 'backed', 'by', 'proven', 'technology', 'engineering', '.']], [['dna', 'replication', 'occurs', 'in', 'the', 'cytoplasm', 'of', 'prokaryotes', 'and', 'in', 'the', 'nucleus', 'of', 'eukaryotes', '.', 'regardless', 'of', 'where', 'dna', 'replication', 'occurs', ',', 'the', 'basic', 'process', 'is', 'the', 'same', '.', 'the', 'structure', 'of', 'dna', 'lends', 'itself', 'easily', 'to', 'dna', 'replication', '.'], ['confidence', 'votes', '4', '.', '0k', '.', 'parathyroid', 'hormone', 'has', 'effects', 'antagonistic', 'to', 'those', 'of', 'calcitonin', '.', 'it', 'increases', 'blood', 'calcium', 'levels', 'by', 'stimulating', 'osteoclasts', 'to', 'break', 'down', 'bone', 'and', 'release', 'calcium', '.', 'it', 'also', 'increases', 'gastrointestinal', 'calcium', 'absorption', 'by', 'activating', 'vitamin', 'd', ',', 'and', 'promotes', 'calcium', 'uptake', 'by', 'the', 'kidneys', '.', 'the', 'hormone', 'produced', 'by', 'the', 'para', 'follicular', 'cells', 'of', 'the', 'thyroid', 'gland', 'is', 'calcitonin', '(', 'ct', ')', '.', 'ct', 'can', 'decrease', 'the', 'level', 'of', 'calcium', 'in', 'the', 'blood', 'by', 'inhibiting', 'the', 'action', 'of', '…', 'osteoclasts', ',', 'the', 'cells', 'that', 'break', 'down', 'bone', 'extracellular', 'matrix', '.'], ['code', 'of', 'ethics', '.', 'a', 'code', 'of', 'ethics', 'is', 'a', 'document', ',', 'usually', 'issued', 'by', 'a', 'board', 'of', 'directors', ',', 'that', 'outlines', 'a', 'set', 'of', 'principles', 'that', 'affect', 'decision-making', '.', 'for', 'example', ',', 'a', 'code', 'of', 'ethics', 'might', 'stipulate', 'that', 'xyz', 'corporation', 'is', 'committed', 'to', 'environmental', 'protection', 'and', 'green', 'initiatives', '.', 'codes', 'of', 'ethics', 'and', 'conduct', 'have', 'proliferated', 'in', 'part', 'because', 'of', 'increasing', 'public', 'concern', 'about', 'the', 'way', 'companies', 'do', 'business', '.', 'codes', 'of', 'ethics', ',', 'which', 'govern', 'decision-making', ',', 'and', 'codes', 'of', 'conduct', ',', 'which', 'govern', 'actions', ',', 'represent', 'two', 'of', 'the', 'most', 'common', 'ways', 'that', 'companies', 'self-regulate', '.'], ['for', 'anywhere', 'from', 'six', 'to', '12', 'months', 'after', 'hip', 'replacement', 'surgery', ',', 'pivoting', 'or', 'twisting', 'on', 'the', 'involved', 'leg', 'should', 'be', 'avoided', '.', 'you', 'should', 'also', 'not', 'cross', 'the', 'involved', 'leg', 'past', 'the', 'midline', 'of', 'the', 'body', 'nor', 'turn', 'the', 'involved', 'leg', 'inward', 'and', 'you', 'should', 'not', 'bend', 'at', 'the', 'hip', 'past', '90', 'degrees', '.'], ['two', 'early', 'sources', 'for', 'bruegel', \"'\", 's', 'biography', 'are', 'lodovico', 'guicciardini', \"'\", 's', 'account', 'of', 'the', 'low', 'countries', 'and', 'karel', 'van', 'mander', \"'\", 's', '1604', 'schilder-boeck', '.', 'guicciardini', 'recorded', 'that', 'bruegel', 'was', 'born', 'in', 'breda', ',', 'but', 'according', 'to', 'van', 'mander', ',', 'he', 'was', 'born', 'in', 'breugel', 'near', 'the', '(', 'now', 'dutch', ')', 'town', 'of', 'eindhoven', '.', 'often', 'bruegel', 'painted', 'a', 'community', 'event', ',', 'as', 'in', 'the', 'peasant', 'wedding', 'and', 'the', 'fight', 'between', 'carnival', 'and', 'lent', '.', 'in', 'paintings', 'like', 'the', 'peasant', 'wedding', ',', 'bruegel', 'painted', 'individual', ',', 'identifiable', 'people', 'while', 'the', 'people', 'in', 'the', 'fight', 'between', 'carnival', 'and', 'lent', 'are', 'unidentifiable', ',', 'muffin-faced', 'allegories', 'of', 'greed', 'or', 'gluttony', '.'], ['(', 'united', 'states', ')', '.', 'the', 'average', 'pay', 'for', 'a', 'medical', 'office', 'manager', 'in', 'miami', ',', 'florida', 'is', '$45', ',', '000', 'per', 'year', '.', 'the', 'highest', 'paying', 'skills', 'associated', 'with', 'this', 'job', 'are', 'human', 'resources', ',', 'operations', 'management', ',', 'and', 'medical', 'credentialing', '.', 'most', 'people', 'with', 'this', 'job', 'move', 'on', 'to', 'other', 'positions', 'after', '20', 'years', 'in', 'this', 'career', '.'], ['spirulina', 'is', 'a', 'type', 'of', 'blue-green', 'algae', 'available', 'as', 'a', 'dietary', 'supplement', 'in', 'powdered', ',', 'capsule', 'or', 'tablet', 'form', '.', 'according', 'to', 'medlineplus', ',', 'spirulina', 'is', 'marketed', 'as', 'an', 'alternative', 'treatment', 'for', 'cardiovascular', ',', 'digestive', 'and', 'immune', 'system', 'problems', ',', 'but', 'none', 'of', 'these', 'claims', 'are', 'supported', 'by', 'scientific', 'evidence', '.', 'like', 'other', 'sea', 'vegetables', 'such', 'as', 'seaweed', ',', 'spirulina', 'has', 'a', 'high', 'concentration', 'of', 'iodine', '.'], ['with', 'a', 'single', 'shot', ',', 'the', 'soviet', 'union', 'vaulted', 'ahead', 'in', 'the', 'space', 'race', '.', 'the', 'country', 'sent', 'sputnik', ',', 'the', 'world', \"'\", 's', 'first', 'artificial', 'satellite', ',', 'into', 'space', 'on', 'oct', '.', '4', ',', '1957', '.', 'the', 'small', 'satellite', 'brought', 'the', 'soviet', 'union', 'into', 'the', 'technological', 'spotlight', 'and', 'demonstrated', 'that', 'the', 'country', 'was', 'capable', 'of', 'modern', 'feats', '.'], ['washington', ',', 'dc', '.', 'on', 'behalf', 'of', 'president', 'obama', 'and', 'the', 'people', 'of', 'the', 'united', 'states', ',', 'i', 'extend', 'my', 'best', 'wishes', 'to', 'the', 'people', 'of', 'paraguay', 'as', 'they', 'celebrate', '202', 'years', 'of', 'independence', 'on', 'may', '15', '.'], ['psychologists', 'and', 'other', 'qualified', 'mental', 'health', 'professionals', 'use', 'psychological', 'tests', 'to', 'measure', 'specific', 'psychological', 'constructs', 'in', 'individuals', '.', 'this', 'lesson', 'will', 'explore', 'the', 'different', 'types', 'of', 'psychological', 'tests', 'and', 'provide', 'several', 'examples', '.', 'these', 'are', 'instruments', 'used', 'to', 'measure', 'how', 'much', 'of', 'a', 'specific', 'psychological', 'construct', 'an', 'individual', 'has', '.', 'psychological', 'tests', 'are', 'used', 'to', 'assess', 'many', 'areas', ',', 'including', '1', 'traits', 'such', 'as', 'introversion', 'and', 'extroversion', '.', '2', 'certain', 'conditions', 'such', 'as', 'depression', 'and', 'anxiety', '.']])\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 3. Tokenise triples\n",
    "# -------------------------------\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Lists to hold the tokenized items\n",
    "tokenized_queries = []\n",
    "tokenized_relevant_docs = []\n",
    "tokenized_irrelevant_docs = []\n",
    "\n",
    "for query, rels, irrels in triples:\n",
    "    # Tokenize the query\n",
    "    tokenized_query = tokenizer(query)\n",
    "    tokenized_queries.append(tokenized_query)\n",
    "    \n",
    "    # Tokenize relevant docs\n",
    "    tokenized_rels = [tokenizer(doc) for doc in rels]\n",
    "    tokenized_relevant_docs.append(tokenized_rels)\n",
    "    \n",
    "    # Tokenize irrelevant docs\n",
    "    tokenized_irrels = [tokenizer(doc) for doc in irrels]\n",
    "    tokenized_irrelevant_docs.append(tokenized_irrels)\n",
    "\n",
    "tokenized_triples = []\n",
    "for query, rels, irrels in triples:\n",
    "    tokenized_query = tokenizer(query)\n",
    "    tokenized_rels = [tokenizer(doc) for doc in rels]\n",
    "    tokenized_irrels = [tokenizer(doc) for doc in irrels]\n",
    "    tokenized_triples.append((tokenized_query, tokenized_rels, tokenized_irrels))\n",
    "\n",
    "# Check the first tokenized items\n",
    "print(tokenized_triples[0])  # Tokenized query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ab125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Load Vocabulary of CBOW model\n",
    "# -----------------------------\n",
    "with open(\"vocab_new.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_to_ix = json.load(f)\n",
    "\n",
    "ix_to_word = {int(i): w for w, i in word_to_ix.items()}\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Load Pre-trained Embeddings (placeholder)\n",
    "# -----------------------------\n",
    "embed_dim = 200  \n",
    "state = torch.load(\"text8_cbow_embeddings.pth\", map_location='cpu')  # Shape: [vocab_size, embed_dim]\n",
    "embeddings = state[\"embeddings.weight\"] \n",
    "\n",
    "assert embeddings.shape[0] == vocab_size, \"Vocab size mismatch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fdcdbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. CBOW Model\n",
    "# -----------------------------\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).mean(dim=1)\n",
    "        return self.linear(embeds)\n",
    "\n",
    "cbow_model = CBOW(vocab_size, embed_dim)\n",
    "cbow_model.embeddings.weight.data.copy_(embeddings)\n",
    "cbow_model.embeddings.weight.requires_grad = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "torch.Size([2, 200])\n",
      "torch.Size([70, 200])\n"
     ]
    }
   ],
   "source": [
    "query_embeddings = []\n",
    "relevant_doc_embeddings = []\n",
    "irrelevant_doc_embeddings = []\n",
    "\n",
    "for i, (tokenized_query, tokenized_rels, tokenized_irrels) in enumerate(tokenized_triples):\n",
    "    # --- Query: embed each token (do not average)\n",
    "    q_ids = [word_to_ix[t] for t in tokenized_query if t in word_to_ix]\n",
    "    if q_ids:\n",
    "        with torch.no_grad():\n",
    "            q_vecs = cbow_model.embeddings(torch.tensor(q_ids))  # (query_seq_len, embed_dim)\n",
    "        query_embeddings.append(q_vecs)\n",
    "    else:\n",
    "        query_embeddings.append(torch.zeros(1, embed_dim))  # Empty query, 1 \"dummy\" token\n",
    "\n",
    "    # --- Relevant docs (list of (doc_seq_len, embed_dim))\n",
    "    rel_embs = []\n",
    "    for doc_tokens in tokenized_rels[:num_docs]:\n",
    "        doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "        if doc_ids:\n",
    "            with torch.no_grad():\n",
    "                doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))  # (doc_seq_len, embed_dim)\n",
    "            rel_embs.append(doc_vecs)\n",
    "        else:\n",
    "            rel_embs.append(torch.zeros(1, embed_dim))  # 1 \"dummy\" token if empty\n",
    "    # Pad if fewer than num_docs\n",
    "    while len(rel_embs) < num_docs:\n",
    "        rel_embs.append(torch.zeros(1, embed_dim))\n",
    "    relevant_doc_embeddings.append(rel_embs)  # List of 10 tensors (doc_seq_len, embed_dim)\n",
    "\n",
    "    # --- Irrelevant docs\n",
    "    irrel_embs = []\n",
    "    for doc_tokens in tokenized_irrels[:num_docs]:\n",
    "        doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "        if doc_ids:\n",
    "            with torch.no_grad():\n",
    "                doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "            irrel_embs.append(doc_vecs)\n",
    "        else:\n",
    "            irrel_embs.append(torch.zeros(1, embed_dim))\n",
    "    while len(irrel_embs) < num_docs:\n",
    "        irrel_embs.append(torch.zeros(1, embed_dim))\n",
    "    irrelevant_doc_embeddings.append(irrel_embs)  # List of 10 tensors\n",
    "\n",
    "# Note: You will NOT stack here because each sequence has a different length.\n",
    "# Instead, you keep:\n",
    "# - query_embeddings: List of N tensors, each (query_seq_len, embed_dim)\n",
    "# - relevant_doc_embeddings: List of N lists of 10 tensors (each (doc_seq_len, embed_dim))\n",
    "# - irrelevant_doc_embeddings: List of N lists of 10 tensors (each (doc_seq_len, embed_dim))\n",
    "\n",
    "print(len(query_embeddings))        # Should be N\n",
    "print(len(relevant_doc_embeddings)) # Should be N\n",
    "print(len(irrelevant_doc_embeddings)) # Should be N\n",
    "print(query_embeddings[0].shape)    # (seq_len, embed_dim)\n",
    "print(relevant_doc_embeddings[0][0].shape) # (doc_seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2822065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7. Define distance functions & Triplet loss function\n",
    "# -----------------------------\n",
    "\n",
    "# Cosine similarity for calculation of cosine distance\n",
    "def cosine_similarity(x, y):\n",
    "    return F.cosine_similarity(x, y, dim=1)\n",
    "\n",
    "# Cosine - Smallest value means most similar\n",
    "def cosine_distance(x, y):\n",
    "    return 1 - cosine_similarity(x, y)\n",
    "\n",
    "# Euclidean (L2) - Smallest value means most similar\n",
    "def euclidean_distance(x, y):\n",
    "    return torch.norm(x - y, p=2, dim=1)\n",
    "\n",
    "# Squared Euclidean - Smallest value means most similar\n",
    "def squared_euclidean_distance(x, y):\n",
    "    return torch.sum((x - y) ** 2, dim=1)\n",
    "\n",
    "# Manhattan (L1) - Smallest value means most similar\n",
    "def manhattan_distance(x, y):\n",
    "    return torch.norm(x - y, p=1, dim=1)\n",
    "\n",
    "# Chebyshev (L-infinity) - Smallest value means most similar\n",
    "def chebyshev_distance(x, y):\n",
    "    return torch.max(torch.abs(x - y), dim=1).values\n",
    "\n",
    "# Minkowski - Smallest value means most similar\n",
    "def minkowski_distance(x, y, p=3):\n",
    "    return torch.norm(x - y, p=p, dim=1)\n",
    "\n",
    "# Triplet loss function - will compute the loss for a batch of triplets\n",
    "def triplet_loss_function(query, relevant_doc, irrelevant_doc, distance_function, margin):\n",
    "    rel_dist = distance_function(query, relevant_doc)         # (batch,)\n",
    "    irrel_dist = distance_function(query, irrelevant_doc)     # (batch,)\n",
    "    triplet_loss = torch.relu(rel_dist - irrel_dist + margin)\n",
    "    return triplet_loss.mean()                                # Average over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b1c3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8. Define Two Tower Model (QueryTower and DocTower)\n",
    "# -----------------------------\n",
    "\n",
    "class QueryTower(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers=1, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown rnn_type: choose 'gru' or 'lstm'\")\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (batch, seq_len, embed_dim)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, h = self.rnn(packed)\n",
    "        if isinstance(h, tuple):  # LSTM\n",
    "            h = h[0]\n",
    "        return h[-1]  # (batch, hidden_dim)\n",
    "\n",
    "class DocTower(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers=1, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown rnn_type: choose 'gru' or 'lstm'\")\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, h = self.rnn(packed)\n",
    "        if isinstance(h, tuple):\n",
    "            h = h[0]\n",
    "        return h[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbab9d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 9. Prepare dataset for DataLoader\n",
    "# -----------------------------\n",
    "\n",
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, X_queries, X_rels, X_irrels):\n",
    "        self.X_queries = X_queries    # list of N tensors (seq_len, embed_dim)\n",
    "        self.X_rels = X_rels          # list of N lists of 10 tensors (seq_len, embed_dim)\n",
    "        self.X_irrels = X_irrels      # list of N lists of 10 tensors (seq_len, embed_dim)\n",
    "        self.n = len(X_queries)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n * 10\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triple_idx = idx // 10\n",
    "        doc_idx = idx % 10\n",
    "\n",
    "        qry = self.X_queries[triple_idx]              # (q_seq_len, embed_dim)\n",
    "        rel = self.X_rels[triple_idx][doc_idx]        # (rel_seq_len, embed_dim)\n",
    "        irrel = self.X_irrels[triple_idx][doc_idx]    # (irrel_seq_len, embed_dim)\n",
    "\n",
    "        return qry, rel, irrel\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    q_seqs, r_seqs, i_seqs = zip(*batch)\n",
    "    q_lens = [x.shape[0] for x in q_seqs]\n",
    "    r_lens = [x.shape[0] for x in r_seqs]\n",
    "    i_lens = [x.shape[0] for x in i_seqs]\n",
    "    q_padded = pad_sequence(q_seqs, batch_first=True)\n",
    "    r_padded = pad_sequence(r_seqs, batch_first=True)\n",
    "    i_padded = pad_sequence(i_seqs, batch_first=True)\n",
    "    return q_padded, r_padded, i_padded, q_lens, r_lens, i_lens\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader for batching\n",
    "batch_size = 32\n",
    "dataset = TripleDataset(query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44335531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 313/313 [01:36<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.1173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 313/313 [01:46<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Loss: 0.0490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 313/313 [01:38<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Loss: 0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 313/313 [02:02<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Loss: 0.0879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 313/313 [02:35<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Loss: 0.1506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 313/313 [02:35<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Loss: 0.1439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 313/313 [02:34<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Loss: 0.1373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 313/313 [01:50<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Loss: 0.1374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 313/313 [02:28<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Loss: 0.1361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 313/313 [02:16<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Loss: 0.1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 10. Train the model (change hyperparameters as needed)\n",
    "# -----------------------------\n",
    "\n",
    "# embed_dim defined in section 5. CBOW Model\n",
    "\n",
    "# Define hyperparameters\n",
    "hidden_dim = 128  # Dimension of the hidden state in RNNs (GRU/LSTM - can be adjusted)\n",
    "margin = 0.2  # Margin for triplet loss (can be adjusted)\n",
    "distance_function = cosine_distance  # Choose distance function (cosine_distance, euclidean_distance, manhattan_distance, squared_euclidean_distance, chebyshev_distance, minkowski_distance)\n",
    "\n",
    "qry_tower = QueryTower(embed_dim, hidden_dim)\n",
    "doc_tower = DocTower(embed_dim, hidden_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(qry_tower.parameters()) + list(doc_tower.parameters()), lr=1e-3)\n",
    "num_epochs = 10  # Set as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        qry_embeds, rel_embeds, irrel_embeds, q_lens, r_lens, i_lens = batch\n",
    "\n",
    "        qry_vecs    = qry_tower(qry_embeds, q_lens)\n",
    "        rel_vecs    = doc_tower(rel_embeds, r_lens)\n",
    "        irrel_vecs  = doc_tower(irrel_embeds, i_lens)\n",
    "\n",
    "        loss = triplet_loss_function(\n",
    "            qry_vecs, rel_vecs, irrel_vecs,\n",
    "            distance_function=distance_function,\n",
    "            margin=margin\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "# Save final model after all epochs are done\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'qry_tower_state_dict': qry_tower.state_dict(),\n",
    "    'doc_tower_state_dict': doc_tower.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': epoch_loss,  # from last epoch\n",
    "}, \"twotower_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05505892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 11. Inference: Encode documents and queries, find top-k relevant docs\n",
    "# -----------------------------\n",
    "\n",
    "def encode_documents(doc_tower, all_doc_embeds, all_doc_lens, device='cpu', batch_size=128):\n",
    "    doc_tower.eval()\n",
    "    all_vecs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(all_doc_embeds), batch_size):\n",
    "            batch_embeds = all_doc_embeds[i:i+batch_size].to(device)\n",
    "            batch_lens = all_doc_lens[i:i+batch_size]\n",
    "            vecs = doc_tower(batch_embeds, batch_lens)  # Shape: (batch, hidden_dim)\n",
    "            all_vecs.append(vecs.cpu())\n",
    "    return torch.cat(all_vecs, dim=0)  # Shape: (num_docs, hidden_dim)\n",
    "\n",
    "def encode_query(qry_tower, query_embed, query_len, device='cpu'):\n",
    "    qry_tower.eval()\n",
    "    with torch.no_grad():\n",
    "        query_vec = qry_tower(query_embed.to(device), query_len)\n",
    "    return query_vec.cpu()  # Shape: (1, hidden_dim)\n",
    "\n",
    "def find_top_k(query_vec, doc_vecs, k=5, distance_fn=None):\n",
    "    # query_vec: (1, hidden_dim), doc_vecs: (num_docs, hidden_dim)\n",
    "    if distance_fn is None:\n",
    "        # Default to cosine distance\n",
    "        def distance_fn(q, d):\n",
    "            return 1 - torch.nn.functional.cosine_similarity(q, d)\n",
    "    distances = distance_fn(query_vec, doc_vecs)\n",
    "    # If query_vec is (1,hidden_dim), expand to (num_docs,hidden_dim)\n",
    "    if query_vec.shape[0] == 1:\n",
    "        distances = distance_fn(query_vec.expand_as(doc_vecs), doc_vecs)\n",
    "    # Get top k smallest distances\n",
    "    topk = torch.topk(-distances, k)  # negative because smallest distance = highest relevance\n",
    "    indices = topk.indices.cpu().numpy()\n",
    "    scores = -topk.values.cpu().numpy()\n",
    "    return indices, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a33a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 12. Inference test example\n",
    "# -----------------------------\n",
    "\n",
    "# Assume you have: \n",
    "#   - all_doc_embeds (Tensor: [num_docs, seq_len, embed_dim])\n",
    "#   - all_doc_lens (List/Tensor: [num_docs])\n",
    "#   - doc_tower, qry_tower loaded/trained\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "doc_tower = doc_tower.to(device)\n",
    "qry_tower = qry_tower.to(device)\n",
    "\n",
    "# 1. Encode all documents once\n",
    "doc_vecs = encode_documents(doc_tower, all_doc_embeds, all_doc_lens, device=device)\n",
    "\n",
    "# 2. Encode input query\n",
    "# query_embed: shape [1, seq_len, embed_dim]\n",
    "# query_len: shape [1]\n",
    "query_vec = encode_query(qry_tower, query_embed, query_len, device=device)\n",
    "\n",
    "# 3. Find top-k relevant docs (change k as you like)\n",
    "k = 5\n",
    "indices, scores = find_top_k(query_vec, doc_vecs, k=k)\n",
    "\n",
    "# 4. Output top k docs\n",
    "print(f\"Top {k} document indices:\", indices)\n",
    "print(f\"Corresponding scores (lower = more similar):\", scores)\n",
    "# If you have the original doc texts, you can do:\n",
    "print([all_doc_texts[i] for i in indices])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
