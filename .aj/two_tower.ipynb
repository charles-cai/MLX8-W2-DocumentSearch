{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c9376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # 1. Load MS MARCO V1.1 training dataset\n",
    "# # -----------------------------\n",
    "\n",
    "# # This will stream the data, you don't have to download the full file\n",
    "# dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"train\")  # or \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdf890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building passage pool...: 100%|██████████| 82326/82326 [00:22<00:00, 3579.49it/s]\n",
      "Creating triples...:   5%|▍         | 4077/82326 [01:18<25:34, 50.98it/s]"
     ]
    }
   ],
   "source": [
    "# # -----------------------------\n",
    "# # 2. Create triples of (query, relevant_docs, irrelevant_docs)\n",
    "# # -----------------------------\n",
    "\n",
    "# # 1. Build passage pool and index mapping for fast sampling and seed for reproducibility\n",
    "# passage_to_idx = dict()\n",
    "# idx_to_passage = []\n",
    "# for row in tqdm(dataset, desc=\"Building passage pool...\"):\n",
    "#     for p in row['passages']['passage_text']:\n",
    "#         if p not in passage_to_idx:\n",
    "#             passage_to_idx[p] = len(idx_to_passage)\n",
    "#             idx_to_passage.append(p)\n",
    "# num_passages = len(idx_to_passage)\n",
    "\n",
    "# # 2. For each query, map relevant passage indices\n",
    "# triples = []\n",
    "# for row in tqdm(dataset, desc=\"Creating triples...\"):\n",
    "#     query = row['query']\n",
    "#     relevant_passages = row['passages']['passage_text'][:10]\n",
    "#     relevant_indices = [passage_to_idx[p] for p in relevant_passages]\n",
    "    \n",
    "#     # For fast sampling: mask out relevant indices\n",
    "#     mask = np.ones(num_passages, dtype=bool)\n",
    "#     mask[relevant_indices] = False\n",
    "#     irrelevant_indices = np.random.choice(np.where(mask)[0], 10, replace=False)\n",
    "#     irrelevant_passages = [idx_to_passage[i] for i in irrelevant_indices]\n",
    "\n",
    "#     triples.append((query, relevant_passages, irrelevant_passages))\n",
    "\n",
    "# with open(\"triples_full.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(triples, f)\n",
    "\n",
    "# print(triples[0][0])  # query\n",
    "# print(triples[0][1])  # 10 relevant docs\n",
    "# print(triples[0][2])  # 10 irrelevant docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2b. Load triples from file\n",
    "# -------------------------------\n",
    "\n",
    "# Load .pkl\n",
    "with open(\"triples_full.pkl\", \"rb\") as f:\n",
    "    triples = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920cd3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['what', 'is', 'rba'], [['since', '2007', ',', 'the', 'rba', \"'\", 's', 'outstanding', 'reputation', 'has', 'been', 'affected', 'by', 'the', \"'\", 'securency', \"'\", 'or', 'npa', 'scandal', '.', 'these', 'rba', 'subsidiaries', 'were', 'involved', 'in', 'bribing', 'overseas', 'officials', 'so', 'that', 'australia', 'might', 'win', 'lucrative', 'note-printing', 'contracts', '.', 'the', 'assets', 'of', 'the', 'bank', 'include', 'the', 'gold', 'and', 'foreign', 'exchange', 'reserves', 'of', 'australia', ',', 'which', 'is', 'estimated', 'to', 'have', 'a', 'net', 'worth', 'of', 'a$101', 'billion', '.', 'nearly', '94%', 'of', 'the', 'rba', \"'\", 's', 'employees', 'work', 'at', 'its', 'headquarters', 'in', 'sydney', ',', 'new', 'south', 'wales', 'and', 'at', 'the', 'business', 'resumption', 'site', '.'], ['the', 'reserve', 'bank', 'of', 'australia', '(', 'rba', ')', 'came', 'into', 'being', 'on', '14', 'january', '1960', 'as', 'australia', \"'\", 's', 'central', 'bank', 'and', 'banknote', 'issuing', 'authority', ',', 'when', 'the', 'reserve', 'bank', 'act', '1959', 'removed', 'the', 'central', 'banking', 'functions', 'from', 'the', 'commonwealth', 'bank', '.', 'the', 'assets', 'of', 'the', 'bank', 'include', 'the', 'gold', 'and', 'foreign', 'exchange', 'reserves', 'of', 'australia', ',', 'which', 'is', 'estimated', 'to', 'have', 'a', 'net', 'worth', 'of', 'a$101', 'billion', '.', 'nearly', '94%', 'of', 'the', 'rba', \"'\", 's', 'employees', 'work', 'at', 'its', 'headquarters', 'in', 'sydney', ',', 'new', 'south', 'wales', 'and', 'at', 'the', 'business', 'resumption', 'site', '.'], ['rba', 'recognized', 'with', 'the', '2014', 'microsoft', 'us', 'regional', 'partner', 'of', 'the', '.', '.', '.', 'by', 'pr', 'newswire', '.', 'contract', 'awarded', 'for', 'supply', 'and', 'support', 'the', '.', 'securitisations', 'system', 'used', 'for', 'risk', 'management', 'and', 'analysis', '.'], ['the', 'inner', 'workings', 'of', 'a', 'rebuildable', 'atomizer', 'are', 'surprisingly', 'simple', '.', 'the', 'coil', 'inside', 'the', 'rba', 'is', 'made', 'of', 'some', 'type', 'of', 'resistance', 'wire', ',', 'normally', 'kanthal', 'or', 'nichrome', '.', 'when', 'a', 'current', 'is', 'applied', 'to', 'the', 'coil', '(', 'resistance', 'wire', ')', ',', 'it', 'heats', 'up', 'and', 'the', 'heated', 'coil', 'then', 'vaporizes', 'the', 'eliquid', '.', '1', 'the', 'bottom', 'feed', 'rba', 'is', ',', 'perhaps', ',', 'the', 'easiest', 'of', 'all', 'rba', 'types', 'to', 'build', ',', 'maintain', ',', 'and', 'use', '.', '2', 'it', 'is', 'filled', 'from', 'below', ',', 'much', 'like', 'bottom', 'coil', 'clearomizer', '.', '3', 'bottom', 'feed', 'rbas', 'can', 'utilize', 'cotton', 'instead', 'of', 'silica', 'for', 'the', 'wick', '.', '4', 'the', 'genesis', ',', 'or', 'genny', ',', 'is', 'a', 'top', 'feed', 'rba', 'that', 'utilizes', 'a', 'short', 'woven', 'mesh', 'wire', '.'], ['results-based', 'accountability®', '(', 'also', 'known', 'as', 'rba', ')', 'is', 'a', 'disciplined', 'way', 'of', 'thinking', 'and', 'taking', 'action', 'that', 'communities', 'can', 'use', 'to', 'improve', 'the', 'lives', 'of', 'children', ',', 'youth', ',', 'families', ',', 'adults', 'and', 'the', 'community', 'as', 'a', 'whole', '.', 'rba', 'is', 'also', 'used', 'by', 'organizations', 'to', 'improve', 'the', 'performance', 'of', 'their', 'programs', '.', 'rba', 'improves', 'the', 'lives', 'of', 'children', ',', 'families', ',', 'and', 'communities', 'and', 'the', 'performance', 'of', 'programs', 'because', 'rba', '1', 'gets', 'from', 'talk', 'to', 'action', 'quickly', '2', 'is', 'a', 'simple', ',', 'common', 'sense', 'process', 'that', 'everyone', 'can', 'understand', '3', 'helps', 'groups', 'to', 'surface', 'and', 'challenge', 'assumptions', 'that', 'can', 'be', 'barriers', 'to', 'innovation'], ['results-based', 'accountability®', '(', 'also', 'known', 'as', 'rba', ')', 'is', 'a', 'disciplined', 'way', 'of', 'thinking', 'and', 'taking', 'action', 'that', 'communities', 'can', 'use', 'to', 'improve', 'the', 'lives', 'of', 'children', ',', 'youth', ',', 'families', ',', 'adults', 'and', 'the', 'community', 'as', 'a', 'whole', '.', 'rba', 'is', 'also', 'used', 'by', 'organizations', 'to', 'improve', 'the', 'performance', 'of', 'their', 'programs', '.', 'creating', 'community', 'impact', 'with', 'rba', '.', 'community', 'impact', 'focuses', 'on', 'conditions', 'of', 'well-being', 'for', 'children', ',', 'families', 'and', 'the', 'community', 'as', 'a', 'whole', 'that', 'a', 'group', 'of', 'leaders', 'is', 'working', 'collectively', 'to', 'improve', '.', 'for', 'example', '“residents', 'with', 'good', 'jobs', ',', '”', '“children', 'ready', 'for', 'school', ',', '”', 'or', '“a', 'safe', 'and', 'clean', 'neighborhood”', '.'], ['rba', 'uses', 'a', 'data-driven', ',', 'decision-making', 'process', 'to', 'help', 'communities', 'and', 'organizations', 'get', 'beyond', 'talking', 'about', 'problems', 'to', 'taking', 'action', 'to', 'solve', 'problems', '.', 'it', 'is', 'a', 'simple', ',', 'common', 'sense', 'framework', 'that', 'everyone', 'can', 'understand', '.', 'rba', 'starts', 'with', 'ends', 'and', 'works', 'backward', ',', 'towards', 'means', '.', 'the', '“end”', 'or', 'difference', 'you', 'are', 'trying', 'to', 'make', 'looks', 'slightly', 'different', 'if', 'you', 'are', 'working', 'on', 'a', 'broad', 'community', 'level', 'or', 'are', 'focusing', 'on', 'your', 'specific', 'program', 'or', 'organization', '.', 'rba', 'improves', 'the', 'lives', 'of', 'children', ',', 'families', ',', 'and', 'communities', 'and', 'the', 'performance', 'of', 'programs', 'because', 'rba', '1', 'gets', 'from', 'talk', 'to', 'action', 'quickly', '2', 'is', 'a', 'simple', ',', 'common', 'sense', 'process', 'that', 'everyone', 'can', 'understand', '3', 'helps', 'groups', 'to', 'surface', 'and', 'challenge', 'assumptions', 'that', 'can', 'be', 'barriers', 'to', 'innovation'], ['vs', '.', 'netiq', 'identity', 'manager', '.', 'risk-based', 'authentication', '(', 'rba', ')', 'is', 'a', 'method', 'of', 'applying', 'varying', 'levels', 'of', 'stringency', 'to', 'authentication', 'processes', 'based', 'on', 'the', 'likelihood', 'that', 'access', 'to', 'a', 'given', 'system', 'could', 'result', 'in', 'its', 'being', 'compromised', '.', 'risk-based', 'authentication', 'can', 'be', 'categorized', 'as', 'either', 'user-dependent', 'or', 'transaction-dependent', '.', 'user-dependent', 'rba', 'processes', 'employ', 'the', 'same', 'authentication', 'for', 'every', 'session', 'initiated', 'by', 'a', 'given', 'user', 'the', 'exact', 'credentials', 'that', 'the', 'site', 'demands', 'depend', 'on', 'who', 'the', 'user', 'is', '.'], ['a', 'rebuildable', 'atomizer', '(', 'rba', ')', ',', 'often', 'referred', 'to', 'as', 'simply', 'a', '“rebuildable', ',', '”', 'is', 'just', 'a', 'special', 'type', 'of', 'atomizer', 'used', 'in', 'the', 'vape', 'pen', 'and', 'mod', 'industry', 'that', 'connects', 'to', 'a', 'personal', 'vaporizer', '.', '1', 'the', 'bottom', 'feed', 'rba', 'is', ',', 'perhaps', ',', 'the', 'easiest', 'of', 'all', 'rba', 'types', 'to', 'build', ',', 'maintain', ',', 'and', 'use', '.', '2', 'it', 'is', 'filled', 'from', 'below', ',', 'much', 'like', 'bottom', 'coil', 'clearomizer', '.', '3', 'bottom', 'feed', 'rbas', 'can', 'utilize', 'cotton', 'instead', 'of', 'silica', 'for', 'the', 'wick', '.', '4', 'the', 'genesis', ',', 'or', 'genny', ',', 'is', 'a', 'top', 'feed', 'rba', 'that', 'utilizes', 'a', 'short', 'woven', 'mesh', 'wire', '.'], ['get', 'to', 'know', 'us', '.', 'rba', 'is', 'a', 'digital', 'and', 'technology', 'consultancy', 'with', 'roots', 'in', 'strategy', ',', 'design', 'and', 'technology', '.', 'our', 'team', 'of', 'specialists', 'help', 'progressive', 'companies', 'deliver', 'modern', 'digital', 'experiences', 'backed', 'by', 'proven', 'technology', 'engineering', '.']], [['dna', 'replication', 'occurs', 'in', 'the', 'cytoplasm', 'of', 'prokaryotes', 'and', 'in', 'the', 'nucleus', 'of', 'eukaryotes', '.', 'regardless', 'of', 'where', 'dna', 'replication', 'occurs', ',', 'the', 'basic', 'process', 'is', 'the', 'same', '.', 'the', 'structure', 'of', 'dna', 'lends', 'itself', 'easily', 'to', 'dna', 'replication', '.'], ['confidence', 'votes', '4', '.', '0k', '.', 'parathyroid', 'hormone', 'has', 'effects', 'antagonistic', 'to', 'those', 'of', 'calcitonin', '.', 'it', 'increases', 'blood', 'calcium', 'levels', 'by', 'stimulating', 'osteoclasts', 'to', 'break', 'down', 'bone', 'and', 'release', 'calcium', '.', 'it', 'also', 'increases', 'gastrointestinal', 'calcium', 'absorption', 'by', 'activating', 'vitamin', 'd', ',', 'and', 'promotes', 'calcium', 'uptake', 'by', 'the', 'kidneys', '.', 'the', 'hormone', 'produced', 'by', 'the', 'para', 'follicular', 'cells', 'of', 'the', 'thyroid', 'gland', 'is', 'calcitonin', '(', 'ct', ')', '.', 'ct', 'can', 'decrease', 'the', 'level', 'of', 'calcium', 'in', 'the', 'blood', 'by', 'inhibiting', 'the', 'action', 'of', '…', 'osteoclasts', ',', 'the', 'cells', 'that', 'break', 'down', 'bone', 'extracellular', 'matrix', '.'], ['code', 'of', 'ethics', '.', 'a', 'code', 'of', 'ethics', 'is', 'a', 'document', ',', 'usually', 'issued', 'by', 'a', 'board', 'of', 'directors', ',', 'that', 'outlines', 'a', 'set', 'of', 'principles', 'that', 'affect', 'decision-making', '.', 'for', 'example', ',', 'a', 'code', 'of', 'ethics', 'might', 'stipulate', 'that', 'xyz', 'corporation', 'is', 'committed', 'to', 'environmental', 'protection', 'and', 'green', 'initiatives', '.', 'codes', 'of', 'ethics', 'and', 'conduct', 'have', 'proliferated', 'in', 'part', 'because', 'of', 'increasing', 'public', 'concern', 'about', 'the', 'way', 'companies', 'do', 'business', '.', 'codes', 'of', 'ethics', ',', 'which', 'govern', 'decision-making', ',', 'and', 'codes', 'of', 'conduct', ',', 'which', 'govern', 'actions', ',', 'represent', 'two', 'of', 'the', 'most', 'common', 'ways', 'that', 'companies', 'self-regulate', '.'], ['for', 'anywhere', 'from', 'six', 'to', '12', 'months', 'after', 'hip', 'replacement', 'surgery', ',', 'pivoting', 'or', 'twisting', 'on', 'the', 'involved', 'leg', 'should', 'be', 'avoided', '.', 'you', 'should', 'also', 'not', 'cross', 'the', 'involved', 'leg', 'past', 'the', 'midline', 'of', 'the', 'body', 'nor', 'turn', 'the', 'involved', 'leg', 'inward', 'and', 'you', 'should', 'not', 'bend', 'at', 'the', 'hip', 'past', '90', 'degrees', '.'], ['two', 'early', 'sources', 'for', 'bruegel', \"'\", 's', 'biography', 'are', 'lodovico', 'guicciardini', \"'\", 's', 'account', 'of', 'the', 'low', 'countries', 'and', 'karel', 'van', 'mander', \"'\", 's', '1604', 'schilder-boeck', '.', 'guicciardini', 'recorded', 'that', 'bruegel', 'was', 'born', 'in', 'breda', ',', 'but', 'according', 'to', 'van', 'mander', ',', 'he', 'was', 'born', 'in', 'breugel', 'near', 'the', '(', 'now', 'dutch', ')', 'town', 'of', 'eindhoven', '.', 'often', 'bruegel', 'painted', 'a', 'community', 'event', ',', 'as', 'in', 'the', 'peasant', 'wedding', 'and', 'the', 'fight', 'between', 'carnival', 'and', 'lent', '.', 'in', 'paintings', 'like', 'the', 'peasant', 'wedding', ',', 'bruegel', 'painted', 'individual', ',', 'identifiable', 'people', 'while', 'the', 'people', 'in', 'the', 'fight', 'between', 'carnival', 'and', 'lent', 'are', 'unidentifiable', ',', 'muffin-faced', 'allegories', 'of', 'greed', 'or', 'gluttony', '.'], ['(', 'united', 'states', ')', '.', 'the', 'average', 'pay', 'for', 'a', 'medical', 'office', 'manager', 'in', 'miami', ',', 'florida', 'is', '$45', ',', '000', 'per', 'year', '.', 'the', 'highest', 'paying', 'skills', 'associated', 'with', 'this', 'job', 'are', 'human', 'resources', ',', 'operations', 'management', ',', 'and', 'medical', 'credentialing', '.', 'most', 'people', 'with', 'this', 'job', 'move', 'on', 'to', 'other', 'positions', 'after', '20', 'years', 'in', 'this', 'career', '.'], ['spirulina', 'is', 'a', 'type', 'of', 'blue-green', 'algae', 'available', 'as', 'a', 'dietary', 'supplement', 'in', 'powdered', ',', 'capsule', 'or', 'tablet', 'form', '.', 'according', 'to', 'medlineplus', ',', 'spirulina', 'is', 'marketed', 'as', 'an', 'alternative', 'treatment', 'for', 'cardiovascular', ',', 'digestive', 'and', 'immune', 'system', 'problems', ',', 'but', 'none', 'of', 'these', 'claims', 'are', 'supported', 'by', 'scientific', 'evidence', '.', 'like', 'other', 'sea', 'vegetables', 'such', 'as', 'seaweed', ',', 'spirulina', 'has', 'a', 'high', 'concentration', 'of', 'iodine', '.'], ['with', 'a', 'single', 'shot', ',', 'the', 'soviet', 'union', 'vaulted', 'ahead', 'in', 'the', 'space', 'race', '.', 'the', 'country', 'sent', 'sputnik', ',', 'the', 'world', \"'\", 's', 'first', 'artificial', 'satellite', ',', 'into', 'space', 'on', 'oct', '.', '4', ',', '1957', '.', 'the', 'small', 'satellite', 'brought', 'the', 'soviet', 'union', 'into', 'the', 'technological', 'spotlight', 'and', 'demonstrated', 'that', 'the', 'country', 'was', 'capable', 'of', 'modern', 'feats', '.'], ['washington', ',', 'dc', '.', 'on', 'behalf', 'of', 'president', 'obama', 'and', 'the', 'people', 'of', 'the', 'united', 'states', ',', 'i', 'extend', 'my', 'best', 'wishes', 'to', 'the', 'people', 'of', 'paraguay', 'as', 'they', 'celebrate', '202', 'years', 'of', 'independence', 'on', 'may', '15', '.'], ['psychologists', 'and', 'other', 'qualified', 'mental', 'health', 'professionals', 'use', 'psychological', 'tests', 'to', 'measure', 'specific', 'psychological', 'constructs', 'in', 'individuals', '.', 'this', 'lesson', 'will', 'explore', 'the', 'different', 'types', 'of', 'psychological', 'tests', 'and', 'provide', 'several', 'examples', '.', 'these', 'are', 'instruments', 'used', 'to', 'measure', 'how', 'much', 'of', 'a', 'specific', 'psychological', 'construct', 'an', 'individual', 'has', '.', 'psychological', 'tests', 'are', 'used', 'to', 'assess', 'many', 'areas', ',', 'including', '1', 'traits', 'such', 'as', 'introversion', 'and', 'extroversion', '.', '2', 'certain', 'conditions', 'such', 'as', 'depression', 'and', 'anxiety', '.']])\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 3. Tokenise triples\n",
    "# -------------------------------\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "tokenized_triples = []\n",
    "for query, rels, irrels in tqdm(triples, desc=\"Tokenizing triples\"):\n",
    "    tokenized_query = tokenizer(query)\n",
    "    tokenized_rels = [tokenizer(doc) for doc in rels]\n",
    "    tokenized_irrels = [tokenizer(doc) for doc in irrels]\n",
    "    tokenized_triples.append((tokenized_query, tokenized_rels, tokenized_irrels))\n",
    "\n",
    "print(tokenized_triples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Load Vocabulary of CBOW model\n",
    "# -----------------------------\n",
    "with open(\"vocab_new.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_to_ix = json.load(f)\n",
    "\n",
    "ix_to_word = {int(i): w for w, i in word_to_ix.items()}\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Load Pre-trained Embeddings (placeholder)\n",
    "# -----------------------------\n",
    "embed_dim = 200  \n",
    "state = torch.load(\"text8_cbow_embeddings.pth\", map_location='cpu')  # Shape: [vocab_size, embed_dim]\n",
    "embeddings = state[\"embeddings.weight\"] \n",
    "\n",
    "assert embeddings.shape[0] == vocab_size, \"Vocab size mismatch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdcdbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. CBOW Model\n",
    "# -----------------------------\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).mean(dim=1)\n",
    "        return self.linear(embeds)\n",
    "\n",
    "cbow_model = CBOW(vocab_size, embed_dim)\n",
    "cbow_model.embeddings.weight.data.copy_(embeddings)\n",
    "cbow_model.embeddings.weight.requires_grad = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "torch.Size([2, 200])\n",
      "torch.Size([70, 200])\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 7. Embed Queries, Relevant Documents and Irrelevant Documents (randomly sampled)\n",
    "# -----------------------------\n",
    "\n",
    "query_embeddings = []\n",
    "relevant_doc_embeddings = []\n",
    "irrelevant_doc_embeddings = []\n",
    "\n",
    "num_docs = 10  # Number of relevant/irrelevant documents to consider\n",
    "\n",
    "for i, (tokenized_query, tokenized_rels, tokenized_irrels) in enumerate(tokenized_triples):\n",
    "    # --- Query: embed each token (do not average)\n",
    "    q_ids = [word_to_ix[t] for t in tokenized_query if t in word_to_ix]\n",
    "    if q_ids:\n",
    "        with torch.no_grad():\n",
    "            q_vecs = cbow_model.embeddings(torch.tensor(q_ids))  # (query_seq_len, embed_dim)\n",
    "        query_embeddings.append(q_vecs)\n",
    "    else:\n",
    "        query_embeddings.append(torch.zeros(1, embed_dim))  # Empty query, 1 \"dummy\" token\n",
    "\n",
    "    # --- Relevant docs (list of (doc_seq_len, embed_dim))\n",
    "    rel_embs = []\n",
    "    for doc_tokens in tokenized_rels[:num_docs]:\n",
    "        doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "        if doc_ids:\n",
    "            with torch.no_grad():\n",
    "                doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))  # (doc_seq_len, embed_dim)\n",
    "            rel_embs.append(doc_vecs)\n",
    "        else:\n",
    "            rel_embs.append(torch.zeros(1, embed_dim))  # 1 \"dummy\" token if empty\n",
    "    # Pad if fewer than num_docs\n",
    "    while len(rel_embs) < num_docs:\n",
    "        rel_embs.append(torch.zeros(1, embed_dim))\n",
    "    relevant_doc_embeddings.append(rel_embs)  # List of 10 tensors (doc_seq_len, embed_dim)\n",
    "\n",
    "    # --- Irrelevant docs\n",
    "    irrel_embs = []\n",
    "    for doc_tokens in tokenized_irrels[:num_docs]:\n",
    "        doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "        if doc_ids:\n",
    "            with torch.no_grad():\n",
    "                doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "            irrel_embs.append(doc_vecs)\n",
    "        else:\n",
    "            irrel_embs.append(torch.zeros(1, embed_dim))\n",
    "    while len(irrel_embs) < num_docs:\n",
    "        irrel_embs.append(torch.zeros(1, embed_dim))\n",
    "    irrelevant_doc_embeddings.append(irrel_embs)  # List of 10 tensors\n",
    "\n",
    "# Note: You will NOT stack here because each sequence has a different length.\n",
    "# Instead, you keep:\n",
    "# - query_embeddings: List of N tensors, each (query_seq_len, embed_dim)\n",
    "# - relevant_doc_embeddings: List of N lists of 10 tensors (each (doc_seq_len, embed_dim))\n",
    "# - irrelevant_doc_embeddings: List of N lists of 10 tensors (each (doc_seq_len, embed_dim))\n",
    "\n",
    "print(len(query_embeddings))        # Should be N\n",
    "print(len(relevant_doc_embeddings)) # Should be N\n",
    "print(len(irrelevant_doc_embeddings)) # Should be N\n",
    "print(query_embeddings[0].shape)    # (seq_len, embed_dim)\n",
    "print(relevant_doc_embeddings[0][0].shape) # (doc_seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2822065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7. Define distance functions & Triplet loss function\n",
    "# -----------------------------\n",
    "\n",
    "# Cosine similarity for calculation of cosine distance\n",
    "def cosine_similarity(x, y):\n",
    "    return F.cosine_similarity(x, y, dim=1)\n",
    "\n",
    "# Cosine - Smallest value means most similar\n",
    "def cosine_distance(x, y):\n",
    "    return 1 - cosine_similarity(x, y)\n",
    "\n",
    "# Euclidean (L2) - Smallest value means most similar\n",
    "def euclidean_distance(x, y):\n",
    "    return torch.norm(x - y, p=2, dim=1)\n",
    "\n",
    "# Squared Euclidean - Smallest value means most similar\n",
    "def squared_euclidean_distance(x, y):\n",
    "    return torch.sum((x - y) ** 2, dim=1)\n",
    "\n",
    "# Manhattan (L1) - Smallest value means most similar\n",
    "def manhattan_distance(x, y):\n",
    "    return torch.norm(x - y, p=1, dim=1)\n",
    "\n",
    "# Chebyshev (L-infinity) - Smallest value means most similar\n",
    "def chebyshev_distance(x, y):\n",
    "    return torch.max(torch.abs(x - y), dim=1).values\n",
    "\n",
    "# Minkowski - Smallest value means most similar\n",
    "def minkowski_distance(x, y, p=3):\n",
    "    return torch.norm(x - y, p=p, dim=1)\n",
    "\n",
    "# Triplet loss function - will compute the loss for a batch of triplets\n",
    "def triplet_loss_function(query, relevant_doc, irrelevant_doc, distance_function, margin):\n",
    "    rel_dist = distance_function(query, relevant_doc)         # (batch,)\n",
    "    irrel_dist = distance_function(query, irrelevant_doc)     # (batch,)\n",
    "    triplet_loss = torch.relu(rel_dist - irrel_dist + margin)\n",
    "    return triplet_loss.mean()                                # Average over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8. Define Two Tower Model (QueryTower and DocTower)\n",
    "# -----------------------------\n",
    "\n",
    "class QueryTower(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers=1, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown rnn_type: choose 'gru' or 'lstm'\")\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (batch, seq_len, embed_dim)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, h = self.rnn(packed)\n",
    "        if isinstance(h, tuple):  # LSTM\n",
    "            h = h[0]\n",
    "        return h[-1]  # (batch, hidden_dim)\n",
    "\n",
    "class DocTower(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers=1, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown rnn_type: choose 'gru' or 'lstm'\")\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, h = self.rnn(packed)\n",
    "        if isinstance(h, tuple):\n",
    "            h = h[0]\n",
    "        return h[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab9d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 9. Prepare dataset for DataLoader\n",
    "# -----------------------------\n",
    "\n",
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, X_queries, X_rels, X_irrels):\n",
    "        self.X_queries = X_queries    # list of N tensors (seq_len, embed_dim)\n",
    "        self.X_rels = X_rels          # list of N lists of 10 tensors (seq_len, embed_dim)\n",
    "        self.X_irrels = X_irrels      # list of N lists of 10 tensors (seq_len, embed_dim)\n",
    "        self.n = len(X_queries)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n * 10\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triple_idx = idx // 10\n",
    "        doc_idx = idx % 10\n",
    "\n",
    "        qry = self.X_queries[triple_idx]              # (q_seq_len, embed_dim)\n",
    "        rel = self.X_rels[triple_idx][doc_idx]        # (rel_seq_len, embed_dim)\n",
    "        irrel = self.X_irrels[triple_idx][doc_idx]    # (irrel_seq_len, embed_dim)\n",
    "\n",
    "        return qry, rel, irrel\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    q_seqs, r_seqs, i_seqs = zip(*batch)\n",
    "    q_lens = [x.shape[0] for x in q_seqs]\n",
    "    r_lens = [x.shape[0] for x in r_seqs]\n",
    "    i_lens = [x.shape[0] for x in i_seqs]\n",
    "    q_padded = pad_sequence(q_seqs, batch_first=True)\n",
    "    r_padded = pad_sequence(r_seqs, batch_first=True)\n",
    "    i_padded = pad_sequence(i_seqs, batch_first=True)\n",
    "    return q_padded, r_padded, i_padded, q_lens, r_lens, i_lens\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader for batching\n",
    "batch_size = 32\n",
    "dataset = TripleDataset(query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44335531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 313/313 [01:36<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.1173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 313/313 [01:46<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Loss: 0.0490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 313/313 [01:38<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Loss: 0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 313/313 [02:02<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Loss: 0.0879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 313/313 [02:35<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Loss: 0.1506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 313/313 [02:35<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Loss: 0.1439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 313/313 [02:34<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Loss: 0.1373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 313/313 [01:50<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Loss: 0.1374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 313/313 [02:28<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Loss: 0.1361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 313/313 [02:16<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Loss: 0.1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 10. Train the model (change hyperparameters as needed)\n",
    "# -----------------------------\n",
    "\n",
    "# embed_dim defined in section 5. CBOW Model\n",
    "\n",
    "# Define hyperparameters\n",
    "hidden_dim = 128  # Dimension of the hidden state in RNNs (GRU/LSTM - can be adjusted)\n",
    "margin = 0.2  # Margin for triplet loss (can be adjusted)\n",
    "distance_function = cosine_distance  # Choose distance function (cosine_distance, euclidean_distance, manhattan_distance, squared_euclidean_distance, chebyshev_distance, minkowski_distance)\n",
    "\n",
    "qry_tower = QueryTower(embed_dim, hidden_dim)\n",
    "doc_tower = DocTower(embed_dim, hidden_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(qry_tower.parameters()) + list(doc_tower.parameters()), lr=1e-3)\n",
    "num_epochs = 10  # Set as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        qry_embeds, rel_embeds, irrel_embeds, q_lens, r_lens, i_lens = batch\n",
    "\n",
    "        qry_vecs    = qry_tower(qry_embeds, q_lens)\n",
    "        rel_vecs    = doc_tower(rel_embeds, r_lens)\n",
    "        irrel_vecs  = doc_tower(irrel_embeds, i_lens)\n",
    "\n",
    "        loss = triplet_loss_function(\n",
    "            qry_vecs, rel_vecs, irrel_vecs,\n",
    "            distance_function=distance_function,\n",
    "            margin=margin\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "# Save final model after all epochs are done\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'qry_tower_state_dict': qry_tower.state_dict(),\n",
    "    'doc_tower_state_dict': doc_tower.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': epoch_loss,  # from last epoch\n",
    "}, \"twotower_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05505892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 11. Inference: Encode documents and queries, find top-k relevant docs\n",
    "# -----------------------------\n",
    "\n",
    "def encode_documents(doc_tower, all_doc_embeds, all_doc_lens, device='cpu', batch_size=128):\n",
    "    doc_tower.eval()\n",
    "    all_vecs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(all_doc_embeds), batch_size):\n",
    "            batch_embeds = all_doc_embeds[i:i+batch_size].to(device)\n",
    "            batch_lens = all_doc_lens[i:i+batch_size]\n",
    "            vecs = doc_tower(batch_embeds, batch_lens)  # Shape: (batch, hidden_dim)\n",
    "            all_vecs.append(vecs.cpu())\n",
    "    return torch.cat(all_vecs, dim=0)  # Shape: (num_docs, hidden_dim)\n",
    "\n",
    "def encode_query(qry_tower, query_embed, query_len, device='cpu'):\n",
    "    qry_tower.eval()\n",
    "    with torch.no_grad():\n",
    "        query_vec = qry_tower(query_embed.to(device), query_len)\n",
    "    return query_vec.cpu()  # Shape: (1, hidden_dim)\n",
    "\n",
    "def find_top_k(query_vec, doc_vecs, k=5, distance_fn=None):\n",
    "    # query_vec: (1, hidden_dim), doc_vecs: (num_docs, hidden_dim)\n",
    "    if distance_fn is None:\n",
    "        # Default to cosine distance\n",
    "        def distance_fn(q, d):\n",
    "            return 1 - torch.nn.functional.cosine_similarity(q, d)\n",
    "    distances = distance_fn(query_vec, doc_vecs)\n",
    "    # If query_vec is (1,hidden_dim), expand to (num_docs,hidden_dim)\n",
    "    if query_vec.shape[0] == 1:\n",
    "        distances = distance_fn(query_vec.expand_as(doc_vecs), doc_vecs)\n",
    "    # Get top k smallest distances\n",
    "    topk = torch.topk(-distances, k)  # negative because smallest distance = highest relevance\n",
    "    indices = topk.indices.cpu().numpy()\n",
    "    scores = -topk.values.cpu().numpy()\n",
    "    return indices, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a33a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 12. Calculate all document embeddings and lengths\n",
    "# -----------------------------\n",
    "\n",
    "all_doc_texts = []  # To hold all relevant doc texts for padding\n",
    "all_query_texts = []\n",
    "\n",
    "for query, rels, irrels in triples:\n",
    "    all_query_texts.append(query)\n",
    "    # rels is the list of relevant doc texts for this query\n",
    "    # We slice to num_docs to match the embedding logic, then pad if needed\n",
    "    for doc in rels[:num_docs]:\n",
    "        all_doc_texts.append(doc)\n",
    "    # Pad if fewer than num_docs\n",
    "    while len(rels) < num_docs:\n",
    "        all_doc_texts.append(\"\")\n",
    "        rels.append(\"\")  # So embedding and text padding always match\n",
    "\n",
    "# Step 1: Gather your document embedding sequences and their lengths\n",
    "all_doc_embeds_list = []\n",
    "all_doc_lens_list = []\n",
    "\n",
    "for batch in dataloader:\n",
    "    rel_embeds, r_lens = batch[1], batch[4]  # rel_embeds: (batch, seq_len, embed_dim)\n",
    "    for i in range(rel_embeds.shape[0]):\n",
    "        all_doc_embeds_list.append(rel_embeds[i])  # (seq_len, embed_dim)\n",
    "        all_doc_lens_list.append(r_lens[i])\n",
    "\n",
    "# Step 2: Pad all embeddings to max seq_len\n",
    "# pad_sequence wants a list of (seq_len, embed_dim), returns (max_seq_len, num_docs, embed_dim)\n",
    "padded = pad_sequence(all_doc_embeds_list, batch_first=True)  # (num_docs, max_seq_len, embed_dim)\n",
    "\n",
    "all_doc_embeds = padded  # (num_docs, max_seq_len, embed_dim)\n",
    "all_doc_lens = torch.tensor(all_doc_lens_list)\n",
    "\n",
    "print(\"all_doc_embeds shape:\", all_doc_embeds.shape)\n",
    "print(\"all_doc_lens shape:\", all_doc_lens.shape)\n",
    "print(\"all_doc_texts length:\", len(all_doc_texts))\n",
    "print(\"all_query_texts length:\", len(all_query_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 13. Encode documents and queries, find top-k relevant docs\n",
    "# -----------------------------\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 1. Encode all documents\n",
    "doc_vecs = encode_documents(doc_tower, all_doc_embeds, all_doc_lens, device=device)\n",
    "#print(\"Encoded document vectors shape:\", doc_vecs.shape)\n",
    "\n",
    "# 2. Encode a sample query (here, just using first doc for demo)\n",
    "query_embed = all_doc_embeds[0].unsqueeze(0)\n",
    "query_len = all_doc_lens[0].unsqueeze(0)\n",
    "query_vec = encode_query(qry_tower, query_embed, query_len, device=device)\n",
    "#print(\"Encoded query vector shape:\", query_vec.shape)\n",
    "\n",
    "# 3. Retrieve top k relevant documents\n",
    "k = 5\n",
    "indices, scores = find_top_k(query_vec, doc_vecs, k=k)\n",
    "\n",
    "print(\"Query: \", all_query_texts[0])\n",
    "print(\"Top document matches:\")\n",
    "for rank, i in enumerate(indices):\n",
    "    print(f\"{rank+1}: {all_doc_texts[i]}\")\n",
    "    print(f\"   (score: {scores[rank]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d56ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 13b. Custom Query Inference\n",
    "# -----------------------------\n",
    "\n",
    "# Hardcode your query here\n",
    "custom_query = \"what is machine learning?\"\n",
    "\n",
    "# Tokenize and embed the custom query\n",
    "tokenized_query = tokenizer(custom_query)  # returns List[int] or torch.Tensor\n",
    "\n",
    "# Map tokens to ids, filtering out tokens not in vocab\n",
    "q_ids = [word_to_ix[t] for t in tokenized_query if t in word_to_ix]\n",
    "\n",
    "if q_ids:\n",
    "    with torch.no_grad():\n",
    "        q_vecs = cbow_model.embeddings(torch.tensor(q_ids))  # (seq_len, embed_dim)\n",
    "else:\n",
    "    q_vecs = torch.zeros(1, embed_dim)  # Fallback for empty/unknown queries)\n",
    "\n",
    "# Pad to (1, seq_len, embed_dim) for model\n",
    "query_embed = q_vecs.unsqueeze(0)  # (1, seq_len, embed_dim)\n",
    "query_len = torch.tensor([q_vecs.shape[0]])\n",
    "\n",
    "# Move to device if needed\n",
    "query_embed = query_embed.to(device)\n",
    "query_len = query_len.to(device)\n",
    "\n",
    "# Encode the hardcoded query\n",
    "query_vec = encode_query(qry_tower, query_embed, query_len, device=device)\n",
    "\n",
    "# Retrieve top k relevant documents\n",
    "k = 5\n",
    "indices, scores = find_top_k(query_vec, doc_vecs, k=k)\n",
    "\n",
    "print(\"Custom Query:\", custom_query)\n",
    "print(\"Top document matches:\")\n",
    "for rank, i in enumerate(indices):\n",
    "    print(f\"{rank+1}: {all_doc_texts[i]}\")\n",
    "    print(f\"   (score: {scores[rank]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26dddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 14. Full Inference test example\n",
    "# -----------------------------\n",
    "\n",
    "# Assume you have: \n",
    "#   - all_doc_embeds (Tensor: [num_docs, seq_len, embed_dim])\n",
    "#   - all_doc_lens (List/Tensor: [num_docs])\n",
    "#   - doc_tower, qry_tower loaded/trained\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "doc_tower = doc_tower.to(device)\n",
    "qry_tower = qry_tower.to(device)\n",
    "\n",
    "# 1. Encode all documents once\n",
    "doc_vecs = encode_documents(doc_tower, all_doc_embeds, all_doc_lens, device=device)\n",
    "\n",
    "# 2. Encode input query\n",
    "# query_embed: shape [1, seq_len, embed_dim]\n",
    "# query_len: shape [1]\n",
    "query_vec = encode_query(qry_tower, query_embed, query_len, device=device)\n",
    "\n",
    "# 3. Find top-k relevant docs (change k as you like)\n",
    "k = 5\n",
    "indices, scores = find_top_k(query_vec, doc_vecs, k=k)\n",
    "\n",
    "# 4. Output top k docs\n",
    "print(f\"Top {k} document indices:\", indices)\n",
    "print(f\"Corresponding scores (lower = more similar):\", scores)\n",
    "# If you have the original doc texts, you can do:\n",
    "print([all_doc_texts[i] for i in indices])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
