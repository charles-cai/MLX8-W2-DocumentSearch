{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c9376b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f517e0c9ee74782b3ed464b454ba54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8eb13740d34c238b8166d6f3445c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4ff1ad0eed45ef8791985b7e54f229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/175M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc835ffb383343279bce765d90e9adfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fc31ff7ba44068955ccacc82df870f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10047 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bd1ad233d542c08941c0839a813c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/82326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef66d999a2af4a6e8a782a3251b581d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/9650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[32m     10\u001b[39m     query = row[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     relevant = [\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpassage_text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m row[\u001b[33m'\u001b[39m\u001b[33mpassages\u001b[39m\u001b[33m'\u001b[39m]][:\u001b[32m10\u001b[39m]\n\u001b[32m     12\u001b[39m     relevant_set = \u001b[38;5;28mset\u001b[39m(relevant)\n\u001b[32m     13\u001b[39m     irrelevant_pool = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(all_passages) - relevant_set)\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Load MS MARCO V1.1 training dataset\n",
    "# -----------------------------\n",
    "\n",
    "# This will stream the data, you don't have to download the full file\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"train\")  # or \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdf890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building passage pool...:   0%|          | 0/82326 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building passage pool...: 100%|██████████| 82326/82326 [00:24<00:00, 3327.26it/s]\n",
      "Creating triples...:   1%|          | 999/82326 [04:14<5:45:39,  3.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is rba\n",
      "[\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", 'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ', 'The inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is made of some type of resistance wire, normally Kanthal or nichrome. When a current is applied to the coil (resistance wire), it heats up and the heated coil then vaporizes the eliquid. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.', 'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.', 'RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The “end” or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the user is.', 'A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.', 'Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. ']\n",
      "['DNA replication occurs in the cytoplasm of prokaryotes and in the nucleus of eukaryotes. Regardless of where DNA replication occurs, the basic process is the same. The structure of DNA lends itself easily to DNA replication.', 'Confidence votes 4.0K. Parathyroid hormone has effects antagonistic to those of calcitonin. It increases blood calcium levels by stimulating osteoclasts to break down bone and release calcium. It also increases gastrointestinal calcium absorption by activating vitamin D, and promotes calcium uptake by the kidneys. The hormone produced by the para follicular cells of the thyroid gland is calcitonin (CT). CT can decrease the level of calcium in the blood by inhibiting the action of … osteoclasts, the cells that break down bone extracellular matrix.', 'Code of Ethics. A code of ethics is a document, usually issued by a board of directors, that outlines a set of principles that affect decision-making. For example, a code of ethics might stipulate that XYZ Corporation is committed to environmental protection and green initiatives. Codes of ethics and conduct have proliferated in part because of increasing public concern about the way companies do business. Codes of ethics, which govern decision-making, and codes of conduct, which govern actions, represent two of the most common ways that companies self-regulate.', 'For anywhere from six to 12 months after hip replacement surgery, pivoting or twisting on the involved leg should be avoided. You should also not cross the involved leg past the midline of the body nor turn the involved leg inward and you should not bend at the hip past 90 degrees.', \"Two early sources for Bruegel's biography are Lodovico Guicciardini 's account of the Low Countries and Karel van Mander 's 1604 Schilder-boeck. Guicciardini recorded that Bruegel was born in Breda, but according to van Mander, he was born in Breugel near the (now Dutch) town of Eindhoven. Often Bruegel painted a community event, as in The Peasant Wedding and The Fight Between Carnival and Lent. In paintings like The Peasant Wedding, Bruegel painted individual, identifiable people while the people in The Fight Between Carnival and Lent are unidentifiable, muffin-faced allegories of greed or gluttony.\", '(United States). The average pay for a Medical Office Manager in Miami, Florida is $45,000 per year. The highest paying skills associated with this job are Human Resources, Operations Management, and Medical Credentialing. Most people with this job move on to other positions after 20 years in this career. ', 'Spirulina is a type of blue-green algae available as a dietary supplement in powdered, capsule or tablet form. According to MedlinePlus, spirulina is marketed as an alternative treatment for cardiovascular, digestive and immune system problems, but none of these claims are supported by scientific evidence. Like other sea vegetables such as seaweed, spirulina has a high concentration of iodine.', \"With a single shot, the Soviet Union vaulted ahead in the Space Race. The country sent Sputnik, the world's first artificial satellite, into space on Oct. 4, 1957. The small satellite brought the Soviet Union into the technological spotlight and demonstrated that the country was capable of modern feats. \", 'Washington, DC. On behalf of President Obama and the people of the United States, I extend my best wishes to the people of Paraguay as they celebrate 202 years of independence on May 15. ', 'Psychologists and other qualified mental health professionals use psychological tests to measure specific psychological constructs in individuals. This lesson will explore the different types of psychological tests and provide several examples. These are instruments used to measure how much of a specific psychological construct an individual has. Psychological tests are used to assess many areas, including: 1  Traits such as introversion and extroversion. 2  Certain conditions such as depression and anxiety.']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 2. Create triples of (query, relevant_docs, irrelevant_docs)\n",
    "# -----------------------------\n",
    "\n",
    "# Create a list of all passages to pass into loop to help create irrelevant documents\n",
    "# all_passages = set()\n",
    "# for row in tqdm(dataset, desc=\"Building passage pool...\"):\n",
    "#     all_passages.update(row['passages']['passage_text'])\n",
    "# all_passages = list(all_passages)\n",
    "\n",
    "# # Create triples of (query, relevant_docs, irrelevant_docs)\n",
    "# triples = []\n",
    "# for row in tqdm(dataset, desc=\"Creating triples...\"):\n",
    "#     query = row['query']\n",
    "#     relevant = row['passages']['passage_text'][:10]\n",
    "#     relevant_set = set(relevant)\n",
    "#     irrelevant_pool = list(set(all_passages) - relevant_set)\n",
    "#     irrelevant = random.sample(irrelevant_pool, 10)\n",
    "#     triples.append((query, relevant, irrelevant))\n",
    "#     if len(triples) >= 1000:\n",
    "#         break\n",
    "\n",
    "# # Save the triples to a file for later use\n",
    "# with open(\"triples_1000.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(triples, f)\n",
    "\n",
    "# with open(\"triples_1000.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(triples, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(triples[0][0])  # query\n",
    "# print(triples[0][1])  # 10 relevant docs\n",
    "# print(triples[0][2])  # 10 irrelevant docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2b. Load triples from file\n",
    "# -------------------------------\n",
    "\n",
    "# Load .pkl\n",
    "with open(\"triples_1000.pkl\", \"rb\") as f:\n",
    "    triples = pickle.load(f)\n",
    "\n",
    "# # Load .json\n",
    "# with open(\"triples_1000.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     triples = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "920cd3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['what', 'is', 'rba'], [['since', '2007', ',', 'the', 'rba', \"'\", 's', 'outstanding', 'reputation', 'has', 'been', 'affected', 'by', 'the', \"'\", 'securency', \"'\", 'or', 'npa', 'scandal', '.', 'these', 'rba', 'subsidiaries', 'were', 'involved', 'in', 'bribing', 'overseas', 'officials', 'so', 'that', 'australia', 'might', 'win', 'lucrative', 'note-printing', 'contracts', '.', 'the', 'assets', 'of', 'the', 'bank', 'include', 'the', 'gold', 'and', 'foreign', 'exchange', 'reserves', 'of', 'australia', ',', 'which', 'is', 'estimated', 'to', 'have', 'a', 'net', 'worth', 'of', 'a$101', 'billion', '.', 'nearly', '94%', 'of', 'the', 'rba', \"'\", 's', 'employees', 'work', 'at', 'its', 'headquarters', 'in', 'sydney', ',', 'new', 'south', 'wales', 'and', 'at', 'the', 'business', 'resumption', 'site', '.'], ['the', 'reserve', 'bank', 'of', 'australia', '(', 'rba', ')', 'came', 'into', 'being', 'on', '14', 'january', '1960', 'as', 'australia', \"'\", 's', 'central', 'bank', 'and', 'banknote', 'issuing', 'authority', ',', 'when', 'the', 'reserve', 'bank', 'act', '1959', 'removed', 'the', 'central', 'banking', 'functions', 'from', 'the', 'commonwealth', 'bank', '.', 'the', 'assets', 'of', 'the', 'bank', 'include', 'the', 'gold', 'and', 'foreign', 'exchange', 'reserves', 'of', 'australia', ',', 'which', 'is', 'estimated', 'to', 'have', 'a', 'net', 'worth', 'of', 'a$101', 'billion', '.', 'nearly', '94%', 'of', 'the', 'rba', \"'\", 's', 'employees', 'work', 'at', 'its', 'headquarters', 'in', 'sydney', ',', 'new', 'south', 'wales', 'and', 'at', 'the', 'business', 'resumption', 'site', '.'], ['rba', 'recognized', 'with', 'the', '2014', 'microsoft', 'us', 'regional', 'partner', 'of', 'the', '.', '.', '.', 'by', 'pr', 'newswire', '.', 'contract', 'awarded', 'for', 'supply', 'and', 'support', 'the', '.', 'securitisations', 'system', 'used', 'for', 'risk', 'management', 'and', 'analysis', '.'], ['the', 'inner', 'workings', 'of', 'a', 'rebuildable', 'atomizer', 'are', 'surprisingly', 'simple', '.', 'the', 'coil', 'inside', 'the', 'rba', 'is', 'made', 'of', 'some', 'type', 'of', 'resistance', 'wire', ',', 'normally', 'kanthal', 'or', 'nichrome', '.', 'when', 'a', 'current', 'is', 'applied', 'to', 'the', 'coil', '(', 'resistance', 'wire', ')', ',', 'it', 'heats', 'up', 'and', 'the', 'heated', 'coil', 'then', 'vaporizes', 'the', 'eliquid', '.', '1', 'the', 'bottom', 'feed', 'rba', 'is', ',', 'perhaps', ',', 'the', 'easiest', 'of', 'all', 'rba', 'types', 'to', 'build', ',', 'maintain', ',', 'and', 'use', '.', '2', 'it', 'is', 'filled', 'from', 'below', ',', 'much', 'like', 'bottom', 'coil', 'clearomizer', '.', '3', 'bottom', 'feed', 'rbas', 'can', 'utilize', 'cotton', 'instead', 'of', 'silica', 'for', 'the', 'wick', '.', '4', 'the', 'genesis', ',', 'or', 'genny', ',', 'is', 'a', 'top', 'feed', 'rba', 'that', 'utilizes', 'a', 'short', 'woven', 'mesh', 'wire', '.'], ['results-based', 'accountability®', '(', 'also', 'known', 'as', 'rba', ')', 'is', 'a', 'disciplined', 'way', 'of', 'thinking', 'and', 'taking', 'action', 'that', 'communities', 'can', 'use', 'to', 'improve', 'the', 'lives', 'of', 'children', ',', 'youth', ',', 'families', ',', 'adults', 'and', 'the', 'community', 'as', 'a', 'whole', '.', 'rba', 'is', 'also', 'used', 'by', 'organizations', 'to', 'improve', 'the', 'performance', 'of', 'their', 'programs', '.', 'rba', 'improves', 'the', 'lives', 'of', 'children', ',', 'families', ',', 'and', 'communities', 'and', 'the', 'performance', 'of', 'programs', 'because', 'rba', '1', 'gets', 'from', 'talk', 'to', 'action', 'quickly', '2', 'is', 'a', 'simple', ',', 'common', 'sense', 'process', 'that', 'everyone', 'can', 'understand', '3', 'helps', 'groups', 'to', 'surface', 'and', 'challenge', 'assumptions', 'that', 'can', 'be', 'barriers', 'to', 'innovation'], ['results-based', 'accountability®', '(', 'also', 'known', 'as', 'rba', ')', 'is', 'a', 'disciplined', 'way', 'of', 'thinking', 'and', 'taking', 'action', 'that', 'communities', 'can', 'use', 'to', 'improve', 'the', 'lives', 'of', 'children', ',', 'youth', ',', 'families', ',', 'adults', 'and', 'the', 'community', 'as', 'a', 'whole', '.', 'rba', 'is', 'also', 'used', 'by', 'organizations', 'to', 'improve', 'the', 'performance', 'of', 'their', 'programs', '.', 'creating', 'community', 'impact', 'with', 'rba', '.', 'community', 'impact', 'focuses', 'on', 'conditions', 'of', 'well-being', 'for', 'children', ',', 'families', 'and', 'the', 'community', 'as', 'a', 'whole', 'that', 'a', 'group', 'of', 'leaders', 'is', 'working', 'collectively', 'to', 'improve', '.', 'for', 'example', '“residents', 'with', 'good', 'jobs', ',', '”', '“children', 'ready', 'for', 'school', ',', '”', 'or', '“a', 'safe', 'and', 'clean', 'neighborhood”', '.'], ['rba', 'uses', 'a', 'data-driven', ',', 'decision-making', 'process', 'to', 'help', 'communities', 'and', 'organizations', 'get', 'beyond', 'talking', 'about', 'problems', 'to', 'taking', 'action', 'to', 'solve', 'problems', '.', 'it', 'is', 'a', 'simple', ',', 'common', 'sense', 'framework', 'that', 'everyone', 'can', 'understand', '.', 'rba', 'starts', 'with', 'ends', 'and', 'works', 'backward', ',', 'towards', 'means', '.', 'the', '“end”', 'or', 'difference', 'you', 'are', 'trying', 'to', 'make', 'looks', 'slightly', 'different', 'if', 'you', 'are', 'working', 'on', 'a', 'broad', 'community', 'level', 'or', 'are', 'focusing', 'on', 'your', 'specific', 'program', 'or', 'organization', '.', 'rba', 'improves', 'the', 'lives', 'of', 'children', ',', 'families', ',', 'and', 'communities', 'and', 'the', 'performance', 'of', 'programs', 'because', 'rba', '1', 'gets', 'from', 'talk', 'to', 'action', 'quickly', '2', 'is', 'a', 'simple', ',', 'common', 'sense', 'process', 'that', 'everyone', 'can', 'understand', '3', 'helps', 'groups', 'to', 'surface', 'and', 'challenge', 'assumptions', 'that', 'can', 'be', 'barriers', 'to', 'innovation'], ['vs', '.', 'netiq', 'identity', 'manager', '.', 'risk-based', 'authentication', '(', 'rba', ')', 'is', 'a', 'method', 'of', 'applying', 'varying', 'levels', 'of', 'stringency', 'to', 'authentication', 'processes', 'based', 'on', 'the', 'likelihood', 'that', 'access', 'to', 'a', 'given', 'system', 'could', 'result', 'in', 'its', 'being', 'compromised', '.', 'risk-based', 'authentication', 'can', 'be', 'categorized', 'as', 'either', 'user-dependent', 'or', 'transaction-dependent', '.', 'user-dependent', 'rba', 'processes', 'employ', 'the', 'same', 'authentication', 'for', 'every', 'session', 'initiated', 'by', 'a', 'given', 'user', 'the', 'exact', 'credentials', 'that', 'the', 'site', 'demands', 'depend', 'on', 'who', 'the', 'user', 'is', '.'], ['a', 'rebuildable', 'atomizer', '(', 'rba', ')', ',', 'often', 'referred', 'to', 'as', 'simply', 'a', '“rebuildable', ',', '”', 'is', 'just', 'a', 'special', 'type', 'of', 'atomizer', 'used', 'in', 'the', 'vape', 'pen', 'and', 'mod', 'industry', 'that', 'connects', 'to', 'a', 'personal', 'vaporizer', '.', '1', 'the', 'bottom', 'feed', 'rba', 'is', ',', 'perhaps', ',', 'the', 'easiest', 'of', 'all', 'rba', 'types', 'to', 'build', ',', 'maintain', ',', 'and', 'use', '.', '2', 'it', 'is', 'filled', 'from', 'below', ',', 'much', 'like', 'bottom', 'coil', 'clearomizer', '.', '3', 'bottom', 'feed', 'rbas', 'can', 'utilize', 'cotton', 'instead', 'of', 'silica', 'for', 'the', 'wick', '.', '4', 'the', 'genesis', ',', 'or', 'genny', ',', 'is', 'a', 'top', 'feed', 'rba', 'that', 'utilizes', 'a', 'short', 'woven', 'mesh', 'wire', '.'], ['get', 'to', 'know', 'us', '.', 'rba', 'is', 'a', 'digital', 'and', 'technology', 'consultancy', 'with', 'roots', 'in', 'strategy', ',', 'design', 'and', 'technology', '.', 'our', 'team', 'of', 'specialists', 'help', 'progressive', 'companies', 'deliver', 'modern', 'digital', 'experiences', 'backed', 'by', 'proven', 'technology', 'engineering', '.']], [['dna', 'replication', 'occurs', 'in', 'the', 'cytoplasm', 'of', 'prokaryotes', 'and', 'in', 'the', 'nucleus', 'of', 'eukaryotes', '.', 'regardless', 'of', 'where', 'dna', 'replication', 'occurs', ',', 'the', 'basic', 'process', 'is', 'the', 'same', '.', 'the', 'structure', 'of', 'dna', 'lends', 'itself', 'easily', 'to', 'dna', 'replication', '.'], ['confidence', 'votes', '4', '.', '0k', '.', 'parathyroid', 'hormone', 'has', 'effects', 'antagonistic', 'to', 'those', 'of', 'calcitonin', '.', 'it', 'increases', 'blood', 'calcium', 'levels', 'by', 'stimulating', 'osteoclasts', 'to', 'break', 'down', 'bone', 'and', 'release', 'calcium', '.', 'it', 'also', 'increases', 'gastrointestinal', 'calcium', 'absorption', 'by', 'activating', 'vitamin', 'd', ',', 'and', 'promotes', 'calcium', 'uptake', 'by', 'the', 'kidneys', '.', 'the', 'hormone', 'produced', 'by', 'the', 'para', 'follicular', 'cells', 'of', 'the', 'thyroid', 'gland', 'is', 'calcitonin', '(', 'ct', ')', '.', 'ct', 'can', 'decrease', 'the', 'level', 'of', 'calcium', 'in', 'the', 'blood', 'by', 'inhibiting', 'the', 'action', 'of', '…', 'osteoclasts', ',', 'the', 'cells', 'that', 'break', 'down', 'bone', 'extracellular', 'matrix', '.'], ['code', 'of', 'ethics', '.', 'a', 'code', 'of', 'ethics', 'is', 'a', 'document', ',', 'usually', 'issued', 'by', 'a', 'board', 'of', 'directors', ',', 'that', 'outlines', 'a', 'set', 'of', 'principles', 'that', 'affect', 'decision-making', '.', 'for', 'example', ',', 'a', 'code', 'of', 'ethics', 'might', 'stipulate', 'that', 'xyz', 'corporation', 'is', 'committed', 'to', 'environmental', 'protection', 'and', 'green', 'initiatives', '.', 'codes', 'of', 'ethics', 'and', 'conduct', 'have', 'proliferated', 'in', 'part', 'because', 'of', 'increasing', 'public', 'concern', 'about', 'the', 'way', 'companies', 'do', 'business', '.', 'codes', 'of', 'ethics', ',', 'which', 'govern', 'decision-making', ',', 'and', 'codes', 'of', 'conduct', ',', 'which', 'govern', 'actions', ',', 'represent', 'two', 'of', 'the', 'most', 'common', 'ways', 'that', 'companies', 'self-regulate', '.'], ['for', 'anywhere', 'from', 'six', 'to', '12', 'months', 'after', 'hip', 'replacement', 'surgery', ',', 'pivoting', 'or', 'twisting', 'on', 'the', 'involved', 'leg', 'should', 'be', 'avoided', '.', 'you', 'should', 'also', 'not', 'cross', 'the', 'involved', 'leg', 'past', 'the', 'midline', 'of', 'the', 'body', 'nor', 'turn', 'the', 'involved', 'leg', 'inward', 'and', 'you', 'should', 'not', 'bend', 'at', 'the', 'hip', 'past', '90', 'degrees', '.'], ['two', 'early', 'sources', 'for', 'bruegel', \"'\", 's', 'biography', 'are', 'lodovico', 'guicciardini', \"'\", 's', 'account', 'of', 'the', 'low', 'countries', 'and', 'karel', 'van', 'mander', \"'\", 's', '1604', 'schilder-boeck', '.', 'guicciardini', 'recorded', 'that', 'bruegel', 'was', 'born', 'in', 'breda', ',', 'but', 'according', 'to', 'van', 'mander', ',', 'he', 'was', 'born', 'in', 'breugel', 'near', 'the', '(', 'now', 'dutch', ')', 'town', 'of', 'eindhoven', '.', 'often', 'bruegel', 'painted', 'a', 'community', 'event', ',', 'as', 'in', 'the', 'peasant', 'wedding', 'and', 'the', 'fight', 'between', 'carnival', 'and', 'lent', '.', 'in', 'paintings', 'like', 'the', 'peasant', 'wedding', ',', 'bruegel', 'painted', 'individual', ',', 'identifiable', 'people', 'while', 'the', 'people', 'in', 'the', 'fight', 'between', 'carnival', 'and', 'lent', 'are', 'unidentifiable', ',', 'muffin-faced', 'allegories', 'of', 'greed', 'or', 'gluttony', '.'], ['(', 'united', 'states', ')', '.', 'the', 'average', 'pay', 'for', 'a', 'medical', 'office', 'manager', 'in', 'miami', ',', 'florida', 'is', '$45', ',', '000', 'per', 'year', '.', 'the', 'highest', 'paying', 'skills', 'associated', 'with', 'this', 'job', 'are', 'human', 'resources', ',', 'operations', 'management', ',', 'and', 'medical', 'credentialing', '.', 'most', 'people', 'with', 'this', 'job', 'move', 'on', 'to', 'other', 'positions', 'after', '20', 'years', 'in', 'this', 'career', '.'], ['spirulina', 'is', 'a', 'type', 'of', 'blue-green', 'algae', 'available', 'as', 'a', 'dietary', 'supplement', 'in', 'powdered', ',', 'capsule', 'or', 'tablet', 'form', '.', 'according', 'to', 'medlineplus', ',', 'spirulina', 'is', 'marketed', 'as', 'an', 'alternative', 'treatment', 'for', 'cardiovascular', ',', 'digestive', 'and', 'immune', 'system', 'problems', ',', 'but', 'none', 'of', 'these', 'claims', 'are', 'supported', 'by', 'scientific', 'evidence', '.', 'like', 'other', 'sea', 'vegetables', 'such', 'as', 'seaweed', ',', 'spirulina', 'has', 'a', 'high', 'concentration', 'of', 'iodine', '.'], ['with', 'a', 'single', 'shot', ',', 'the', 'soviet', 'union', 'vaulted', 'ahead', 'in', 'the', 'space', 'race', '.', 'the', 'country', 'sent', 'sputnik', ',', 'the', 'world', \"'\", 's', 'first', 'artificial', 'satellite', ',', 'into', 'space', 'on', 'oct', '.', '4', ',', '1957', '.', 'the', 'small', 'satellite', 'brought', 'the', 'soviet', 'union', 'into', 'the', 'technological', 'spotlight', 'and', 'demonstrated', 'that', 'the', 'country', 'was', 'capable', 'of', 'modern', 'feats', '.'], ['washington', ',', 'dc', '.', 'on', 'behalf', 'of', 'president', 'obama', 'and', 'the', 'people', 'of', 'the', 'united', 'states', ',', 'i', 'extend', 'my', 'best', 'wishes', 'to', 'the', 'people', 'of', 'paraguay', 'as', 'they', 'celebrate', '202', 'years', 'of', 'independence', 'on', 'may', '15', '.'], ['psychologists', 'and', 'other', 'qualified', 'mental', 'health', 'professionals', 'use', 'psychological', 'tests', 'to', 'measure', 'specific', 'psychological', 'constructs', 'in', 'individuals', '.', 'this', 'lesson', 'will', 'explore', 'the', 'different', 'types', 'of', 'psychological', 'tests', 'and', 'provide', 'several', 'examples', '.', 'these', 'are', 'instruments', 'used', 'to', 'measure', 'how', 'much', 'of', 'a', 'specific', 'psychological', 'construct', 'an', 'individual', 'has', '.', 'psychological', 'tests', 'are', 'used', 'to', 'assess', 'many', 'areas', ',', 'including', '1', 'traits', 'such', 'as', 'introversion', 'and', 'extroversion', '.', '2', 'certain', 'conditions', 'such', 'as', 'depression', 'and', 'anxiety', '.']])\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 3. Tokenise triples\n",
    "# -------------------------------\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Lists to hold the tokenized items\n",
    "tokenized_queries = []\n",
    "tokenized_relevant_docs = []\n",
    "tokenized_irrelevant_docs = []\n",
    "\n",
    "for query, rels, irrels in triples:\n",
    "    # Tokenize the query\n",
    "    tokenized_query = tokenizer(query)\n",
    "    tokenized_queries.append(tokenized_query)\n",
    "    \n",
    "    # Tokenize relevant docs\n",
    "    tokenized_rels = [tokenizer(doc) for doc in rels]\n",
    "    tokenized_relevant_docs.append(tokenized_rels)\n",
    "    \n",
    "    # Tokenize irrelevant docs\n",
    "    tokenized_irrels = [tokenizer(doc) for doc in irrels]\n",
    "    tokenized_irrelevant_docs.append(tokenized_irrels)\n",
    "\n",
    "tokenized_triples = []\n",
    "for query, rels, irrels in triples:\n",
    "    tokenized_query = tokenizer(query)\n",
    "    tokenized_rels = [tokenizer(doc) for doc in rels]\n",
    "    tokenized_irrels = [tokenizer(doc) for doc in irrels]\n",
    "    tokenized_triples.append((tokenized_query, tokenized_rels, tokenized_irrels))\n",
    "\n",
    "# Check the first tokenized items\n",
    "print(tokenized_triples[0])  # Tokenized query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99ab125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Load Vocabulary of CBOW model\n",
    "# -----------------------------\n",
    "with open(\"vocab_new.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_to_ix = json.load(f)\n",
    "\n",
    "ix_to_word = {int(i): w for w, i in word_to_ix.items()}\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Load Pre-trained Embeddings (placeholder)\n",
    "# -----------------------------\n",
    "embed_dim = 200  \n",
    "state = torch.load(\"text8_cbow_embeddings.pth\", map_location='cpu')  # Shape: [vocab_size, embed_dim]\n",
    "embeddings = state[\"embeddings.weight\"] \n",
    "\n",
    "assert embeddings.shape[0] == vocab_size, \"Vocab size mismatch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fdcdbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. CBOW Model\n",
    "# -----------------------------\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).mean(dim=1)\n",
    "        return self.linear(embeds)\n",
    "\n",
    "cbow_model = CBOW(vocab_size, embed_dim)\n",
    "cbow_model.embeddings.weight.data.copy_(embeddings)\n",
    "cbow_model.embeddings.weight.requires_grad = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7b0907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 200])\n",
      "torch.Size([1000, 10, 200])\n",
      "torch.Size([1000, 10, 200])\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 7. Create query, relevant document and irrelevant document embeddings\n",
    "# -----------------------------\n",
    "\n",
    "num_docs = 10  # number of relevant/irrelevant docs you expect per triple\n",
    "\n",
    "query_embeddings = []\n",
    "relevant_doc_embeddings = []\n",
    "irrelevant_doc_embeddings = []\n",
    "\n",
    "for i, (tokenized_query, tokenized_rels, tokenized_irrels) in enumerate(tokenized_triples):\n",
    "    # Query\n",
    "    q_ids = [word_to_ix[t] for t in tokenized_query if t in word_to_ix]\n",
    "    if q_ids:\n",
    "        with torch.no_grad():\n",
    "            q_vecs = cbow_model.embeddings(torch.tensor(q_ids))\n",
    "            q_emb = q_vecs.mean(dim=0)\n",
    "        query_embeddings.append(q_emb)\n",
    "    else:\n",
    "        query_embeddings.append(torch.zeros(embed_dim))\n",
    "    \n",
    "    # Relevant docs (pad to num_docs)\n",
    "    rel_embs = []\n",
    "    for doc_tokens in tokenized_rels[:num_docs]:\n",
    "        doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "        if doc_ids:\n",
    "            with torch.no_grad():\n",
    "                doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "                doc_emb = doc_vecs.mean(dim=0)\n",
    "            rel_embs.append(doc_emb)\n",
    "        else:\n",
    "            rel_embs.append(torch.zeros(embed_dim))\n",
    "    # Pad if fewer than num_docs\n",
    "    while len(rel_embs) < num_docs:\n",
    "        rel_embs.append(torch.zeros(embed_dim))\n",
    "    relevant_doc_embeddings.append(torch.stack(rel_embs))\n",
    "    \n",
    "    # Irrelevant docs (pad to num_docs)\n",
    "    irrel_embs = []\n",
    "    for doc_tokens in tokenized_irrels[:num_docs]:\n",
    "        doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "        if doc_ids:\n",
    "            with torch.no_grad():\n",
    "                doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "                doc_emb = doc_vecs.mean(dim=0)\n",
    "            irrel_embs.append(doc_emb)\n",
    "        else:\n",
    "            irrel_embs.append(torch.zeros(embed_dim))\n",
    "    while len(irrel_embs) < num_docs:\n",
    "        irrel_embs.append(torch.zeros(embed_dim))\n",
    "    irrelevant_doc_embeddings.append(torch.stack(irrel_embs))\n",
    "\n",
    "# Now you can safely stack\n",
    "X_queries = torch.stack(query_embeddings)                  # [n, embed_dim]\n",
    "X_rels = torch.stack(relevant_doc_embeddings)              # [n, 10, embed_dim]\n",
    "X_irrels = torch.stack(irrelevant_doc_embeddings)          # [n, 10, embed_dim]\n",
    "\n",
    "print(X_queries.shape)  # Should be [1000, embed_dim]\n",
    "print(X_rels.shape)     # Should be [1000, 10, embed_dim]\n",
    "print(X_irrels.shape)   # Should be [1000, 10, embed_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7. Define Two Tower Model (QueryTower and DocTower)\n",
    "# -----------------------------\n",
    "\n",
    "class QueryTower(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers=1, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown rnn_type: choose 'gru' or 'lstm'\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, embed_dim)\n",
    "        out, h = self.rnn(x)\n",
    "        # If LSTM, h is a tuple (h_n, c_n); for GRU, h is just h_n\n",
    "        if isinstance(h, tuple):\n",
    "            h = h[0]\n",
    "        # h: (num_layers, batch, hidden_dim)\n",
    "        # Use the last layer's hidden state\n",
    "        return h[-1]  # (batch, hidden_dim)\n",
    "\n",
    "class DocTower(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers=1, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown rnn_type: choose 'gru' or 'lstm'\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, h = self.rnn(x)\n",
    "        if isinstance(h, tuple):\n",
    "            h = h[0]\n",
    "        return h[-1]  # (batch, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 9. \n",
    "# -----------------------------\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return F.cosine_similarity(x, y, dim=1)  # (batch,)\n",
    "\n",
    "def cosine_distance(x, y):\n",
    "    return 1 - cosine_similarity(x, y)       # (batch,)\n",
    "\n",
    "def triplet_loss_function(query, relevant_doc, irrelevant_doc, distance_function, margin):\n",
    "    # query, relevant_doc, irrelevant_doc: (batch, dim)\n",
    "    relevant_distance = distance_function(query, relevant_doc)       # (batch,)\n",
    "    irrelevant_distance = distance_function(query, irrelevant_doc)   # (batch,)\n",
    "    # Triplet loss per sample: max(0, rel_dist - irrel_dist + margin)\n",
    "    triplet_loss = torch.relu(relevant_distance - irrelevant_distance + margin)\n",
    "    return triplet_loss.mean()  # Mean over batch\n",
    "\n",
    "# Example setup\n",
    "embed_dim = 200      # Dimension of your CBOW word embeddings\n",
    "hidden_dim = 128     # You choose this (can tune it)\n",
    "num_layers = 1\n",
    "\n",
    "qry_tower = QueryTower(embed_dim, hidden_dim, num_layers)\n",
    "doc_tower = DocTower(embed_dim, hidden_dim, num_layers)\n",
    "\n",
    "# Example data (single instance, batch size = 1)\n",
    "# Suppose q_embeds, rel_embeds, irrel_embeds are (seq_len, embed_dim)\n",
    "# Add batch dimension: (1, seq_len, embed_dim)\n",
    "\n",
    "q_vec = qry_tower(q_embeds.unsqueeze(0))           # (1, hidden_dim)\n",
    "rel_vec = doc_tower(rel_embeds.unsqueeze(0))       # (1, hidden_dim)\n",
    "irrel_vec = doc_tower(irrel_embeds.unsqueeze(0))   # (1, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66900e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet loss function\n",
    "\n",
    "def triplet_loss_function((query, relevant_document, irrelevant_document), distance_function, margin):\n",
    "    relevant_distance = distance_function(query, relevant_document)\n",
    "    irrelevant_distance = distance_function(query, irrelevant_document)\n",
    "    triplet_loss = max(0, relevant_distance - irrelevant_distance + margin)\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine distance function\n",
    "def cosine_similarity(x, y):\n",
    "    return F.cosine_similarity(x, y, dim=1)\n",
    "\n",
    "cosine_distance(x,y) = 1 - cosine_similarity(x, y)\n",
    "\n",
    "# To minimise distance between query & relevant doc - Maximise cosine similarity\n",
    "def distance_function(query, relevant_document):\n",
    "    return cosine_distance(query, relevant_document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
