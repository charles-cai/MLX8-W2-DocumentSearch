{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c24b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95c9376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # 1. Load MS MARCO V1.1 training dataset\n",
    "# # -----------------------------\n",
    "\n",
    "# # This will stream the data, you don't have to download the full file\n",
    "# dataset = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"train\")  # or \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ffdf890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # 2. Create triples of (query, relevant_docs, irrelevant_docs)\n",
    "# # -----------------------------\n",
    "\n",
    "# # 1. Build passage pool and index mapping for fast sampling and seed for reproducibility\n",
    "# passage_to_idx = dict()\n",
    "# idx_to_passage = []\n",
    "# for row in tqdm(dataset, desc=\"Building passage pool...\"):\n",
    "#     for p in row['passages']['passage_text']:\n",
    "#         if p not in passage_to_idx:\n",
    "#             passage_to_idx[p] = len(idx_to_passage)\n",
    "#             idx_to_passage.append(p)\n",
    "# num_passages = len(idx_to_passage)\n",
    "\n",
    "# # 2. For each query, map relevant passage indices\n",
    "# triples = []\n",
    "# for row in tqdm(dataset, desc=\"Creating triples...\"):\n",
    "#     query = row['query']\n",
    "#     relevant_passages = row['passages']['passage_text'][:10]\n",
    "#     relevant_indices = [passage_to_idx[p] for p in relevant_passages]\n",
    "    \n",
    "#     # For fast sampling: mask out relevant indices\n",
    "#     mask = np.ones(num_passages, dtype=bool)\n",
    "#     mask[relevant_indices] = False\n",
    "#     irrelevant_indices = np.random.choice(np.where(mask)[0], 10, replace=False)\n",
    "#     irrelevant_passages = [idx_to_passage[i] for i in irrelevant_indices]\n",
    "\n",
    "#     triples.append((query, relevant_passages, irrelevant_passages))\n",
    "\n",
    "# with open(\"triples_full.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(triples, f)\n",
    "\n",
    "# print(triples[0][0])  # query\n",
    "# print(triples[0][1])  # 10 relevant docs\n",
    "# print(triples[0][2])  # 10 irrelevant docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1b. Load triples from file & tokenize\n",
    "# -------------------------------\n",
    "\n",
    "# Load triples\n",
    "with open(\"triples_full.pkl\", \"rb\") as f:\n",
    "    triples = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca5bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing triples: 100%|██████████| 82326/82326 [03:59<00:00, 343.64it/s] \n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1c. Tokenize triples\n",
    "# -------------------------------\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9 ]+', '', text)\n",
    "    return text.split()\n",
    "\n",
    "tokenized_triples = []\n",
    "for query, rel_docs, irrel_docs in tqdm(triples, desc=\"Tokenizing triples\"):\n",
    "    tokenized_query = preprocess(query)\n",
    "    tokenized_rels = [preprocess(doc) for doc in rel_docs]\n",
    "    tokenized_irrels = [preprocess(doc) for doc in irrel_docs]\n",
    "    tokenized_triples.append((tokenized_query, tokenized_rels, tokenized_irrels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Load Vocabulary & Pre-trained Embeddings\n",
    "# -----------------------------\n",
    "with open(\"vocab_new.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_to_ix = json.load(f)\n",
    "\n",
    "ix_to_word = {int(i): w for w, i in word_to_ix.items()}\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "embed_dim = 200  \n",
    "state = torch.load(\"text8_cbow_embeddings.pth\", map_location='cpu')  # Shape: [vocab_size, embed_dim]\n",
    "embeddings = state[\"embeddings.weight\"] \n",
    "\n",
    "assert embeddings.shape[0] == vocab_size, \"Vocab size mismatch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. CBOW Model\n",
    "# -----------------------------\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).mean(dim=1)\n",
    "        return self.linear(embeds)\n",
    "\n",
    "cbow_model = CBOW(vocab_size, embed_dim)\n",
    "cbow_model.embeddings.weight.data.copy_(embeddings)\n",
    "cbow_model.embeddings.weight.requires_grad = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920cd3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CBOW embedding + streaming: 100%|██████████| 5120/5120 [00:36<00:00, 141.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# # -------------------------------\n",
    "# # 4. Embed queries, rel and irrel documents using pre-trained CBOW model\n",
    "# # -------------------------------\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "# #torch.set_num_threads(12)\n",
    "\n",
    "# num_docs = 10  # number of relevant/irrelevant docs you expect per triple\n",
    "# batch_size = 64  # adjust to taste; big enough to speed up, small enough to never threaten RAM\n",
    "\n",
    "# query_embeddings = []\n",
    "# relevant_doc_embeddings = []\n",
    "# irrelevant_doc_embeddings = []\n",
    "\n",
    "# def process_and_save_embeddings(\n",
    "#     tokenized_triples, word_to_ix, cbow_model, \n",
    "#     query_embeds_path, rel_doc_embeds_path, irrel_doc_embeds_path,\n",
    "#     batch_size=batch_size, num_docs=num_docs\n",
    "# ):\n",
    "#     query_embeds_batch = []\n",
    "#     rel_doc_embeds_batch = []\n",
    "#     irrel_doc_embeds_batch = []\n",
    "    \n",
    "#     for i, (tokenized_query, tokenized_rels, tokenized_irrels) in enumerate(\n",
    "#         tqdm(tokenized_triples, desc=\"CBOW embedding + streaming\", total=len(tokenized_triples))\n",
    "#     ):\n",
    "#         # Query: embeddings per token\n",
    "#         q_ids = [word_to_ix[t] for t in tokenized_query if t in word_to_ix]\n",
    "#         if q_ids:\n",
    "#             with torch.no_grad():\n",
    "#                 q_vecs = cbow_model.embeddings(torch.tensor(q_ids))\n",
    "#             query_embeds_batch.append(q_vecs)  # shape: [query_len, embed_dim]\n",
    "#         else:\n",
    "#             query_embeds_batch.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "        \n",
    "#         # Relevant docs: list of (doc_len, embed_dim)\n",
    "#         rel_embs = []\n",
    "#         for doc_tokens in tokenized_rels[:num_docs]:\n",
    "#             doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "#             if doc_ids:\n",
    "#                 with torch.no_grad():\n",
    "#                     doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "#                 rel_embs.append(doc_vecs)\n",
    "#             else:\n",
    "#                 rel_embs.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "#         while len(rel_embs) < num_docs:\n",
    "#             rel_embs.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "#         rel_doc_embeds_batch.append(rel_embs)\n",
    "\n",
    "#         # Irrelevant docs: list of (doc_len, embed_dim)\n",
    "#         irrel_embs = []\n",
    "#         for doc_tokens in tokenized_irrels[:num_docs]:\n",
    "#             doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "#             if doc_ids:\n",
    "#                 with torch.no_grad():\n",
    "#                     doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "#                 irrel_embs.append(doc_vecs)\n",
    "#             else:\n",
    "#                 irrel_embs.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))(0)\n",
    "#         while len(irrel_embs) < num_docs:\n",
    "#             irrel_embs.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "#         irrel_doc_embeds_batch.append(irrel_embs)\n",
    "\n",
    "#         # Save every batch_size triples\n",
    "#         if (i + 1) % batch_size == 0 or (i + 1) == len(tokenized_triples):\n",
    "#             # Save as batch-lists (not stacked), because sequences are ragged\n",
    "#             with open(query_embeds_path, 'ab') as fq:\n",
    "#                 pickle.dump(query_embeds_batch, fq)\n",
    "#             with open(rel_doc_embeds_path, 'ab') as fr:\n",
    "#                 pickle.dump(rel_doc_embeds_batch, fr)\n",
    "#             with open(irrel_doc_embeds_path, 'ab') as fi:\n",
    "#                 pickle.dump(irrel_doc_embeds_batch, fi)\n",
    "\n",
    "#             # Free RAM\n",
    "#             query_embeds_batch.clear()\n",
    "#             rel_doc_embeds_batch.clear()\n",
    "#             irrel_doc_embeds_batch.clear()\n",
    "\n",
    "\n",
    "# # Usage example:\n",
    "# process_and_save_embeddings(\n",
    "#     tokenized_triples[:5120], word_to_ix, cbow_model,\n",
    "#     \"query_embeds.pkl\", \"rel_doc_embeds.pkl\", \"irrel_doc_embeds.pkl\",\n",
    "#     batch_size=batch_size, num_docs=num_docs\n",
    "# )\n",
    "\n",
    "# # # Usage:\n",
    "# # process_and_save_embeddings(\n",
    "# #     tokenized_triples, word_to_ix, cbow_model,\n",
    "# #     \"query_embeds.pkl\", \"rel_doc_embeds.pkl\", \"irrel_doc_embeds.pkl\",\n",
    "# #     batch_size=2048, num_docs=10\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Load Embeddings\n",
    "# -----------------------------\n",
    "\n",
    "def load_all_batches(path):\n",
    "    all_data = []\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                batch = pickle.load(f)\n",
    "                all_data.extend(batch)  # for lists of tensors, this flattens batches\n",
    "            except EOFError:\n",
    "                break\n",
    "    return all_data\n",
    "\n",
    "# Load them all\n",
    "query_embeds = load_all_batches(\"query_embeds.pkl\")           # list of [query_len, embed_dim] tensors\n",
    "rel_doc_embeds = load_all_batches(\"rel_doc_embeds.pkl\")       # list of lists: each is [num_docs] of [doc_len, embed_dim] tensors\n",
    "irrel_doc_embeds = load_all_batches(\"irrel_doc_embeds.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2822065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Define distance function & Triplet loss function\n",
    "# -----------------------------\n",
    "\n",
    "# Cosine similarity for calculation of cosine distance\n",
    "def cosine_similarity(x, y):\n",
    "    return F.cosine_similarity(x, y, dim=1)\n",
    "\n",
    "# Triplet loss function - will compute the loss for a batch of triplets\n",
    "def triplet_loss_function(query, relevant_doc, irrelevant_doc, distance_function, margin):\n",
    "    rel_dist = distance_function(query, relevant_doc)         # (batch,)\n",
    "    irrel_dist = distance_function(query, irrelevant_doc)     # (batch,)\n",
    "    triplet_loss = torch.relu(rel_dist - irrel_dist + margin)\n",
    "    return triplet_loss.mean()                                # Average over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Define Two Tower Model (QueryTower and DocTower)\n",
    "# -----------------------------\n",
    "\n",
    "class QueryTower(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers=1, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown rnn_type: choose 'gru' or 'lstm'\")\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (batch, seq_len, embed_dim)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, h = self.rnn(packed)\n",
    "        if isinstance(h, tuple):  # LSTM\n",
    "            h = h[0]\n",
    "        return h[-1]  # (batch, hidden_dim)\n",
    "\n",
    "class DocTower(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers=1, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown rnn_type: choose 'gru' or 'lstm'\")\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, h = self.rnn(packed)\n",
    "        if isinstance(h, tuple):\n",
    "            h = h[0]\n",
    "        return h[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab9d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Prepare dataset for DataLoader\n",
    "# -----------------------------\n",
    "\n",
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, X_queries, X_rels, X_irrels):\n",
    "        self.X_queries = X_queries    # list of N tensors (seq_len, embed_dim)\n",
    "        self.X_rels = X_rels          # list of N lists of 10 tensors (seq_len, embed_dim)\n",
    "        self.X_irrels = X_irrels      # list of N lists of 10 tensors (seq_len, embed_dim)\n",
    "        self.n = len(X_queries)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n * 10\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triple_idx = idx // 10\n",
    "        doc_idx = idx % 10\n",
    "\n",
    "        qry = self.X_queries[triple_idx]              # (q_seq_len, embed_dim)\n",
    "        rel = self.X_rels[triple_idx][doc_idx]        # (rel_seq_len, embed_dim)\n",
    "        irrel = self.X_irrels[triple_idx][doc_idx]    # (irrel_seq_len, embed_dim)\n",
    "\n",
    "        return qry, rel, irrel\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    q_seqs, r_seqs, i_seqs = zip(*batch)\n",
    "    q_lens = [x.shape[0] for x in q_seqs]\n",
    "    r_lens = [x.shape[0] for x in r_seqs]\n",
    "    i_lens = [x.shape[0] for x in i_seqs]\n",
    "    q_padded = pad_sequence(q_seqs, batch_first=True)\n",
    "    r_padded = pad_sequence(r_seqs, batch_first=True)\n",
    "    i_padded = pad_sequence(i_seqs, batch_first=True)\n",
    "    return q_padded, r_padded, i_padded, q_lens, r_lens, i_lens\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader for batching\n",
    "batch_size = 64\n",
    "dataset = TripleDataset(query_embeds, rel_doc_embeds, irrel_doc_embeds)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44335531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 1600/1600 [10:28<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Loss: 0.0916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 1600/1600 [08:48<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Loss: 0.0322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 1600/1600 [09:45<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Loss: 0.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 1600/1600 [10:03<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Loss: 0.0088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 1600/1600 [11:44<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Loss: 0.0055\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 7. Train the model\n",
    "# -----------------------------\n",
    "\n",
    "# embed_dim defined in section 3.\n",
    "\n",
    "# Define hyperparameters\n",
    "hidden_dim = 128  # Dimension of the hidden state in RNNs (GRU/LSTM - can be adjusted)\n",
    "margin = 0.2  # Margin for triplet loss (can be adjusted)\n",
    "distance_function = cosine_similarity)\n",
    "\n",
    "qry_tower = QueryTower(embed_dim, hidden_dim, rnn_type='gru')  # or 'lstm'\n",
    "doc_tower = DocTower(embed_dim, hidden_dim, rnn_type='gru')  # or 'lstm'\n",
    "\n",
    "optimizer = torch.optim.Adam(list(qry_tower.parameters()) + list(doc_tower.parameters()), lr=1e-3)\n",
    "num_epochs = 5  # Set as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        qry_embeds, rel_embeds, irrel_embeds, q_lens, r_lens, i_lens = batch\n",
    "        # qry_embeds: (batch, q_seq_len, embed_dim)\n",
    "        # rel_embeds: (batch, r_seq_len, embed_dim)\n",
    "        # irrel_embeds: (batch, i_seq_len, embed_dim)\n",
    "\n",
    "        # Mask: keep items where rel doc is *not* all zeros\n",
    "        # mask shape: (batch,)\n",
    "        mask = ~torch.all(rel_embeds == 0, dim=(1,2))\n",
    "\n",
    "        # If mask is all False, skip batch (shouldn't happen)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # Only keep non-padded triples\n",
    "        qry_embeds    = qry_embeds[mask]\n",
    "        rel_embeds    = rel_embeds[mask]\n",
    "        irrel_embeds  = irrel_embeds[mask]\n",
    "        q_lens        = [q_lens[i] for i in range(len(mask)) if mask[i]]\n",
    "        r_lens        = [r_lens[i] for i in range(len(mask)) if mask[i]]\n",
    "        i_lens        = [i_lens[i] for i in range(len(mask)) if mask[i]]\n",
    "\n",
    "        qry_vecs    = qry_tower(qry_embeds, q_lens)\n",
    "        rel_vecs    = doc_tower(rel_embeds, r_lens)\n",
    "        irrel_vecs  = doc_tower(irrel_embeds, i_lens)\n",
    "\n",
    "        loss = triplet_loss_function(\n",
    "            qry_vecs, rel_vecs, irrel_vecs,\n",
    "            distance_function=distance_function,\n",
    "            margin=margin\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "# Save final model after all epochs are done\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'qry_tower_state_dict': qry_tower.state_dict(),\n",
    "    'doc_tower_state_dict': doc_tower.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': epoch_loss,  # from last epoch\n",
    "}, \"two_tower_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9129194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8. Load the trained model\n",
    "# -----------------------------\n",
    "hidden_dim = 128\n",
    "\n",
    "# 1. Define your model classes (as in your code)\n",
    "qry_tower = QueryTower(embed_dim, hidden_dim, num_layers=1, rnn_type='gru')\n",
    "doc_tower = DocTower(embed_dim, hidden_dim, num_layers=1, rnn_type='gru')\n",
    "\n",
    "# 2. Create optimizer with the same params as training\n",
    "optimizer = torch.optim.Adam(list(qry_tower.parameters()) + list(doc_tower.parameters()), lr=1e-3)\n",
    "\n",
    "# 3. Load checkpoint\n",
    "checkpoint = torch.load(\"two_tower_final.pt\", map_location=\"cpu\")  # or map_location=\"cuda\" if using GPU\n",
    "\n",
    "# 4. Restore state dicts\n",
    "qry_tower.load_state_dict(checkpoint['qry_tower_state_dict'])\n",
    "doc_tower.load_state_dict(checkpoint['doc_tower_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# 5. (Optional) get other info\n",
    "epoch = checkpoint['epoch']\n",
    "last_loss = checkpoint['loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8edefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8. Evaluate the model using Recall@K\n",
    "# -----------------------------\n",
    "\n",
    "def evaluate_model(qry_tower, doc_tower, val_data, distance_fn, K=1, device=\"cpu\"):\n",
    "    qry_tower.eval()\n",
    "    doc_tower.eval()\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for q_embed, rel_embeds, irrel_embeds in val_data:\n",
    "            # q_embed: (q_seq_len, embed_dim)\n",
    "            # rel_embeds: list of (d_seq_len, embed_dim) [just 1, or list if more]\n",
    "            # irrel_embeds: list of (d_seq_len, embed_dim)\n",
    "            \n",
    "            # Stack all candidate docs: relevant + irrelevants\n",
    "            candidates = [rel_embeds] + list(irrel_embeds)\n",
    "            all_doc_tensors = [doc.to(device) for doc in candidates]\n",
    "            doc_lens = [doc.shape[0] for doc in all_doc_tensors]\n",
    "            padded_docs = torch.nn.utils.rnn.pad_sequence(all_doc_tensors, batch_first=True)\n",
    "            \n",
    "            # Encode query\n",
    "            q_input = q_embed.unsqueeze(0).to(device)              # (1, q_seq_len, embed_dim)\n",
    "            q_len = [q_embed.shape[0]]\n",
    "            q_vec = QueryTower(q_input, q_len)                     # (1, hidden_dim)\n",
    "            \n",
    "            # Encode all docs in batch\n",
    "            d_vecs = DocTower(padded_docs, doc_lens)               # (num_candidates, hidden_dim)\n",
    "\n",
    "            # Compute distances (query vs. all docs)\n",
    "            dists = distance_fn(q_vec.repeat(len(doc_lens), 1), d_vecs)  # (num_candidates,)\n",
    "            sorted_indices = torch.argsort(dists)  # Smallest = most similar\n",
    "\n",
    "            # Recall@K: Is the relevant doc (index 0) in top K?\n",
    "            if 0 in sorted_indices[:K]:\n",
    "                num_correct += 1\n",
    "            total += 1\n",
    "\n",
    "    recall_at_k = num_correct / total\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac1598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing triples: 100%|██████████| 10047/10047 [00:06<00:00, 1594.66it/s]\n",
      "CBOW embedding + streaming: 100%|██████████| 10047/10047 [00:59<00:00, 168.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9. Load MS MARCO V1.1 validation dataset, process into triples and tokenize & embed\n",
    "# -----------------------------\n",
    "\n",
    "# Load triples\n",
    "with open(\"triples_val.pkl\", \"rb\") as f:\n",
    "    triples_val = pickle.load(f)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9 ]+', '', text)\n",
    "    return text.split()\n",
    "\n",
    "tokenized_triples_val = []\n",
    "for query, rel_docs, irrel_docs in tqdm(triples_val, desc=\"Tokenizing triples\"):\n",
    "    tokenized_query = preprocess(query)\n",
    "    tokenized_rels = [preprocess(doc) for doc in rel_docs]\n",
    "    tokenized_irrels = [preprocess(doc) for doc in irrel_docs]\n",
    "    tokenized_triples_val.append((tokenized_query, tokenized_rels, tokenized_irrels))\n",
    "\n",
    "\n",
    "num_docs = 10  # number of relevant/irrelevant docs you expect per triple\n",
    "batch_size = 64  # adjust to taste; big enough to speed up, small enough to never threaten RAM\n",
    "\n",
    "query_embeddings_val = []\n",
    "relevant_doc_embeddings_val = []\n",
    "irrelevant_doc_embeddings_val = []\n",
    "\n",
    "def process_and_save_embeddings(\n",
    "    tokenized_triples_val, word_to_ix, cbow_model, \n",
    "    query_embeds_path, rel_doc_embeds_path, irrel_doc_embeds_path,\n",
    "    batch_size=batch_size, num_docs=num_docs\n",
    "):\n",
    "    query_embeds_batch = []\n",
    "    rel_doc_embeds_batch = []\n",
    "    irrel_doc_embeds_batch = []\n",
    "    \n",
    "    for i, (tokenized_query, tokenized_rels, tokenized_irrels) in enumerate(\n",
    "        tqdm(tokenized_triples_val, desc=\"CBOW embedding + streaming\", total=len(tokenized_triples_val))\n",
    "    ):\n",
    "        # Query: embeddings per token\n",
    "        q_ids = [word_to_ix[t] for t in tokenized_query if t in word_to_ix]\n",
    "        if q_ids:\n",
    "            with torch.no_grad():\n",
    "                q_vecs = cbow_model.embeddings(torch.tensor(q_ids))\n",
    "            query_embeds_batch.append(q_vecs)  # shape: [query_len, embed_dim]\n",
    "        else:\n",
    "            query_embeds_batch.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "        \n",
    "        # Relevant docs: list of (doc_len, embed_dim)\n",
    "        rel_embs = []\n",
    "        for doc_tokens in tokenized_rels[:num_docs]:\n",
    "            doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "            if doc_ids:\n",
    "                with torch.no_grad():\n",
    "                    doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "                rel_embs.append(doc_vecs)\n",
    "            else:\n",
    "                rel_embs.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "        while len(rel_embs) < num_docs:\n",
    "            rel_embs.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "        rel_doc_embeds_batch.append(rel_embs)\n",
    "\n",
    "        # Irrelevant docs: list of (doc_len, embed_dim)\n",
    "        irrel_embs = []\n",
    "        for doc_tokens in tokenized_irrels[:num_docs]:\n",
    "            doc_ids = [word_to_ix[t] for t in doc_tokens if t in word_to_ix]\n",
    "            if doc_ids:\n",
    "                with torch.no_grad():\n",
    "                    doc_vecs = cbow_model.embeddings(torch.tensor(doc_ids))\n",
    "                irrel_embs.append(doc_vecs)\n",
    "            else:\n",
    "                irrel_embs.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "        while len(irrel_embs) < num_docs:\n",
    "            irrel_embs.append(torch.zeros(1, cbow_model.embeddings.embedding_dim))\n",
    "        irrel_doc_embeds_batch.append(irrel_embs)\n",
    "\n",
    "        # Save every batch_size triples\n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == len(tokenized_triples_val):\n",
    "            # Save as batch-lists (not stacked), because sequences are ragged\n",
    "            with open(query_embeds_path, 'ab') as fq:\n",
    "                pickle.dump(query_embeds_batch, fq)\n",
    "            with open(rel_doc_embeds_path, 'ab') as fr:\n",
    "                pickle.dump(rel_doc_embeds_batch, fr)\n",
    "            with open(irrel_doc_embeds_path, 'ab') as fi:\n",
    "                pickle.dump(irrel_doc_embeds_batch, fi)\n",
    "\n",
    "            # Free RAM\n",
    "            query_embeds_batch.clear()\n",
    "            rel_doc_embeds_batch.clear()\n",
    "            irrel_doc_embeds_batch.clear()\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "process_and_save_embeddings(\n",
    "    tokenized_triples_val, word_to_ix, cbow_model,\n",
    "    \"query_embeds_val.pkl\", \"rel_doc_embeds_val.pkl\", \"irrel_doc_embeds_val.pkl\",\n",
    "    batch_size=batch_size, num_docs=num_docs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9610d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.eval() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m qry_tower.eval()\n\u001b[32m     15\u001b[39m doc_tower.eval()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m recall = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQueryTower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDocTower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosine_distance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecall@1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(QueryTower, DocTower, val_data, distance_fn, K, device)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_model\u001b[39m(QueryTower, DocTower, val_data, distance_fn, K=\u001b[32m1\u001b[39m, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mQueryTower\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     DocTower.eval()\n\u001b[32m      8\u001b[39m     num_correct = \u001b[32m0\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Module.eval() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9b. Load validation data embeddings\n",
    "# -----------------------------\n",
    "\n",
    "# Load them all\n",
    "query_embeds_val = load_all_batches(\"query_embeds_val.pkl\")           # list of [query_len, embed_dim] tensors\n",
    "rel_doc_embeds_val = load_all_batches(\"rel_doc_embeds_val.pkl\")       # list of lists: each is [num_docs] of [doc_len, embed_dim] tensors\n",
    "irrel_doc_embeds_val = load_all_batches(\"irrel_doc_embeds_val.pkl\")\n",
    "\n",
    "val_data = []\n",
    "# Ensure lengths match (or handle indexing errors)\n",
    "for i in range(len(query_embeds_val)):\n",
    "    q_embed = query_embeds_val[i]                       # [q_len, embed_dim]\n",
    "    rel_embed = rel_doc_embeds_val[i][0]                # [rel_len, embed_dim]; use the first relevant doc\n",
    "    irrel_embed = irrel_doc_embeds_val[i][0]            # [irrel_len, embed_dim]; use the first irrelevant doc\n",
    "    val_data.append((q_embed, rel_embed, [irrel_embed])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3e57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h3/syzlph196gggwwlj2q3qdr_40000gn/T/ipykernel_64787/3108892676.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_doc_tensors = [torch.tensor(doc, device=device) for doc in candidates]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 0.9021\n"
     ]
    }
   ],
   "source": [
    "qry_tower.eval()\n",
    "doc_tower.eval()\n",
    "\n",
    "recall = evaluate_model(qry_tower, doc_tower, val_data, cosine_distance, K=1)\n",
    "print(f\"Recall@1: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ddc826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 800/800 [14:41<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Margin: 0.2, Hidden: 128 | Epoch 1/3 | Loss: 0.1212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 800/800 [14:49<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Margin: 0.2, Hidden: 128 | Epoch 2/3 | Loss: 0.0529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 800/800 [18:02<00:00,  1.35s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Margin: 0.2, Hidden: 128 | Epoch 3/3 | Loss: 0.0310\n",
      "Margin: 0.2, Hidden: 128, Recall@1: 0.8677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   2%|▎         | 20/800 [00:49<32:18,  2.48s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m i_lens = [i_lens[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(mask)) \u001b[38;5;28;01mif\u001b[39;00m mask[i]]\n\u001b[32m     36\u001b[39m qry_vecs = qry_tower(qry_embeds, q_lens)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m rel_vecs = \u001b[43mdoc_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m irrel_vecs = doc_tower(irrel_embeds, i_lens)\n\u001b[32m     40\u001b[39m loss = triplet_loss_function(\n\u001b[32m     41\u001b[39m     qry_vecs, rel_vecs, irrel_vecs,\n\u001b[32m     42\u001b[39m     distance_function=distance_function,\n\u001b[32m     43\u001b[39m     margin=margin\n\u001b[32m     44\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mDocTower.forward\u001b[39m\u001b[34m(self, x, lengths)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, lengths):\n\u001b[32m     34\u001b[39m     packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     out, h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(h, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m     37\u001b[39m         h = h[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/rnn.py:881\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m    878\u001b[39m     result = _VF.lstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m._flat_weights, \u001b[38;5;28mself\u001b[39m.bias, \u001b[38;5;28mself\u001b[39m.num_layers,\n\u001b[32m    879\u001b[39m                       \u001b[38;5;28mself\u001b[39m.dropout, \u001b[38;5;28mself\u001b[39m.training, \u001b[38;5;28mself\u001b[39m.bidirectional, \u001b[38;5;28mself\u001b[39m.batch_first)\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m output = result[\u001b[32m0\u001b[39m]\n\u001b[32m    884\u001b[39m hidden = result[\u001b[32m1\u001b[39m:]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 10. Hyperparameter Search & Evaluation\n",
    "# -----------------------------\n",
    "\n",
    "margins = [0.2, 0.1, 0.05]\n",
    "hidden_dims = [128]\n",
    "distance_function = cosine_similarity\n",
    "\n",
    "best_result = {\"score\": 0, \"settings\": None}\n",
    "\n",
    "for margin in margins:\n",
    "    for hidden_dim in hidden_dims:\n",
    "        qry_tower = QueryTower(embed_dim, hidden_dim, rnn_type='lstm')  # or 'lstm'\n",
    "        doc_tower = DocTower(embed_dim, hidden_dim, rnn_type='lstm')    # or 'lstm'\n",
    "\n",
    "        optimizer = torch.optim.Adam(list(qry_tower.parameters()) + list(doc_tower.parameters()), lr=1e-3)\n",
    "        num_epochs = 3  # Set as needed\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                qry_embeds, rel_embeds, irrel_embeds, q_lens, r_lens, i_lens = batch\n",
    "\n",
    "                mask = ~torch.all(rel_embeds == 0, dim=(1,2))\n",
    "\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                qry_embeds = qry_embeds[mask]\n",
    "                rel_embeds = rel_embeds[mask]\n",
    "                irrel_embeds = irrel_embeds[mask]\n",
    "                q_lens = [q_lens[i] for i in range(len(mask)) if mask[i]]\n",
    "                r_lens = [r_lens[i] for i in range(len(mask)) if mask[i]]\n",
    "                i_lens = [i_lens[i] for i in range(len(mask)) if mask[i]]\n",
    "\n",
    "                qry_vecs = qry_tower(qry_embeds, q_lens)\n",
    "                rel_vecs = doc_tower(rel_embeds, r_lens)\n",
    "                irrel_vecs = doc_tower(irrel_embeds, i_lens)\n",
    "\n",
    "                loss = triplet_loss_function(\n",
    "                    qry_vecs, rel_vecs, irrel_vecs,\n",
    "                    distance_function=distance_function,\n",
    "                    margin=margin\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"Margin: {margin}, Hidden: {hidden_dim} | Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "        recall = evaluate_model(qry_tower, doc_tower, val_data, distance_function, K=1, device=device)\n",
    "        print(f\"Margin: {margin}, Hidden: {hidden_dim}, Recall@1: {recall:.4f}\")\n",
    "        if recall > best_result[\"score\"]:\n",
    "            best_result = {\"score\": recall, \"settings\": (margin, hidden_dim)}\n",
    "\n",
    "print(\"Best config:\", best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05505892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 11. Inference: Encode documents and queries, find top-k relevant docs\n",
    "# -----------------------------\n",
    "\n",
    "def encode_documents(doc_tower, all_doc_embeds, all_doc_lens, device='cpu', batch_size=128):\n",
    "    doc_tower.eval()\n",
    "    all_vecs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(all_doc_embeds), batch_size):\n",
    "            batch_embeds = all_doc_embeds[i:i+batch_size].to(device)\n",
    "            batch_lens = all_doc_lens[i:i+batch_size]\n",
    "            vecs = doc_tower(batch_embeds, batch_lens)  # Shape: (batch, hidden_dim)\n",
    "            all_vecs.append(vecs.cpu())\n",
    "    return torch.cat(all_vecs, dim=0)  # Shape: (num_docs, hidden_dim)\n",
    "\n",
    "def encode_query(qry_tower, query_embed, query_len, device='cpu'):\n",
    "    qry_tower.eval()\n",
    "    with torch.no_grad():\n",
    "        query_vec = qry_tower(query_embed.to(device), query_len)\n",
    "    return query_vec.cpu()  # Shape: (1, hidden_dim)\n",
    "\n",
    "def find_top_k(query_vec, doc_vecs, k=5, distance_fn=None):\n",
    "    # query_vec: (1, hidden_dim), doc_vecs: (num_docs, hidden_dim)\n",
    "    if distance_fn is None:\n",
    "        # Default to cosine distance\n",
    "        def distance_fn(q, d):\n",
    "            return 1 - torch.nn.functional.cosine_similarity(q, d)\n",
    "    distances = distance_fn(query_vec, doc_vecs)\n",
    "    # If query_vec is (1,hidden_dim), expand to (num_docs,hidden_dim)\n",
    "    if query_vec.shape[0] == 1:\n",
    "        distances = distance_fn(query_vec.expand_as(doc_vecs), doc_vecs)\n",
    "    # Get top k smallest distances\n",
    "    topk = torch.topk(-distances, k)  # negative because smallest distance = highest relevance\n",
    "    indices = topk.indices.cpu().numpy()\n",
    "    scores = -topk.values.cpu().numpy()\n",
    "    return indices, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a33a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 12. Calculate all document embeddings and lengths\n",
    "# -----------------------------\n",
    "\n",
    "all_doc_texts = []  # To hold all relevant doc texts for padding\n",
    "all_query_texts = []\n",
    "\n",
    "for query, rels, irrels in triples:\n",
    "    all_query_texts.append(query)\n",
    "    # rels is the list of relevant doc texts for this query\n",
    "    # We slice to num_docs to match the embedding logic, then pad if needed\n",
    "    for doc in rels[:num_docs]:\n",
    "        all_doc_texts.append(doc)\n",
    "    # Pad if fewer than num_docs\n",
    "    while len(rels) < num_docs:\n",
    "        all_doc_texts.append(\"\")\n",
    "        rels.append(\"\")  # So embedding and text padding always match\n",
    "\n",
    "# Step 1: Gather your document embedding sequences and their lengths\n",
    "all_doc_embeds_list = []\n",
    "all_doc_lens_list = []\n",
    "\n",
    "for batch in dataloader:\n",
    "    rel_embeds, r_lens = batch[1], batch[4]  # rel_embeds: (batch, seq_len, embed_dim)\n",
    "    for i in range(rel_embeds.shape[0]):\n",
    "        all_doc_embeds_list.append(rel_embeds[i])  # (seq_len, embed_dim)\n",
    "        all_doc_lens_list.append(r_lens[i])\n",
    "\n",
    "# Step 2: Pad all embeddings to max seq_len\n",
    "# pad_sequence wants a list of (seq_len, embed_dim), returns (max_seq_len, num_docs, embed_dim)\n",
    "padded = pad_sequence(all_doc_embeds_list, batch_first=True)  # (num_docs, max_seq_len, embed_dim)\n",
    "\n",
    "all_doc_embeds = padded  # (num_docs, max_seq_len, embed_dim)\n",
    "all_doc_lens = torch.tensor(all_doc_lens_list)\n",
    "\n",
    "print(\"all_doc_embeds shape:\", all_doc_embeds.shape)\n",
    "print(\"all_doc_lens shape:\", all_doc_lens.shape)\n",
    "print(\"all_doc_texts length:\", len(all_doc_texts))\n",
    "print(\"all_query_texts length:\", len(all_query_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c68b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m device = \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 1. Encode all documents\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m doc_vecs = \u001b[43mencode_documents\u001b[49m(doc_tower, all_doc_embeds, all_doc_lens, device=device)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#print(\"Encoded document vectors shape:\", doc_vecs.shape)\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 2. Encode a sample query (here, just using first doc for demo)\u001b[39;00m\n\u001b[32m     12\u001b[39m query_embed = all_doc_embeds[\u001b[32m0\u001b[39m].unsqueeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'encode_documents' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 13. Encode documents and queries, find top-k relevant docs\n",
    "# -----------------------------\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 1. Encode all documents\n",
    "doc_vecs = encode_documents(doc_tower, all_doc_embeds, all_doc_lens, device=device)\n",
    "#print(\"Encoded document vectors shape:\", doc_vecs.shape)\n",
    "\n",
    "# 2. Encode a sample query (here, just using first doc for demo)\n",
    "query_embed = all_doc_embeds[0].unsqueeze(0)\n",
    "query_len = all_doc_lens[0].unsqueeze(0)\n",
    "query_vec = encode_query(qry_tower, query_embed, query_len, device=device)\n",
    "#print(\"Encoded query vector shape:\", query_vec.shape)\n",
    "\n",
    "# 3. Retrieve top k relevant documents\n",
    "k = 5\n",
    "indices, scores = find_top_k(query_vec, doc_vecs, k=k)\n",
    "\n",
    "print(\"Query: \", all_query_texts[0])\n",
    "print(\"Top document matches:\")\n",
    "for rank, i in enumerate(indices):\n",
    "    print(f\"{rank+1}: {all_doc_texts[i]}\")\n",
    "    print(f\"   (score: {scores[rank]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d56ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 14. Custom Query Inference\n",
    "# -----------------------------\n",
    "\n",
    "# Hardcode your query here\n",
    "custom_query = \"what is machine learning?\"\n",
    "\n",
    "# Tokenize and embed the custom query\n",
    "tokenized_query = tokenizer(custom_query)  # returns List[int] or torch.Tensor\n",
    "\n",
    "# Map tokens to ids, filtering out tokens not in vocab\n",
    "q_ids = [word_to_ix[t] for t in tokenized_query if t in word_to_ix]\n",
    "\n",
    "if q_ids:\n",
    "    with torch.no_grad():\n",
    "        q_vecs = cbow_model.embeddings(torch.tensor(q_ids))  # (seq_len, embed_dim)\n",
    "else:\n",
    "    q_vecs = torch.zeros(1, embed_dim)  # Fallback for empty/unknown queries)\n",
    "\n",
    "# Pad to (1, seq_len, embed_dim) for model\n",
    "query_embed = q_vecs.unsqueeze(0)  # (1, seq_len, embed_dim)\n",
    "query_len = torch.tensor([q_vecs.shape[0]])\n",
    "\n",
    "# Move to device if needed\n",
    "query_embed = query_embed.to(device)\n",
    "query_len = query_len.to(device)\n",
    "\n",
    "# Encode the hardcoded query\n",
    "query_vec = encode_query(qry_tower, query_embed, query_len, device=device)\n",
    "\n",
    "# Retrieve top k relevant documents\n",
    "k = 5\n",
    "indices, scores = find_top_k(query_vec, doc_vecs, k=k)\n",
    "\n",
    "print(\"Custom Query:\", custom_query)\n",
    "print(\"Top document matches:\")\n",
    "for rank, i in enumerate(indices):\n",
    "    print(f\"{rank+1}: {all_doc_texts[i]}\")\n",
    "    print(f\"   (score: {scores[rank]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26dddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 15. Redis test\n",
    "# -----------------------------\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "\n",
    "r.set('foo', 'bar')\n",
    "# True\n",
    "r.get('foo')\n",
    "# bar\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
